---
title: "Missing Data"
subtitle: "Psy-504"
author: 
  - Steven Mesquiti
institute: "Princeton University"
date: today
format: 
  revealjs:
    theme: [default, styles.scss]
    highlight-style: github-dark
    toc: true
    toc-depth: 1
    slide-number: true
    fig-width: 8
    fig-height: 5
    transition: slide
categories: [Lab, code, analysis, missing-data]
execute:
  message: false
  warning: false
params:
  SHOW_SOLS: true
  TOGGLE: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE,fig.path = "Missing_data/fig_")
options(scipen=999)
```


```{r}
#| label: get-started
#| include: false

library(pacman)
pacman::p_load(tidyverse,mice,naniar,devtools,patchwork,gghalves,install = T)
devtools::install_github("hadley/emo")
set.seed(42)


palette <- c(
  "#772e25", "#c44536", "#ee9b00", "#197278", "#283d3b", 
  "#9CC5A1", "#6195C6", "#ADA7C9", "#4D4861", "grey50",
  "#d4a373", "#8a5a44", "#4a6a74", "#5c80a8", "#a9c5a0",
  "#7b9b8e", "#e1b16a", "#a69b7c", "#9d94c4", "#665c54"
)

palette_condition = c("#ee9b00", "#c44536","#005f73", "#283d3b", "#9CC5A1", "#6195C6", "#ADA7C9", "#4D4861")
plot_aes = theme_minimal() +
  theme(
    legend.position = "top",
    legend.text = element_text(size = 12),
    text = element_text(size = 16, family = "Futura Medium"),
    axis.text = element_text(color = "black"),
    axis.ticks.y = element_blank(),
    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering
  )
```

## Overview

-   Importance of addressing missing data in research\
-   Traditional vs. modern methods\
-   Focus on:
    -   Maximum Likelihood (ML)\
    -   Multiple Imputation (MI)\
-   Benefits of modern methods\
-   Demo using a toy dataset

::: notes
Hi! Today im going to be talking to you today about missing data.\
Missing data is a common problem in research, and it can have a significant impact on the validity of your results.\
In this presentation, I will discuss the importance of addressing missing data, traditional methods for handling it, and modern techniques like Maximum Likelihood and Multiple Imputation.\
I will also attempt to provide a demo using a toy dataset to illustrate these concepts.\

Let's get started
:::

------------------------------------------------------------------------

## The Problem of Missing Data {.scrollable}

-   Missing data is common in quantitative research
-   Traditional methods (e.g., deletion, mean imputation) are often inadequate
    -   Biased estimates
    -   Reduced statistical power
    
<div style="text-align: center;">
  ![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExbTA0Ynp6Z21tcm5ubTBudzVzOGNsdnl2bHM0NnJjeXhoZnE5YnF2eiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ImHfnm01jqvostvlLP/giphy.gif)
</div>
    

::: notes
Missing data are ubiquitous in quantitative research studies.\
Because of its pervasive nature, some methodologists have described missing data as “one of the most important statistical and design problems in research” (methodologist William Shadish, quoted in Azar, 2002, p. 70).
:::

------------------------------------------------------------------------

## Different `Types` of Missing Data 

-   **Missing Completely at Random (MCAR)**: Missingness is unrelated to observed or unobserved data
-   **Missing at Random (MAR)**: Missingness is related to observed data but not unobserved data
-   **Missing Not at Random (MNAR)**: Missingness is related to unobserved data

::: notes
There are theree different `types` of missing data. \ 
These mechanisms describe relationships between measured variables and the probability of missing data. While these terms have a precise probabilistic and mathematical meaning, they are essentially three different explanations for why the data are missing.

CAR (Missing Completely at Random)
Definition: The missingness is entirely unrelated to any variables in the dataset, including the variable with missing values.

In Practice: Data are missing for reasons that are truly random.

Example: A student moves to another district mid-study, and the move isn’t related to anything being measured (like grades or behavior).

Implication: Analyses remain unbiased if MCAR is true, but it's a rare scenario.

MAR (Missing at Random)
Definition: The missingness is related to other measured variables but not to the missing variable’s own values.

In Practice: You can predict the missingness based on other known data.

Example: Students who use substances more often are more likely to miss school and therefore miss a self-esteem questionnaire. The missing self-esteem data is related to substance use (which is observed), not self-esteem itself.

Implication: Common in real-world data. Methods like multiple imputation or maximum likelihood can handle MAR well.

MNAR (Missing Not at Random)
Definition: The missingness depends on the missing values themselves.

In Practice: The reason data is missing is due to what the missing value would have been.

Example: Poor readers skip hard test items because they don’t understand them—so the missing responses depend on their actual (low) reading ability.

Implication: This is the most difficult type to deal with statistically, because the missingness contains information that is unobserved.
:::

---

## Building and deploying example {.scrollable}

::: panel-tabset
### Create `Toy` Dataset

First, let's create our toy dataset

```{r}
#| echo: true
set.seed(42)
n <- 1000  # Number of observations
toy <- data.frame(
  x = rnorm(n, mean = 0, sd = 5),  
  y = rnorm(n, mean = 0, sd = 7),  
  z = sample(0:3, n, replace = TRUE)
)

# Introduce missingness in `y`
toy$y[sample(1:n, 300)] <- NA  # 300 missing values randomly assigned to `y`
```

### Visualize Missing Data

-   visualize with `gg_miss_var` from the `naniar` package
-   as you can see we have missing data in the `y` variable

```{r, fig.align='center'}
toy |> 
gg_miss_var(show_pct = TRUE) +
  labs(title = "Missing Data by Variable in our dataset") + 
  scale_fill_manual(values = palette) +
  plot_aes
```
:::

::: notes
First, we'll create our synthetic dataset using hte code provided.\
The dataset contains three variables: `x`, `y`, and `z`. and y has \~300 missing values.\

What are some ways in which we can deal with them?
:::

------------------------------------------------------------------------

## Traditional Missing Data Techniques

-   **Listwise Deletion**: Drops any *row* with NA (e.g., `na.rm = T`)\
-   **Mean Imputation**: Replaces missing values with the mean value\
-   These are easy to use, but often leads to **biased** results, let's see why

::: notes
Traditionally, people use things like listwise deletion or mean imputation to deal with missing data.\
But this can create problems because it'll reduce power (if you remove observations from your dataset) and can also introduce bias (if you replace missing values with the mean). Let's see how we'd implement this in R
:::

------------------------------------------------------------------------

## How to implement Mean Imputation {.scrollable}

-   Here, we are asking R to impute the mean of `y` and replace any missing values with that mean\

```{r}
#| echo: true

# Mean imputation
toy$y_mean <- ifelse(is.na(toy$y), mean(toy$y, na.rm = TRUE), toy$y)
```

------------------------------------------------------------------------

## Visualizing Mean Imputation {.scrollable}


```{r, fig.align='center'}
# make long 
toy_long <- toy |> 
  pivot_longer(cols = c("y", "y_mean"), names_to = "imputation", values_to = "value") |> 
  mutate(imputation = recode(imputation, "y" = "Original", "y_mean" = "Mean Imputation"))

ggplot(toy_long, aes(x = imputation, y = value, fill = imputation)) +
  geom_half_violin(
    side = "r",
    alpha = 0.6,
    color = NA
  ) +
  geom_boxplot(
    aes(color = imputation),
    width = 0.1,
    outlier.shape = NA,
    alpha = 0.4
  ) +
  geom_half_point(
    aes(color = imputation),
    side = "l",
    alpha = 0.1,
    shape = 16,
    size = 1
  ) +
  labs(title = "Mean Imputation vs. Original Data", x = NULL, y = "Value") +
  scale_fill_manual(values = palette_condition[6:9]) +
  scale_color_manual(values = palette_condition[6:9]) +  # Add this line
  plot_aes +
  coord_flip() +
  theme(legend.position = "none")


```
::: notes
*   Visualize the original data and the imputed data to compare the distributions
*   Notice the thick line at the mean
:::



------------------------------------------------------------------------

## How does this influence downstream processes? {.scrollable}

```{r fig.align='center'}
#| message: false
#| warning: false
# Mean imputation

# Fit the two linear models
model_no_imp <- lm(y ~ x, data = toy)
model_imp <- lm(y_mean ~ x, data = toy)

# Extract coefficients and 95% CIs
coef_no_imp <- coef(summary(model_no_imp))["x", ]
ci_no_imp <- confint(model_no_imp)["x", ]

coef_imp <- coef(summary(model_imp))["x", ]
ci_imp <- confint(model_imp)["x", ]

# Format regression info text
label_no_imp <- sprintf("β = %.2f [%.2f, %.2f]", coef_no_imp["Estimate"], ci_no_imp[1], ci_no_imp[2])
label_imp <- sprintf("β = %.2f [%.2f, %.2f]", coef_imp["Estimate"], ci_imp[1], ci_imp[2])

# Create the plots with annotation
plot_no_imputation <- ggplot(toy, aes(x = x, y = y)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", color = palette[1]) +
  annotate("text", x = Inf, y = -Inf, label = label_no_imp, hjust = 1.1, vjust = -0.5, size = 4) +
  labs(title = "Regression Without Mean Imputation") +
  plot_aes +
  theme(legend.position = "none")

plot_with_imputation <- ggplot(toy, aes(x = x, y = y_mean)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", color = palette[4]) +
  annotate("text", x = Inf, y = -Inf, label = label_imp, hjust = 1.1, vjust = -0.5, size = 4) +
  labs(title = "Regression With Mean Imputation") +
  plot_aes +
  theme(legend.position = "none")

# Combine with patchwork
combined_plot <- plot_no_imputation / plot_with_imputation

# Display
combined_plot
```

::: notes
*   Now, let's see how this affects our regression model. 
*   The difference seems neligible (at least with working with big data) but the mean imputation is biased
*   In smaller datasets mean imputation can have larger downstream consequences

:::

------------------------------------------------------------------------

## Modern Method: Multiple Imputation (MI)

-   Imputes multiple plausible values\
-   Models missingness using relationships among variables\
-   Pools results for accurate estimates and standard errors\
-   More robust than traditional methods

------------------------------------------------------------------------

## Multiple Imputation with `MICE` Package {.scrollable}

::: notes
Mice stands for Multiple Imputation with Chained Equations
:::

::: panel-tabset
## Code

```{r}
#| echo: true
# Multiple imputation
imp <- mice(toy[, c("x", "y", "z")], m = 5, method = "pmm", seed = 42,printFlag = F)
fit_mi <- with(imp, lm(y ~ x + z))
pooled_summary = summary(pool(fit_mi))

pooled_summary |> 
  as.data.frame() |>
  mutate_if(is.numeric, round, 3)  |> 
  DT::datatable(options = list(pageLength = 10, autoWidth = TRUE), 
                rownames = FALSE)

```

## Explanation

**What is this code doing?**

-   Step 1: Use the `mice()` function to create 5 imputed datasets for the variables x, y, and z
-   Step 2: Fit a linear model (y \~ x + z) on each imputed dataset using x`with()`.
-   Step 3: Combine the results across all models using `pool()` to account for variability between imputations
:::

------------------------------------------------------------------------

## Plotting Imputed Datasets produced from MICE

```{r, fig.align='center'}
long_imp <- complete(imp, "long", include = TRUE)
ggplot(long_imp, aes(x = x, y = y, color = factor(.imp))) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE,alpha = 0.5) +
  labs(title = "Imputed Datasets with Regression Lines", color = "Imputation") +
  scale_color_manual(values = palette_condition) +
  plot_aes
```

------------------------------------------------------------------------

## Modern Method: Maximum Likelihood (ML) {.scrollable}

*   MLE doesn't fill in missing values.

*   Instead, it finds the parameter values (e.g., mean, regression coefficients) that make the observed data most probable.

*   Based on the log likelihood function – it chooses parameters that minimize the distance between the model and the data.

**Requires:**

*   Assumes MAR

*   Assumes multivariate normality

* The under the hood math

$$
\begin{align}
\log L &= \sum_{i=1}^{N} \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{1}{2} \left( \frac{y_i - \mu}{\sigma} \right)^2 \right) \right)
\end{align}
$$

::: notes

Like multiple imputation, this approach assumes multivariate normality and MAR data. However, the mechanics of maximum likelihood are quite different. Rather than filling in the missing values, maximum likelihood uses all of the available data – complete and incomplete – to identify the parameter values that have the highest probability of producing the sample data. At a broad level, maximum likelihood estimation identifies the population parameter values that have the highest probability of producing the sample data. This estimation process uses a mathematical function called a log likelihood to quantify the standardized distance between the observed data points and the parameters of interest (e.g., the mean), and the goal is to identify parameter estimates that minimize these distances. This is conceptually similar to ordinary least squares estimation, where the goal is to identify the regression coefficients that minimize the collective distances between the data points and the predicted scores. The difference is that maximum likelihood uses all of the available data – complete and incomplete – to identify the parameter values that have the highest probability of producing the sample data. 
:::

---

## Using Auxiliary Variables

-   Auxiliary variables can help improve imputation\
-   These are variables that are not of primary interest but can help explain the missingness\
-   Improves imputation quality and reduces bias

------------------------------------------------------------------------

## Implementation Example {.scrollable}

```{r}
#| echo: true
# Add auxiliary variable
toy$aux <- toy$x + rnorm(n)
toy$y[sample(1:n, 300)] <- NA  # More missingness

imp_aux <- mice(toy[, c("x", "y", "z", "aux")], m = 5, method = "pmm", seed = 42,printFlag = F)
fit_aux <- with(imp_aux, lm(y ~ x + z + aux))
pooled_summary = summary(pool(fit_aux))
pooled_summary |> 
  as.data.frame() |>
  mutate_if(is.numeric, round, 3)  |> 
  DT::datatable(options = list(pageLength = 10, autoWidth = TRUE), 
                rownames = FALSE)

```

## Plotting Imputed Datasets produced from MICE with auxiliary variable

```{r, fig.align='center'}
long_imp <- complete(imp_aux, "long", include = TRUE)
ggplot(long_imp, aes(x = x, y = y, color = factor(.imp))) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE,alpha = 0.5) +
  labs(title = "Imputed Datasets with Regression Lines", color = "Imputation") +
  scale_color_manual(values = palette_condition) +
  plot_aes
```

::: notes 
Here we are testing whether adding the auxiliary variable aux improves your model's estimation of y, especially given the extra missingness. Including helpful predictors like aux can improve the quality of imputations and regression estimates. To do that we compare stuff like fit stats between the two models to see which produces the better fit. 
:::

------------------------------------------------------------------------

## Summary of Key Takeaways

| Method          | Bias    | Variability | Ease of Use     |
|-----------------|---------|-------------|-----------------|
| Listwise        | ❌ High | ❌ Reduced  | ✅ Easy         |
| Mean Imputation | ❌ High | ❌ Too Low  | ✅ Easy         |
| ML              | ✅ Low  | ✅ Accurate | ⚠:Intermediate  |
| MI              | ✅ Low  | ✅ Accurate | ⚠️ Intermediate |

------------------------------------------------------------------------

## Conclusion

-   Traditional methods can lead to biased results\
-   Modern techniques (ML, MI) use all available data\
-   Better estimates, standard errors, and power\
-   Use tools like `mice` and `naniar` for effective handling of missing data

------------------------------------------------------------------------

## Package Citations

```{r}
report::cite_packages()
```

## Thanks for listening!

<div style="text-align: center;">
  ![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ2t5cmgyeDVqbXc0djlsZ3BlcXE0bHd3N3h4Z282MmMxdmRxaDJ5NiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o7TKF44elNqqK84U0/giphy.gif)
</div>


---