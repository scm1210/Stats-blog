[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi!\nMy name is Steven Mesquiti! I’m a first-year in the Department of Psychology at Princeton University advised by Erik Nook.\nI study how we can use Natural Language Processing techniques and Artifical Intelligence to explain, predict, and improve people’s Mental Health.\nBefore Princeton, I worked as a Lab Manager for Emily Falk in the Communication Neuroscience Lab at the Univeristy of Pennsylvania. Along the way, I have worked with excessively generous scientists like Jamie Pennebaker, Lyle Ungar, & Angela Duckworth pursing questions at the nexus of Computer Science, Computational Linguistics, and Psychology\nThis is my blog for PSY-504. You can find various assignments that I complete on this website."
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html",
    "href": "posts/Lab-3/ord_lab_q.html",
    "title": "Ordinal Regression Lab Answers",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#instructions",
    "href": "posts/Lab-3/ord_lab_q.html#instructions",
    "title": "Ordinal Regression Lab Answers",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#load-packages",
    "href": "posts/Lab-3/ord_lab_q.html#load-packages",
    "title": "Ordinal Regression Lab Answers",
    "section": "Load packages:",
    "text": "Load packages:\n\nlibrary(pacman)\npacman::p_load(tidyverse, DT, broom, performance,\n               ordinal,car,ggeffects,gofact,brms,\n               emmeans,knirt,MASS,brant,\n               install = TRUE)\n\n\n#### define plot objects and stuff\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\n\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#load-data",
    "href": "posts/Lab-3/ord_lab_q.html#load-data",
    "title": "Ordinal Regression Lab Answers",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\n# gb &lt;- ....\n\n### only use the the first three ranks \ndata = gbbo |&gt; \n  rename(Technical_Rank = `Technical Rank`) |&gt; \n  filter(Technical_Rank &lt; 4) |&gt; \n  mutate(Technical_Rank = factor(Technical_Rank, levels = c(1, 2, 3), ordered = TRUE),\n         Gender = factor(Gender))"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#explore",
    "href": "posts/Lab-3/ord_lab_q.html#explore",
    "title": "Ordinal Regression Lab Answers",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\ngb &lt;- data %&gt;% \n  mutate(AgeGroup = cut(Age, \n                        breaks = seq(floor(min(Age, na.rm = TRUE)), ceiling(max(Age, na.rm = TRUE)), by = 10),\n                        include.lowest = TRUE, right = FALSE))\n\n# Compute percentages by Age Group\nage_rank &lt;- gb %&gt;%\n  group_by(AgeGroup, `Technical_Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\n# Compute percentages by Gender\ngender_rank &lt;- gb %&gt;%\n  group_by(Gender, `Technical_Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\n# Plot for Age Group\nggplot(age_rank, aes(x = AgeGroup, y = perc, fill = factor(`Technical_Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_manual(values = palette_condition) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age Group\",\n       x = \"Age Group\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  plot_aes +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Plot for Gender\nggplot(gender_rank, aes(x = Gender, y = perc, fill = factor(`Technical_Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_manual(values = palette_condition) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Gender\",\n       x = \"Gender\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  plot_aes"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#ordinal-analysis",
    "href": "posts/Lab-3/ord_lab_q.html#ordinal-analysis",
    "title": "Ordinal Regression Lab Answers",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\ndata = gbbo |&gt; \n  rename(Technical_Rank = `Technical Rank`) |&gt; \n  filter(Technical_Rank &lt; 4) |&gt; \n  mutate(Technical_Rank = factor(Technical_Rank, levels = c(1, 2, 3), ordered = TRUE),\n  Gender = factor(Gender)) \n\n\nThe order represents their placement in a technical bake-off.\n\n\nConvert input variables to categorical factors as appropriate.\n\n# Factorizing gender\nGender = factor(Gender)\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\n# Fit the ordinal logistic regression model\nmodel &lt;- clm(`Technical_Rank` ~ Gender * Age, data = gb)\n\n# Extract results with 95% confidence intervals\nresults &lt;- tidy(model, conf.int = TRUE) %&gt;%\n  rename(Estimate = estimate, `Lower CI` = conf.low, `Upper CI` = conf.high) %&gt;%\n  mutate(\n    Estimate = round(Estimate, 3),\n    `Lower CI` = round(`Lower CI`, 3),\n    `Upper CI` = round(`Upper CI`, 3),\n    p.value = round(2 * (1 - pnorm(abs(statistic))), 3)  # Compute p-values manually and round\n  )\n\n# Display results in an interactive DT table\ndatatable(results, \n          options = list(pageLength = 5, scrollX = TRUE),\n          caption = \"Ordinal Logistic Regression Results with 95% Confidence Intervals\")\n\n\n\n\nsummary(model)\n\nformula: Technical_Rank ~ Gender * Age\ndata:    gb\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  309  -336.64 683.28 3(0)  4.04e-08 1.1e+05\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \nGenderM     -1.14947    0.67290  -1.708   0.0876 .\nAge         -0.02311    0.01246  -1.855   0.0636 .\nGenderM:Age  0.03879    0.01853   2.093   0.0363 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -1.416692   0.459602  -3.082\n2|3  0.004944   0.452043   0.011\n\n\n\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n::: {.cell}\n\n```{.r .cell-code}\nmodel_interaction &lt;- clm(`Technical_Rank` ~ Gender * Age, data = gb)\n\n# Fit the model without the interaction term\nmodel_main &lt;- clm(`Technical_Rank` ~ Gender + Age, data = gb)\n\n# Compare the two models using ANOVA\nanova_results &lt;- anova(model_main, model_interaction)\nanova_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of cumulative link models:\n \n                  formula:                      link: threshold:\nmodel_main        Technical_Rank ~ Gender + Age logit flexible  \nmodel_interaction Technical_Rank ~ Gender * Age logit flexible  \n\n                  no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel_main             4 685.72 -338.86                        \nmodel_interaction      5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nwe should use the interaction term since th emodel has significantly better fit\n\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\npreds &lt;- ggemmeans(model_interaction, terms = c(\"Age\", \"Gender\"), type = \"fixed\")\n\nggplot(preds, aes(x = x, y = predicted, color = group, fill = group)) +\n  geom_line(size = 1) +  \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, linetype = \"dashed\") +  \n  facet_wrap(~response.level, scales = \"free_y\") +  \n  labs(title = \"Predicted Probability of Technical Rank by Age and Gender\",\n       x = \"Age\",\n       y = \"Predicted Probability\",\n       color = \"Gender\",\n       fill = \"Gender\") +  \n  scale_color_manual(values = palette) +  \n  scale_fill_manual(values = palette) +  \n  plot_aes\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\nols_clm = MASS::polr(Technical_Rank~Gender*Age, data=gb)\n\nggeffect(ols_clm, c(\"Age[all]\", \"Gender\"), latent=TRUE) %&gt;% plot() +  scale_color_manual(values = palette) +  \n  scale_fill_manual(values = palette) +  plot_aes \n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nbrant(ols_clm)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     1.29    3   0.73\nGenderM     0.58    1   0.44\nAge     0.06    1   0.8\nGenderM:Age 0.92    1   0.34\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\nWe fail to rejecet it proportional odds assumption holds"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#brms",
    "href": "posts/Lab-3/ord_lab_q.html#brms",
    "title": "Ordinal Regression Lab Answers",
    "section": "brms",
    "text": "brms\n\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\nmodel_path &lt;- file.path(\"/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/PSY-504/stevens-blog/posts/Lab-3/models/brms_model.rds\")\n\nif (!file.exists(model_path)) {\n  # If the RDS file does not exist, create the model\n  ols2_brm &lt;- brm(Technical_Rank ~ Gender * Age, data = gb, \n                  family = cumulative, cores = 4, chains = 4)\n  \n  # Save the model output to an RDS file\n  saveRDS(ols2_brm, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  ols2_brm &lt;- readRDS(model_path)\n}\n\n\nThe results are the same since we are using an uninformative prior and the estimates are similar to that of ML (frequentist estimations)\n\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n\n\nconditional_effects(ols2_brm, categorical = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?).\n\nRun the below code. What do you think?\n\ncheck_predictions(ols2_brm) |&gt;  plot() + plot_aes\n\n\n\n\n\n\n\n\n\nYes, the model appears to fit the data"
  },
  {
    "objectID": "posts/Lab-2/index.html",
    "href": "posts/Lab-2/index.html",
    "title": "Lab 2: Logistic Regression",
    "section": "",
    "text": "Assignment requirements:\n\nIf you are using Github (recommended), make sure to commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages, and share the link to your assignment with me. If not, you can also send me the rmd & rendered file via Canvas.\nIn this assignment, you will not need to code from scratch. Rather, you’ll need to fill in code where needed. This assignment has a logisitic regression implementation for a scenario from EDA down to model comparison (and would be useful for whenever you may encounter such a situation in the future).\nI want the assignments to begin reflecting a bit more of how you’d be doing things on your own, where you have some prior knowledge and you figure other things out (by referring to documentation, etc.) . In addition to the rmd, I also want you to submit to me notes of anything new that you learn while finishing the assignment. And any pain-points, and we’ll discuss more.\n\nNote:\n\nIf you are fitting a model, display the model output in a neatly formatted table. (The gt tidy and kable functions can help!). Modelsummary also looks good(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)\nMake sure that your plots are clearly labeled – for all axes, titles, etc."
  },
  {
    "objectID": "posts/Lab-2/index.html#data-general-social-survey",
    "href": "posts/Lab-2/index.html#data-general-social-survey",
    "title": "Lab 2: Logistic Regression",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "posts/Lab-2/index.html#eda",
    "href": "posts/Lab-2/index.html#eda",
    "title": "Lab 2: Logistic Regression",
    "section": "EDA",
    "text": "EDA\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\nFill in the “____” below to encode the binary variable\n\ndata &lt;- read.csv(\"gss2016.csv\")\n\ndata = data |&gt; \n  mutate(mass_trans_spend_right = if_else(natmass == \"About right\", 1, 0))\n\ndata |&gt; \n  DT::datatable()\n\n\n\n\n\n\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if needed\nmass_spend_long &lt;- mass_spend_summary %&gt;%\n  mutate(category = if_else(mass_trans_spend_right == 1, \"About right\", \"Not right\")) \n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\nggplot(mass_spend_summary, aes(x = factor(mass_trans_spend_right), y = proportion, fill = factor(mass_trans_spend_right))) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"),\n                   labels = c(\"Not right\", \"About right\")) +\n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Response\",\n       y = \"Proportion\",\n       fill = \"Spending View\") +\n  scale_x_discrete(labels = c(\"Not right\", \"About right\")) +\n  plot_aes\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                           levels = c(\"Extremely liberal\", \"Liberal\", \"Slightly liberal\", \n                                      \"Moderate\", \"Slghtly conservative\", \"Conservative\", \n                                      \"Extrmly conservative\"),\n                           ordered = TRUE))\n\n\nMake a plot of the distribution of polviews\n\n\n#Get proportions, format, and produce a plot like you did previously for mass_trans_spend_right\n\npol_view_summary &lt;- data %&gt;%\n  count(polviews) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(pol_view_summary, aes(x = polviews, y = proportion, fill = polviews)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = palette) +  # Removed extra parenthesis\n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Response\",\n       y = \"Proportion\",\n       fill = \"Spending View\") +\n  plot_aes\n\n\n\n\n\n\n\n\n\nWhich political view occurs most frequently in this data set?\n_____\n\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\n\n\ndata |&gt; \n  group_by(polviews) |&gt; \n  summarize(prop_satisfied = mean(mass_trans_spend_right), na.rn = T) |&gt; \n  ggplot(aes(x = polviews, y = prop_satisfied, fill = polviews)) + \n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = palette_condition) +  \n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Political Views\",\n       y = \"Proportion\\nSatisfied with Spending\",\n       fill = \"Spending View\") +\n  plot_aes\n\n\n\n\n\n\n\n\nThe more conservative one’s political views are the more they think the amount of spending on mass transportation is correct.\n\nWe’d like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as “89 or older”.\n\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values “89 or older” with a single value.\n\n\ndata = data |&gt; \n  mutate(age = if_else(age == \"89 or older\", \"89\", age)) |&gt; \n  mutate(age = as.numeric(age))\n\n\nPlot the frequency distribution of age.\n\n\ndata |&gt; \n  ggplot(aes(x = age)) + \n  geom_density(binwidth = 5, fill = \"#6195C6\") + \n  labs(title = \"Frequency Distribution of Age\",\n       x = \"Age\",\n       y = \"Frequency\") +\n  plot_aes"
  },
  {
    "objectID": "posts/Lab-2/index.html#logistic-regression",
    "href": "posts/Lab-2/index.html#logistic-regression",
    "title": "Lab 2: Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLet’s start by fitting a logistic regression model with just the intercept\n\n\nintercept_only_model &lt;- glm(mass_trans_spend_right ~ 1, data = data, family = binomial(link = \"logit\"))\n\nintercept_only_model %&gt;% \n  tidy() %&gt;%\n  DT::datatable()\n\n\n\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two–what is the basic thing this probability tells us about?\n\n\nb0 &lt;- coef(intercept_only_model)[\"(Intercept)\"]\n\n# Logistic transformation of the intercept (log-odds to probability)\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) \n\n# Compute the 95% confidence intervals on the log-odds scale\nci_lower &lt;- b0 - 1.96 * 0.0393685\nci_upper &lt;- b0 + 1.96 * 0.0393685\n\n# Transform the confidence intervals into probabilities\np_lower &lt;- exp(ci_lower) / (1 + exp(ci_lower))\np_upper &lt;- exp(ci_upper) / (1 + exp(ci_upper))\n\n# Print results\ncat(\"Intercept (probability):\", round(b0_transformed, 3), \"\\n\")\n\nIntercept (probability): 0.53 \n\ncat(\"95% CI (probability): [\", round(p_lower, 3), \",\", round(p_upper, 3), \"]\\n\")\n\n95% CI (probability): [ 0.51 , 0.549 ]\n\n\n\nThe the baseline probability of supporting the policy is 53%.\n\n\nNow let’s fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\n\ndata &lt;- data |&gt; \n  mutate(sex = as.factor(sex)) \ndata$sex &lt;- relevel(data$sex, ref = \"Male\")\n\n\nm1 &lt;- glm(mass_trans_spend_right ~ age +sex + sei10, data = data, family = binomial(link = \"logit\"))\n\nm1 %&gt;% \n  tidy() %&gt;%\n  DT::datatable()\n\n\n\n\n\n\nConsider the relationship between sex and one’s opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\nlist(\n  \"Model Coefficients\" = m1 %&gt;% tidy(),\n  \"Exponentiated Coefficients\" = m1 %&gt;% tidy(exponentiate = TRUE)\n) %&gt;%\n  purrr::map(DT::datatable)\n\n$`Model Coefficients`\n\n$`Exponentiated Coefficients`\n\n# Calculate confidence intervals for sexFemale coefficient\nbsex &lt;- coef(m1)[\"sexFemale\"]\nci_lower_lo &lt;- bsex - 1.96 * 0.0798020\nci_upper_lo &lt;- bsex + 1.96 * 0.0798020\n\n# Convert to odds ratios and calculate confidence intervals\nci_lower_or &lt;- exp(bsex - 1.96 * 0.0798020)\nci_upper_or &lt;- exp(bsex + 1.96 * 0.0798020)\n\n# Output the results\nlist(\n  \"CI for log-odds\" = c(ci_lower_lo, ci_upper_lo),\n  \"CI for Odds Ratio\" = c(ci_lower_or, ci_upper_or)\n)\n\n$`CI for log-odds`\n sexFemale  sexFemale \n0.09933194 0.41215578 \n\n$`CI for Odds Ratio`\nsexFemale sexFemale \n 1.104433  1.510070 \n\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\n\nemm_sex \n\n sex     prob     SE  df asymp.LCL asymp.UCL\n Male   0.495 0.0147 Inf     0.467     0.524\n Female 0.559 0.0133 Inf     0.533     0.585\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nIf you did this right, you’ll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n# Get the coefficient for age\nb_age &lt;- coef(m1)[\"age\"]\n\n# Compute the 95% CI for the coefficient of age in log-odds\nage_se &lt;- summary(m1)$coefficients[\"age\", \"Std. Error\"]\nci_lower_log_odds &lt;- b_age - 1.96 * age_se\nci_upper_log_odds &lt;- b_age + 1.96 * age_se\n\n# Convert log-odds to odds ratio (OR) by applying the logistic transformation\nor_age &lt;- exp(b_age)\n\n# Compute the 95% CI for the odds ratio\nci_lower_or &lt;- exp(ci_lower_log_odds)\nci_upper_or &lt;- exp(ci_upper_log_odds)\n\n# Create a data frame with the results\nresult_df &lt;- data.frame(\n  Metric = c(\"Coefficient (log-odds)\", \"95% CI for log-odds\", \"Odds Ratio\", \"95% CI for Odds Ratio\"),\n  Estimate = c(b_age, paste(round(ci_lower_log_odds, 3), \"to\", round(ci_upper_log_odds, 3)),\n               round(or_age, 3), paste(round(ci_lower_or, 3), \"to\", round(ci_upper_or, 3)))\n)\n\n# Display the results in an interactive datatable\nDT::datatable(result_df, options = list(pageLength = 5))\n\n\n\n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by -0.0062, holding all other variables constant. The odds ratio is 0.994, which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.994, or approximately 0.6% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one’s opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. ß\n\n\nbses &lt;- coef(m1)[\"sei10\"]\n\n\n# Compute the 95% CI for the coefficient of age in log-odds\nses_se &lt;- summary(m1)$coefficients[\"sei10\", \"Std. Error\"]\nci_lower_log_odds &lt;- bses - 1.96 * age_se\nci_upper_log_odds &lt;- bses + 1.96 * age_se\n\n# Convert log-odds to odds ratio (OR) by applying the logistic transformation\nor_age &lt;- exp(bses)\n\n# Compute the 95% CI for the odds ratio\nci_lower_or &lt;- exp(ci_lower_log_odds)\nci_upper_or &lt;- exp(ci_upper_log_odds)\n\n# Create a data frame with the results\nresult_df &lt;- data.frame(\n  Metric = c(\"Coefficient (log-odds)\", \"95% CI for log-odds\", \"Odds Ratio\", \"95% CI for Odds Ratio\"),\n  Estimate = c(b_age, paste(round(ci_lower_log_odds, 3), \"to\", round(ci_upper_log_odds, 3)),\n               round(or_age, 3), paste(round(ci_lower_or, 3), \"to\", round(ci_upper_or, 3)))\n)\n\n# Display the results in an interactive datatable\nDT::datatable(result_df, options = list(pageLength = 5))\n\n\n\n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0107, -0.0017]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998])."
  },
  {
    "objectID": "posts/Lab-2/index.html#marginal-effects",
    "href": "posts/Lab-2/index.html#marginal-effects",
    "title": "Lab 2: Logistic Regression",
    "section": "Marginal effects",
    "text": "Marginal effects\n\nLet’s examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\navg_comparisons(m1, comparison = \"difference\") %&gt;% \n  DT::datatable()\n\n\n\n\n\n\nThe marginal effect of age is -0.0015 (95% CI [-0.0026, -0.0004]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other factors constant (p = 0.0066).\nThe marginal effect of SES is -0.0015 (95% CI [-0.0023, -0.0007]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other variables constant.\nThe marginal effect for being female compared to male is 0.0631 (95% CI [0.0263, 0.1000]). This indicates that females are, on average, about 6.31 percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant."
  },
  {
    "objectID": "posts/Lab-2/index.html#model-comparison",
    "href": "posts/Lab-2/index.html#model-comparison",
    "title": "Lab 2: Logistic Regression",
    "section": "Model comparison",
    "text": "Model comparison\n\nNow let’s see whether a person’s political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\nfit2 &lt;- glm(mass_trans_spend_right ~age + sex + sei10, data = data, family = binomial(link = \"logit\"))\nfit3 &lt;- glm(mass_trans_spend_right ~ polviews + age + sex + sei10, data = data, family = binomial(link = \"logit\"))\n\ntest_likelihoodratio(fit2, fit3) %&gt;% kable()\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes. The model with polviews is significantly better than the model without it, as indicated by the likelihood ratio test (p &lt; 0.001)."
  },
  {
    "objectID": "posts/Lab-2/index.html#visualization",
    "href": "posts/Lab-2/index.html#visualization",
    "title": "Lab 2: Logistic Regression",
    "section": "Visualization",
    "text": "Visualization\n\nLet’s plot the results\nWe next use the model to produce visualizations:\n\nGiven the code below, interpet what is being plotted:\n\npol_plot : people that are extremely conservative are more likely to support mass transit spending\nsex_plot : women are more likely to support mass transit spending than men\nses_plot: people of lower ses are more likely to support mass transit spending\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nadjust the various settings in your plot to make it look professional.\nYou can use ggeffects to get the predicted probabilities for these models.\n\n\n\n\n\nlibrary(ggeffects)\n\n\n# Load the gridExtra package for arranging plots\nlibrary(gridExtra)\n\n# Plot for political views\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = palette) +\n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  plot_aes\n\n# Plot for sex\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  plot_aes\n\n# Plot for socioeconomic status\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\nses_plot &lt;- ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"red4\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"red4\", alpha = 0.2) +\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  plot_aes + theme(legend.position = \"none\")\n\n# Arrange the plots using grid.arrange\ngrid.arrange(pol_plot, sex_plot, ses_plot, ncol = 1)"
  },
  {
    "objectID": "posts/Lab-2/index.html#model-assumptions",
    "href": "posts/Lab-2/index.html#model-assumptions",
    "title": "Lab 2: Logistic Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nIs the logistic model a good choice for this data?\n\n\nbinned_residuals(fit2)\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: No, because only 86% of the residuals are inside the error bounds (~95% or higher would be good)."
  },
  {
    "objectID": "posts/Lab-2/index.html#model-fit",
    "href": "posts/Lab-2/index.html#model-fit",
    "title": "Lab 2: Logistic Regression",
    "section": "Model fit",
    "text": "Model fit\n\nCalculate the \\(R^2\\) for this model\n\n\nr2_mcfadden(fit2)\n\n# R2 for Generalized Linear Regression\n       R2: 0.010\n  adj. R2: 0.009\n\n\n\nR2 interpretation: The model accounts for 0.01% of the variance in the outcome variable, which is very low.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\nbinned_residuals(fit2, term=\"sei10\")\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\nbinned_residuals(fit2, term=\"age\")\n\nOk: About 98% of the residuals are inside the error bounds.\n\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYes, there are a few predictors that stick out. The residuals for the socioeconomic index (sei10) are not evenly distributed across the bins, indicating a non-linear relationship. To improve model fit, we could consider transforming the variable or using a different model that can capture non-linear relationships."
  },
  {
    "objectID": "posts/Lab-2/index.html#testing-polviews",
    "href": "posts/Lab-2/index.html#testing-polviews",
    "title": "Lab 2: Logistic Regression",
    "section": "Testing Polviews",
    "text": "Testing Polviews\n\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 0.37 times more likely to support mass transit spending compared to extremely liberals and 0.46 times more likely to support mass transit than liberals.\n\nExtreme liberals are 2.70 times more likely to support spending compared to conservatives, 2.53 times compared to moderates, and 2.34 times compared to slightly conservatives.\n\nExtremely conservatives are 3.82 times less likely to support mass spending than liberals and 2.08 times less likely than slightly liberals.\n\nLiberals are 2.03 times more likely to support spending than moderates and 1.88 times more likely than slightly conservatives.\n\n\nHow These Numbers Were Derived The reported odds ratios in the original output describe how much less likely a group is to support spending compared to another group. To express how much more likely one group is compared to another, we compute the inverse of the odds ratio:\n\n[ = ]"
  },
  {
    "objectID": "posts/Lab-2/index.html#conclusion",
    "href": "posts/Lab-2/index.html#conclusion",
    "title": "Lab 2: Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nPolitical views have the strongest effect on the dependent variable, given the largest deviance reduction. Age and sex also have a significant impact, with similar deviance reductions. Socioeconomic status (sei10) matters but has a smaller effect compared to other predictors.\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html",
    "title": "Lab 4: Multinomial Regression",
    "section": "",
    "text": "Lab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#data",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#data",
    "title": "Lab 4: Multinomial Regression",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\nlibrary(pacman)\npacman::p_load(nnet,car,tidyverse,emmeans,ggeffects,knitr,patchwork,broom,parameters,easystats,install = T)\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\n\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")\n\nvoter_data |&gt; \n  head() |&gt; \n  DT::datatable()"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#lrt",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#lrt",
    "title": "Lab 4: Multinomial Regression",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\nvoter_model_expanded |&gt; \n  tidy(conf.int = TRUE )|&gt; \n  mutate(across(where(is.numeric), round, 3)) |&gt; \nDT::datatable(options = list(pageLength = 10, scrollX = TRUE))"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-political-group---emmeans",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-political-group---emmeans",
    "title": "Lab 4: Multinomial Regression",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n#Get estimated marginal means from the model\n\n#using \nmultinomial_id&lt;- emmeans(voter_model_expanded, ~ pol_ident_new|voter_category)\n\n\ncoefs = contrast(regrid(multinomial_id, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\")  \n\ncontrast = sporadic - (rarely/never):\n pol_ident_new estimate     SE df t.ratio p.value\n Dem             0.9613 0.0701 28  13.722  &lt;.0001\n Indep           0.5909 0.0773 28   7.643  &lt;.0001\n Other           0.0782 0.0868 28   0.902  0.7475\n Rep             0.8832 0.0844 28  10.469  &lt;.0001\n\ncontrast = always - (rarely/never):\n pol_ident_new estimate     SE df t.ratio p.value\n Dem             0.4797 0.0738 28   6.498  &lt;.0001\n Indep          -0.0494 0.0838 28  -0.590  0.8999\n Other          -0.8353 0.1100 28  -7.577  &lt;.0001\n Rep             0.3269 0.0890 28   3.672  0.0037\n\nResults are averaged over the levels of: race, gender, income_cat, educ \nResults are given on the log (not the response) scale. \nP value adjustment: dunnettx method for 4 tests"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-of-education---emmeans",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-of-education---emmeans",
    "title": "Lab 4: Multinomial Regression",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n#Enter code\nmultinomial_edu &lt;- emmeans(voter_model_expanded, ~ educ|voter_category)\n\n\ncoefs = contrast(regrid(multinomial_edu, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\") \n\ncontrast = sporadic - (rarely/never):\n educ                estimate     SE df t.ratio p.value\n College                0.986 0.0764 28  12.904  &lt;.0001\n High school or less    0.187 0.0691 28   2.705  0.0313\n Some college           0.707 0.0744 28   9.512  &lt;.0001\n\ncontrast = always - (rarely/never):\n educ                estimate     SE df t.ratio p.value\n College                0.477 0.0800 28   5.960  &lt;.0001\n High school or less   -0.711 0.0800 28  -8.883  &lt;.0001\n Some college           0.167 0.0791 28   2.114  0.1117\n\nResults are averaged over the levels of: race, gender, income_cat, pol_ident_new \nResults are given on the log (not the response) scale. \nP value adjustment: dunnettx method for 3 tests \n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\npredictions &lt;- ggemmeans(voter_model_expanded, terms = c(\"age_centered\", \"pol_ident_new\"))\n\n# Create the plot with facets for each party ID category.\nggplot(predictions, aes(x = x, y = predicted, fill = response.level)) +\n  geom_area() +\n  geom_rug(sides = \"b\", position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"\\nAge\",\n    y = \"Predicted Probability\\n\",\n    title = \"Predicted Probabilities of Voting Frequency by Age and Party ID\"\n  ) +\n  facet_wrap(~ group, labeller = label_both) +  # Facet by Party ID\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"RARELY OR NEVER VOTE\", \"SOMETIMES VOTE\", \"ALMOST ALWAYS VOTE\"),\n    breaks = c(\"rarely/never\", \"sporadic\", \"always\")\n  ) +\n  plot_aes\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\npredictions &lt;- ggemmeans(voter_model_expanded, terms = c(\"educ\"))\n\n# Create the plot with facets for each party ID category.\nggplot(predictions, aes(x = x, y = predicted, fill = response.level)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.7) +  # Use stat = \"identity\" for bar heights\n  labs(\n    x = \"\\nEducation\",\n    y = \"Predicted Probability\\n\",\n    title = \"Predicted Probabilities of Voting Frequency by Age and Party ID\"\n  ) +\n  facet_wrap(~ group, labeller = label_both) +  # Facet by Party ID\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"RARELY OR NEVER VOTE\", \"SOMETIMES VOTE\", \"ALMOST ALWAYS VOTE\"),\n    breaks = c(\"rarely/never\", \"sporadic\", \"always\")\n  ) +\n  plot_aes"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#write-up",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#write-up",
    "title": "Lab 4: Multinomial Regression",
    "section": "Write-up",
    "text": "Write-up\n\nAge: The older people get the less likely they are to note vote, regardless of political idenitiy. However, younger, Independents are the least likely to vote the Education: People with a college education are more likely to sometimes and always vote that than counterpart, and less likely to never vote. On the other hand, people with a high school education or less are more likely to not engage in voting than their counterparts\n\n\nDifferences between political groups and voting behavior - Emmeans\n\nmulti_an &lt;- emmeans(voter_model_expanded, ~  pol_ident_new|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\nmulti_an &lt;- emmeans(voter_model_expanded, ~ educ|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\nEnter your interpretation here: &gt; The contrast analysis reveals significant differences in voting frequency based on educational attainment. Individuals with a high school education or less are less likely to vote sporadically or always compared to those with a college education. Those with some college education show mixed results; they are less likely to vote always compared to college graduates but more likely to vote sporadically than those with a high school education or less. Overall, these findings highlight the significant impact of education on voting behavior, indicating that higher educational attainment is associated with increased likelihood of electoral participation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats-blog (PSY-504)",
    "section": "",
    "text": "Lab 2: Logistic Regression\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4: Multinomial Regression\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Regression Lab Answers\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\nNo matching items"
  }
]