[
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "",
    "text": "This worksheet helps to give you a better idea about what to do with the trace plots."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#packages-and-data",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#packages-and-data",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "Packages and data",
    "text": "Packages and data\nLoad the primary packages.\n\nlibrary(pacman)\npacman::p_load(tidyverse, brms, tidybayes,\n               ggdist,bayesplot,moderndive,faux,GGally,ggmcmc,install = T)\nset.seed(42)\n\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nThis time we’ll simulate data with the faux package.\n\n# how many cases?\nn &lt;- 100\n\n# population values\nmu    &lt;- 0\nsigma &lt;- 1\nrho   &lt;- .5\n\n# simulate and save\nset.seed(1)\n\nd &lt;- rnorm_multi(\n  n = n,\n  mu = c(mu, mu),\n  sd = c(sigma, sigma), \n  r = rho, \n  varnames = list(\"x\", \"y\")\n)\n\nglimpse(d)\n\nRows: 100\nColumns: 2\n$ x &lt;dbl&gt; -0.232341576, 0.137981847, -0.268214782, 1.302539315, 0.612654423, -…\n$ y &lt;dbl&gt; -0.85270825, 0.18009772, -1.17913643, 1.46056809, -0.04193022, 0.173…\n\n\nWe might look at the data with a ggpairs() plot.\n\nd %&gt;% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", binwidth = 0.25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE))) + plot_aes\n\n\n\n\n\n\n\n\nCheck the sample statistics.\n\n# univariate\nd %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(m = mean(value),\n            s = sd(value))\n\n# A tibble: 2 × 3\n  name       m     s\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 x     0.113  0.914\n2 y     0.0754 0.913\n\n# bivariate\nd %&gt;% \n  summarise(r = cor(y, x))\n\n          r\n1 0.4502206"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#base-model",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#base-model",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "Base model",
    "text": "Base model\nLet’s fit a simple model\n\\[\n\\begin{align}\ny_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 x_i \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1),\n\\end{align}\n\\]\nAs we fit the model with brm(), take the opportunity to consider some of the default settings.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit13b.rds'\n\nif (!file.exists(model_path)) {\n\nfit13.b &lt;- brm(\n  data = d,\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 13,\n  \n  # default settings we've been ignoring up to this point\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n  # if you have a good computer, maybe try setting cores = 4\n)\nsaveRDS(fit13.b,model_path)\n} else {\n  fit13.b &lt;- readRDS(model_path)\n}\n\nIf you’d like to use multiple cores, but you’re not sure how many you have, execute parallel::detectCores().\n\nQuestion 1: How many cores do you have?\n\ncores = parallel::detectCores()\ncat(\"Steven's computer has:\",cores)\n\nSteven's computer has: 14\n\n\nCheck the model summary.\n\nsummary(fit13.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at the parameter posteriors in a pairs() plot.\n\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3)) \n\n\n\n\n\n\n\n\nThe pairs() plot is a wrapper around the mcmc_pairs() function from bayesplot. By default, half of the chains are depicted in the scatter plots below the diagonal, and the other half are displayed above the diagonal. The basic idea is you want the results form different chains to mirror one another. You can control this behavior with the condition argument.\n\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3),\n      # here we put the first chain in above the diagonal,\n      # and we put the second through fourth chains below the diagonal\n      condition = pairs_condition(chains = list(1, 2:4)))\n\n\n\n\n\n\n\n\nThis particular arrangement is a little silly, but it should give you a sense of how to control the output. Also, by default the histograms on the diagonal use the draws from all the chains.\nIf you wanted, you could also make a similar kind of plot with ggpairs().\n\nas_draws_df(fit13.b) %&gt;% \n  select(b_Intercept:sigma) %&gt;% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", bins = 25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE)),\n          lower = list(continuous = wrap(\"points\", size = 1/4, alpha = 1/3))) + plot_aes\n\n\n\n\n\n\n\n\nNow take a look at the plot() output.\n\nplot(fit13.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nThese trace plots look like a dream. They have the appearance of fuzzy caterpillars, which is why they’re even sometimes called caterpillar plots.\nLet’s work directly with the chains via as_draws_df().\n\nas_draws_df(fit13.b) %&gt;% \n  # notice the 3 meta-data columns at the end\n  glimpse()\n\nRows: 4,000\nColumns: 9\n$ b_Intercept &lt;dbl&gt; 0.023961191, 0.008764307, -0.066725637, -0.085990390, 0.01…\n$ b_x         &lt;dbl&gt; 0.5574909, 0.5822172, 0.4748096, 0.4267265, 0.4081808, 0.4…\n$ sigma       &lt;dbl&gt; 0.8455813, 0.8642759, 0.7653377, 0.9147409, 0.8412759, 0.7…\n$ Intercept   &lt;dbl&gt; 0.08707099, 0.07467320, -0.01297564, -0.03768355, 0.063654…\n$ lprior      &lt;dbl&gt; -2.842647, -2.874429, -2.716021, -2.844376, -2.764485, -2.…\n$ lp__        &lt;dbl&gt; -124.8296, -125.3474, -125.0186, -126.0295, -124.1495, -13…\n$ .chain      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ .iteration  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ .draw       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n\n\nWe can use those meta-data columns to make our own trace plots with ggplot functions.\n\nas_draws_df(fit13.b) %&gt;% \n  pivot_longer(b_Intercept:sigma) %&gt;% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for Greek formatted strip labels\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots!\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\nWe might restrict to the first few post-warmup iterations to help give us a better sense of what’s happening.\n\nas_draws_df(fit13.b) %&gt;% \n  filter(.iteration &lt; 21) %&gt;% \n  pivot_longer(b_Intercept:sigma) %&gt;% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for nice formatting\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots (zoomed in)\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\nNote that these are all post-warmup draws. The brms package doesn’t make it easy to visualize the warmup draws. But we can do so with a little help from the ggmcmc package’s ggs() function.\n\n# first execute without summarise()\nggs(fit13.b) %&gt;% \n  summarise(min = min(Iteration),\n            max = max(Iteration))\n\n# A tibble: 1 × 2\n    min   max\n  &lt;int&gt; &lt;int&gt;\n1     1  2000\n\n\nNote how how the values in the Iteration column range from 1 to 2,000. By brms default, the first 1,000 of those iterations are the warmup’s. Here is how we can use the ggs() output to make trace plots that include the warmup draws.\n\nggs(fit13.b) %&gt;% \n  filter(Parameter != \"lprior\") %&gt;% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  # this marks off the warmups\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  labs(title = \"More hand-made trace plots\",\n       subtitle = \"warmup/post-warmup by background\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\nLet’s take a closer look at the first few warmup iterations.\n\nggs(fit13.b) %&gt;% \n  filter(Parameter != \"lprior\") %&gt;% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 2/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  coord_cartesian(xlim = c(0, 50)) +\n  labs(title = \"More hand-made trace plots (zoomed in)\",\n       subtitle = \"warmup only\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Can you use the results here to describe the need for discarding warmup draws?\nThe warmup draws are not representative of the posterior distribution. The warmup draws are used to help the HMC algorithm find a good starting point for the post-warmup draws. The post-warmup draws are what we use to make inferences about the posterior distribution.\nAnother issue is autocorrelation, the degree to which a given HMC draw is correlated with the previous draw(s). We can make a plot of the autocorrelations with the mcmc_acf() function from the bayesplot package.\n\nfit13.b %&gt;% \n  mcmc_acf(pars = vars(b_Intercept, b_x, sigma),\n           lags = 10)  # lags = 20 is the default\n\n\n\n\n\n\n\n\nThis is what we like to see: Nice L-shaped autocorrelation plots. Low autocorrelations like this are one of the major achievements of Stan’s implementation of HMC. It’s not uncommon for MCMC via the older Gibbs sampler method to routinely show much higher autocorrelations. You can get a sense of this by comparing the various models in Kruschke’s (2015) textbook, which often uses the Gibbs sampler, versus their brms() analogues in my (2023) ebook translation.\n\n\n\n\n\n\nNote\n\n\n\nMixing describes how efficiently MCMC chains explore the posterior distribution. Good mixing means samples move freely across the parameter space. And high autocorrelation =&gt; poor mixing.\n\n\n\n\nQuestion 3: Why are L-shaped autocorrelation plots are desirable? What would an undesirable autocorrelation plot look like?\nL-shaped autocorrelation plots are desirable because they indicate that the MCMC chains are mixing well and exploring the parameter space efficiently. In contrast, an undesirable autocorrelation plot would show high autocorrelations at many lags, indicating that the samples are highly correlated and not effectively exploring the posterior distribution.\nThose low autocorrelations also have a lot to do with our effective sample size (ESS) estimates. Take another look at the summary() output.\n\nsummary(fit13.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere used to be a single ESS column. Starting with version 2.10.0, brms returns two columns: Bulk_ESS and Tail_ESS. These originate from Vehtari et al (2019). From the paper, we read:\n\nWhen reporting quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3, we show that convergence of Markov chains is not uniform across the parameter space, that is, convergence might be different in the bulk of the distribution (e.g., for the mean or median) than in the tails (e.g., for extreme quantiles). We propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as bulk-ESS), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (pp. 672-673)\n\nWe generally like the values in both the Bulk_ESS and Tail_ESS columns to be as close to the total number of post-warmup draws as possible, which would be 4,000 for a default brm() model. Sometimes, as in the case of the Bulk_ESS value for our \\(\\beta_1\\) parameter, the HMC chains are so efficient that we can get larger numbers than the actual number of post-warmup draws. This is related to when we have negative autocorrelations (see above).\nHow much is enough, and how low is too low? Yeah, indeed… Higher is generally better, with diminishing returns rolling in somewhere between 1,000 and 10,000. brms will give you a warning message when the ESS estimates get below a couple hundred.\nNow look back at the Rhat column in the summary() output. This is the potential scale reduction factor \\(\\hat R\\). It has its origins in Gelman & Rubin (1992), but the current version used in brms is from Vehtari et al (2019), as cited above. In short, it is something of a ratio of the between-chain variation versus the within-chain variation. This ratio is usually a little above 1, and we want it to be as close to 1 as possible. The Stan team (e.g., https://mc-stan.org/rstan/reference/Rhat.html) recommends against values greater than 1.05. In our case, we’re good to go."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#what-bad-chains-look-like..",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#what-bad-chains-look-like..",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "What bad chains look like..",
    "text": "What bad chains look like..\nNow let’s break the model. This time, we’ll subset the d data to just the first 2 rows, we’ll make the priors very wide on the scale of the data, and we’ll dramatically reduce the warmup period.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit14b.rds'\n\nif (!file.exists(model_path)) {\n\nfit14.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this for real data analyses\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 1100, warmup = 100, chains = 4, cores = 10\n)\n\nsaveRDS(fit14.b,model_path)\n} else {\n  fit14.b &lt;- readRDS(model_path)\n}\n\nCheck the parameter summary.\n\nprint(fit14.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %&gt;% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 1100; warmup = 100; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   478.83   2036.34 -1979.74  6652.81 1.46        8       13\nx         -1228.64   3124.10 -6776.94  5668.31 1.88        6       15\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma  6231.76  11923.16   165.35 42287.98 1.13       22      336\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNever ignore Warning messages like that.\nThose Rhat, Bulk_ESS, and Tail_ESS look really bad. Also notice how large the posterior means (Estimate) and standard deviations (Est.Error) are. Seems off, eh?\nLet’s investigate further with a pairs() plot.\n\nplot(fit14.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nThis is a full-scale disaster. DO NOT trust model results from chains that look like this.\nIn this case, just giving the model a longer warmup period helped a lot.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit15b.rds'\n\nif (!file.exists(model_path)) {\n\nfit15.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this in real life\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 10\n)\n\nsaveRDS(fit15.b,model_path)\n} else {\n  fit15.b &lt;- readRDS(model_path)\n}\n\n\nplot(fit15.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nWe still have a lot of Warning messages, but things have improved.\nWe can do an even better with default weakly-regularizing priors.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit16b.rds'\n\nif (!file.exists(model_path)) {\n\nfit16.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n)\n\nsaveRDS(fit16.b,model_path)\n} else {\n  fit16.b &lt;- readRDS(model_path)\n}\n\n\nplot(fit16.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nNow look at the parameter summaries.\n\nprint(fit16.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %&gt;% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.23      0.49    -1.21     0.83 1.00     1810     1643\nx             0.47      0.99    -1.46     2.38 1.00     1955     1932\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.86      0.59     0.19     2.45 1.00     1065      982\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThose Warning messages still remain, but they’re less dire than before. Also, most of the other diagnostics look better. I still wouldn’t trust this model. It is only based on 2 data points, after all. But look how far we got by paying attention to the diagnostics and picking better priors."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#references",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#references",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "References",
    "text": "References\nGelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences. Statistical Science, 7(4):457–472. https://dx.doi.org/10.1214/ss/1177011136\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\nKurz, A. S. (2023). Doing Bayesian data analysis in brms and the tidyverse (Version 1.1.0). https://bookdown.org/content/3686/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved \\(\\widehat R\\) for assessing convergence of MCMC (with discussion). Bayesian Analysis, 16(2), 667-718. https://doi.org/10.1214/20-BA1221"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#session-information",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#session-information",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "Session information",
    "text": "Session information\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggmcmc_1.5.1.1   GGally_2.2.1     faux_1.2.2       moderndive_0.7.0\n [5] bayesplot_1.11.1 ggdist_3.3.2     tidybayes_3.0.7  brms_2.21.0     \n [9] Rcpp_1.0.13      lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.4      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  pacman_0.5.1    \n\nloaded via a namespace (and not attached):\n  [1] infer_1.0.7          RColorBrewer_1.1-3   tensorA_0.36.2.1    \n  [4] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [7] estimability_1.5.1   farver_2.1.2         nloptr_2.1.1        \n [10] rmarkdown_2.28       vctrs_0.6.5          minqa_1.2.8         \n [13] base64enc_0.1-3      janitor_2.2.1        htmltools_0.5.8.1   \n [16] distributional_0.5.0 curl_5.2.2           broom_1.0.7         \n [19] StanHeaders_2.32.10  htmlwidgets_1.6.4    plyr_1.8.9          \n [22] emmeans_1.10.7       zoo_1.8-12           igraph_2.0.3        \n [25] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n [28] colourpicker_1.3.0   Matrix_1.7-0         R6_2.5.1            \n [31] fastmap_1.2.0        rbibutils_2.3        shiny_1.9.1         \n [34] snakecase_0.11.1     digest_0.6.37        colorspace_2.1-1    \n [37] crosstalk_1.2.1      labeling_0.4.3       fansi_1.0.6         \n [40] timechange_0.3.0     abind_1.4-5          compiler_4.4.1      \n [43] withr_3.0.1          backports_1.5.0      inline_0.3.19       \n [46] shinystan_2.6.0      ggstats_0.9.0        QuickJSR_1.3.1      \n [49] pkgbuild_1.4.4       MASS_7.3-60.2        gtools_3.9.5        \n [52] loo_2.8.0            tools_4.4.1          httpuv_1.6.15       \n [55] threejs_0.3.3        glue_1.8.0           nlme_3.1-164        \n [58] promises_1.3.0       grid_4.4.1           checkmate_2.3.2     \n [61] reshape2_1.4.4       generics_0.1.3       operator.tools_1.6.3\n [64] gtable_0.3.5         tzdb_0.4.0           formula.tools_1.7.1 \n [67] hms_1.1.3            utf8_1.2.4           pillar_1.9.0        \n [70] markdown_1.13        posterior_1.6.0      later_1.3.2         \n [73] splines_4.4.1        lattice_0.22-6       survival_3.6-4      \n [76] tidyselect_1.2.1     miniUI_0.1.1.1       knitr_1.48          \n [79] reformulas_0.4.0     arrayhelpers_1.1-0   gridExtra_2.3       \n [82] V8_6.0.1             stats4_4.4.1         xfun_0.52           \n [85] rstanarm_2.32.1      bridgesampling_1.1-2 matrixStats_1.4.1   \n [88] DT_0.33              rstan_2.32.6         stringi_1.8.4       \n [91] yaml_2.3.10          boot_1.3-30          evaluate_1.0.0      \n [94] codetools_0.2-20     cli_3.6.4            RcppParallel_5.1.9  \n [97] shinythemes_1.2.0    xtable_1.8-4         Rdpack_2.6.2        \n[100] munsell_0.5.1        coda_0.19-4.1        svUnit_1.0.6        \n[103] parallel_4.4.1       rstantools_2.4.0     dygraphs_1.1.1.6    \n[106] Brobdingnag_1.2-9    lme4_1.1-36          viridisLite_0.4.2   \n[109] mvtnorm_1.3-1        scales_1.3.0         xts_0.14.1          \n[112] rlang_1.1.5          shinyjs_2.1.0"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html",
    "title": "Priors and Predicitive Checks",
    "section": "",
    "text": "During the first Bayes Lab you considered exploratory data analysis, compared default brms with lm(), and extracted posteriors after fitting models. You summarized posterior distributions and also generated a distribution of predictions using these posterior draws.\n\nDuring the second Bayes lab, you looked at the different types of distributions that are relevant for Bayesian analysis, including priors.\nDuring today’s lab, you will go into prior predictive checks and some HMC diagnostics. While we look at the simple linear modeling case, this workflow is relevant for all Bayesian models."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#setup-packages-and-data",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#setup-packages-and-data",
    "title": "Priors and Predicitive Checks",
    "section": "Setup: Packages and data",
    "text": "Setup: Packages and data\nLoad the primary packages.\n\nlibrary(pacman)\npacman::p_load(tidyverse, brms, tidybayes,\n               ggdist,bayesplot,moderndive,faux,GGally,ggmcmc,install = T)\nset.seed(42)\n\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nThis time we’ll be taking data from the moderndive package. We want the evals data set.\n\ndata(evals, package = \"moderndive\")\n\nThe evals data were originally in the paper by Hamermesh and Parker (2005; https://doi.org/10.1016/j.econedurev.2004.07.013). You can learn more about the data like this:\n\n?moderndive::evals\n\nYou can learn even more information about the data from https://www.openintro.org/data/index.php?data=evals.\nAnyway, we need to subset the data.\n\nevals94 &lt;- evals %&gt;% \n  group_by(prof_ID) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\nglimpse(evals94) |&gt; \n  DT::datatable()\n\nRows: 94\nColumns: 14\n$ ID           &lt;int&gt; 1, 5, 8, 10, 18, 24, 31, 36, 43, 50, 60, 63, 68, 75, 79, …\n$ prof_ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ score        &lt;dbl&gt; 4.7, 4.6, 4.1, 4.5, 4.8, 4.4, 4.4, 3.4, 4.8, 4.0, 3.6, 4.…\n$ age          &lt;int&gt; 36, 59, 51, 40, 31, 62, 33, 51, 33, 47, 35, 37, 42, 49, 3…\n$ bty_avg      &lt;dbl&gt; 5.000, 3.000, 3.333, 3.167, 7.333, 5.500, 4.167, 4.000, 4…\n$ gender       &lt;fct&gt; female, male, male, female, female, male, female, female,…\n$ ethnicity    &lt;fct&gt; minority, not minority, not minority, not minority, not m…\n$ language     &lt;fct&gt; english, english, english, english, english, english, eng…\n$ rank         &lt;fct&gt; tenure track, tenured, tenured, tenured, tenure track, te…\n$ pic_outfit   &lt;fct&gt; not formal, not formal, not formal, not formal, not forma…\n$ pic_color    &lt;fct&gt; color, color, color, color, color, color, color, color, c…\n$ cls_did_eval &lt;int&gt; 24, 17, 55, 40, 42, 182, 33, 25, 48, 16, 18, 30, 28, 30, …\n$ cls_students &lt;int&gt; 43, 20, 55, 46, 48, 282, 41, 41, 60, 19, 25, 34, 40, 36, …\n$ cls_level    &lt;fct&gt; upper, upper, upper, upper, upper, upper, upper, upper, u…"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#intercept-only-model",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#intercept-only-model",
    "title": "Priors and Predicitive Checks",
    "section": "Intercept-only model",
    "text": "Intercept-only model\nLet’s start by fitting an intercept-only model\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???},\n\\end{align}\n\\]\nwhere \\(\\beta_0\\) is the same as the unconditional population mean, and the population standard deviation is \\(\\sigma\\). Our next task will be choosing our priors.\n\nQuestion 1: Why have we left some of the specification above unfilled / with questions marks at this point?\n[[Answer: We have left some of the specifcation abvoe unfilled because we have not ovserved the data yet and therefore do not have priors]]\n\n\nVisualize possible prior distributions.\nIn this exercise, we’ll choose the priors together. Let’s start with prior on \\(\\beta_0\\). Below are a few candidate distributions visualized with ggdist and friends.\n\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(8, 2)),\n  prior(normal(5.5, 2))\n) %&gt;% \n  parse_dist() %&gt;% \n\n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper boundaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL) + plot_aes\n\n\n\n\n\n\n\n\nThe red lines in the figures (shown at x=1 and x=10) represent the lower and upper boundaries for the beauty ratings scale used in the study. With the simple intercept model, setting a prior on the intercept parameter is the same as setting a prior on the expected mean in observation space.\nNow let’s visualize a few potential priors for \\(\\sigma\\).\n\nc(\n  prior(exponential(1)), \n  prior(normal(0, 1), lb = 0), \n  prior(normal(2, 0.3), lb = 0)\n) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  xlab(expression(italic(p)(sigma))) +\n  ylab(NULL) + plot_aes\n\n\n\n\n\n\n\n\n\nQuestion 2: Given that \\(\\sigma\\) refers to the standard deviation, are these three priors theoretically possible? If yes, give an example of a theoretically impossible prior for \\(\\sigma\\).\n[[Answer: Yes, these three priors are theoretically possible. A theoretically impossible prior for sigma would be a normal distribution with a mean of 0 and a standard deviation of 1. This is because the standard deviation cannot be negative.]]\n\n\n\nPrior-predictive checks (by hand).\nNote: It’s possible we’ll need the truncnorm::rtruncnorm() function in this section. Once we have candidate priors for both \\(\\beta_0\\) and \\(\\sigma\\), we can simulate values from those priors and plot the implied distributions.\n\n# how many distributions do you want?\nn &lt;- 50\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n\n\n\n\n\n\n\n\nThe simulated values constitute predictions that are made using our prior beliefs (a prior is set for beta0 and another for sigma) When you check if these predictions (prior predictive) make sense or not, it is called the prior predictive check. The point of the prior predictive check is to iterate on specifying the priors until the prior predictive is sensible/satisfactory.\n(Again, the red boundaries denote that the only possible bty_avg values are between 1 and 10.)\n\nQuestion 3: Can Explain what the section of the previous command, before ggplot is doing?\n[[This code generates a dataset of normal distribution densities across a range of bty_avg values for n randomly sampled (mean = beta0, sd = sigma) parameter pairs.]]\n\n\nQuestion 4: The prior predictive above is for one combination of our candidate priors. Why don’t you also try the \\(\\beta_0\\) prior centered at 8, along with the \\(\\sigma\\) prior centered at 2? What do you observe? Among these two , which would you pick? And why? (Optional: try others too if you’d like)\n[[The second prior (with (_0) centered at 8 and () centered at 2) shifts the predictions higher and makes them more spread out; I would pick the one that best matches what I expect in the real data—if I expect higher and more variable values, I’d go with this new one.]]\n\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the simulation reproducible?\n# set.seed(1)\n\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 8, sd = 2),\n       sigma = rnorm(n = n, mean = 2, sd = 0.3)) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n\n\n\n\n\n\n\n\n\n\n\nFit the model that you prefer\nWe should practice writing out our model equation with our priors of choice:\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(8,\\ 2) \\\\\n\\sigma & \\sim \\operatorname{Normal}(2,\\ 0.3)\n\\end{align}\n\\]\nLet’s fit a model with our priors of choice.\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit9b.rds\")\n\nif (!file.exists(model_path)) {\nfit9.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  # make sure we're settled on our priors \n  # we don't need to use these; they're placeholders\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma)\n)\n  saveRDS(fit9.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit9.b &lt;- readRDS(model_path)\n}\n\nCheck the model summary.\n\nsummary(fit9.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.63      0.17     4.29     4.95 1.00     2964     2310\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.60      0.12     1.39     1.85 1.00     3568     2545\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we might do a posterior predictive check to see how well our model describes the data.\n\nset.seed(1)\npp_check(fit9.b, ndraws = 100) +\n  ggtitle(\"posterior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit9.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\")  +\n  ggtitle(\"posterior predictive check\") + plot_aes\n\n\n\n\n\n\n\n\nOur simple Gaussian model doesn’t do a great job respecting the lower and upper boundaries, but this is about as good as it gets when you’re in Gaussian land. On the whole, the model did a pretty okay reproducing the gross features of the distribution of the sample data.\n\nQuestion 5: To ensure you’ve understood things well, can you write below about the difference between the prior predictive check and the posterior predictive check? How do they differ in their objectives?\n[[The prior predictive check is used to evaluate the plausibility of the prior distributions before observing the data, while the posterior predictive check is used to evaluate how well the model fits the observed data after accounting for the priors and likelihood. The former focuses on the prior beliefs, while the latter focuses on the model’s performance with actual data.]]"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#prior-predictive-checks-by-sample_prior-only",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#prior-predictive-checks-by-sample_prior-only",
    "title": "Priors and Predicitive Checks",
    "section": "Prior-predictive checks (by sample_prior = \"only\")",
    "text": "Prior-predictive checks (by sample_prior = \"only\")\nWe can also sample from the prior predictive distribution from brm() itself. To do so, we use the sample_prior argument, which has the following options:\n\n\"no\", which is the default, and does not sample from the prior;\n\"yes\",, which will sample from both the prior and the posterior; and\n\"only\", which will only sample from the prior.\n\nLet’s set sample_prior = \"only\".\n\n# check to see if we want to use other priors\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit10b.rds\")\n\nif (!file.exists(model_path)) {\nfit10.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  # we can set our seed, too!\n  seed = 1\n)\nsaveRDS(fit10.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit10.b &lt;- readRDS(model_path)\n}\n\nDid you notice how we used the seed argument? This makes the results reproducible.\nNow the summary() function only returns summaries for the priors, NOT the posterior.\n\nsummary(fit10.b)  # this summarizes the prior\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     5.50      1.00     3.61     7.49 1.00     1876     1938\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.00      1.01     0.03     3.63 1.00     1957     1424\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe as_draws_df() function also returns draws from the prior.\n\nas_draws_df(fit10.b) %&gt;% \n  head() |&gt; \n  DT::datatable()\n\n\n\n\n\nHere’s how we might use that as_draws_df() output to make a similar plot to the one we made before.\n\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the results reproducible?\n# set.seed(1)\n\nas_draws_df(fit10.b) %&gt;% \n  \n  # subset\n  slice_sample(n = n) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  # notice we're defining the mean by b_Intercept\n  mutate(density = dnorm(x = bty_avg, mean = b_Intercept, sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = bty_avg, y = density, \n             # notice we're grouping by .draw\n             group = .draw)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n\n\n\n\n\n\n\n\nWe can also use functions like pp_check() to compare the prior to the sample data.\n\nset.seed(1)\npp_check(fit10.b, ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit10.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#univariable-predictor-model",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#univariable-predictor-model",
    "title": "Priors and Predicitive Checks",
    "section": "Univariable predictor model",
    "text": "Univariable predictor model\nNow we’ll add gender as the sole predictor in the model,\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{gender}_i \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\beta_1 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???}.\n\\end{align}\n\\]\nLet’s try these same set of \\(\\beta_0\\) priors\n\n# change as needed\n\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(7, 0.5)),\n  prior(normal(5.5, 2))\n) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper bondaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL) + plot_aes\n\n\n\n\n\n\n\n\nNow we update our by-hand prior predictive simulation to accomodate \\(\\beta_0\\) and \\(\\beta_1\\).\n\nn &lt;- 50\n\nset.seed(1)\n\ntibble(iter = 1:n,\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       # notice our new line\n       beta1 = rnorm(n = n, mean = 0, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %&gt;% \n  # we have a new expand_grid() line\n  # make sure everyone understands this coding scheme\n  expand_grid(gendermale = 0:1) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  # notice the updated mean formula\n  mutate(density = dnorm(x = bty_avg, \n                         mean = beta0 + beta1 * gendermale, \n                         sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~ and~italic(p)(beta[1])~and~italic(p)(sigma))) +\n  facet_wrap(~ gendermale, labeller = label_both) + plot_aes\n\n\n\n\n\n\n\n\nBefore we fit the model, let’s practice the sample_prior = \"only\" approach.\n\n# check to see if we want to use other priors\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit11b.rds'\n\nif (!file.exists(model_path)) {\n\nfit11.b = brm(\n  data = evals94,\n  family = gaussian,\n  # notice the 0 + Intercept syntax\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  seed = 2\n)\nsaveRDS(fit11.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit11.b &lt;- readRDS(model_path)\n}\n\nCheck the prior summary.\n\nsummary(fit11.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      5.46      1.00     3.53     7.43 1.00     3147     2820\ngendermale     0.02      0.99    -1.91     1.99 1.00     4109     2946\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.02      0.98     0.03     3.66 1.00     3149     1903\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompare the prior with the data with pp_check().\n\nset.seed(1)\npp_check(fit11.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit11.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\n\nThere isn’t a great grouped histogram option for pp_check(), so we experimented with type = \"freqpoly_grouped\" instead.\nIf we wanted, we could also use the predict() function to simulate bty_avg values from the priors.\n\n# walk through this slowly\n\nset.seed(1)\n\npredict(fit11.b,\n        summary = FALSE,\n        ndraws = 5) %&gt;% \n  str()\n\n num [1:5, 1:94] 5.37 7.82 3.87 4.89 6.06 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\n\n# customize the predictor grid, as desired\nnd &lt;- tibble(gender = rep(c(\"female\", \"male\"), each = 50)) %&gt;% \n  # this will make it easier to connect the nd data to the predict() output\n  mutate(row = 1:n())\n\nset.seed(1)\n\npredict(fit11.b,\n        newdata = nd,\n        summary = FALSE,\n        ndraws = 5) %&gt;% \n  data.frame() %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(row = str_remove(name, \"X\") %&gt;% as.double()) %&gt;% \n  left_join(nd, by = \"row\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 0.5, boundary = 1) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  facet_grid(draw ~ gender, labeller = label_both) + plot_aes\n\n\n\n\n\n\n\n\nOnce we’ve settled on our priors, we should once again practice writing out the full model equation:\n\\[\n\\begin{align} \\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i & = \\beta_0 + \\beta_1 \\cdot \\text{gender(male)}_i \\\\ \\beta_0 & \\sim \\operatorname{Normal}(5.46,\\ 1.00) \\\\ \\beta_1 & \\sim \\operatorname{Normal}(0.02,\\ 0.99) \\\\ \\sigma & \\sim \\operatorname{Normal}(1.02,\\ 0.98) \\end{align}\n\\]\nOkay, let’s fit the real model.\n\n# check to see if we want to use other priors\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit12b.rds'\n\nif (!file.exists(model_path)) {\n\nfit12.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  \n  # yes, you can set your seed for your posteriors, too\n  # this makes the results reproducible\n  seed = 3\n)\nsaveRDS(fit12.b,model_path)\n} else {\n  fit12.b &lt;- readRDS(model_path)\n}\n\nCheck the model summary.\n\nsummary(fit12.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      4.93      0.24     4.48     5.40 1.00     2134     2345\ngendermale    -0.54      0.31    -1.14     0.06 1.00     2050     2218\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.59      0.12     1.38     1.83 1.00     2431     2371\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHow does the posterior-predictive check look?\n\nset.seed(1)\npp_check(fit12.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12)) +\n  ggtitle(\"posterior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit12.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\n\n\nQuestion 6: Does the posterior predictive check look satsifactory to you?\n[[The posterior predictive check looks satisfactory to me. The model seems to be able to capture the distribution of the data well, and the red lines marking the boundaries are respected.]]\n\n\n\n\n\n\nNote\n\n\n\nFor more on prior predictive checks, see McElreath (from Chapter 4), and Solomon Kurz’s brms/tidverse implementations as well.\nFor a comprehensive guide to set priors for a given situation, look at reccomendations made by the Stan team https://github.com/stan-dev/stan/wiki/prior-choice-recommendations\nThey generally recommend against uniform priors on \\(\\beta\\) and \\(\\sigma\\) parameters. This is based on a general principle that you should not use a prior that places an artificial boundary on a parameter.\nE.g. \\(\\sigma\\) parameters have natural lower boundaries at zero, but they don’t have upper boundaries. Thus, a uniform prior adds an unnatural upper boundary. A better prior would be something that is weakly informative"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#references",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#references",
    "title": "Priors and Predicitive Checks",
    "section": "References",
    "text": "References\nHamermesh, D. S., & Parker, A. (2005). Beauty in the classroom: Instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24(4), 369-376. https://doi.org/10.1016/j.econedurev.2004.07.013\nKurz, A. S. (2023). Statistical Rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.4.0). https://bookdown.org/content/4857/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#session-information",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#session-information",
    "title": "Priors and Predicitive Checks",
    "section": "Session information",
    "text": "Session information\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggmcmc_1.5.1.1   GGally_2.2.1     faux_1.2.2       moderndive_0.7.0\n [5] bayesplot_1.11.1 ggdist_3.3.2     tidybayes_3.0.7  brms_2.21.0     \n [9] Rcpp_1.0.13      lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.4      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  pacman_0.5.1    \n\nloaded via a namespace (and not attached):\n  [1] infer_1.0.7          RColorBrewer_1.1-3   tensorA_0.36.2.1    \n  [4] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [7] estimability_1.5.1   farver_2.1.2         nloptr_2.1.1        \n [10] rmarkdown_2.28       vctrs_0.6.5          minqa_1.2.8         \n [13] base64enc_0.1-3      janitor_2.2.1        htmltools_0.5.8.1   \n [16] distributional_0.5.0 curl_5.2.2           broom_1.0.7         \n [19] sass_0.4.9           StanHeaders_2.32.10  bslib_0.8.0         \n [22] htmlwidgets_1.6.4    plyr_1.8.9           emmeans_1.10.7      \n [25] zoo_1.8-12           cachem_1.1.0         igraph_2.0.3        \n [28] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n [31] colourpicker_1.3.0   Matrix_1.7-0         R6_2.5.1            \n [34] fastmap_1.2.0        rbibutils_2.3        shiny_1.9.1         \n [37] snakecase_0.11.1     digest_0.6.37        numDeriv_2016.8-1.1 \n [40] colorspace_2.1-1     crosstalk_1.2.1      labeling_0.4.3      \n [43] fansi_1.0.6          timechange_0.3.0     abind_1.4-5         \n [46] compiler_4.4.1       withr_3.0.1          backports_1.5.0     \n [49] inline_0.3.19        shinystan_2.6.0      ggstats_0.9.0       \n [52] QuickJSR_1.3.1       pkgbuild_1.4.4       MASS_7.3-60.2       \n [55] gtools_3.9.5         loo_2.8.0            tools_4.4.1         \n [58] httpuv_1.6.15        threejs_0.3.3        glue_1.8.0          \n [61] nlme_3.1-164         promises_1.3.0       grid_4.4.1          \n [64] checkmate_2.3.2      reshape2_1.4.4       generics_0.1.3      \n [67] operator.tools_1.6.3 gtable_0.3.5         tzdb_0.4.0          \n [70] formula.tools_1.7.1  hms_1.1.3            utf8_1.2.4          \n [73] pillar_1.9.0         markdown_1.13        posterior_1.6.0     \n [76] later_1.3.2          splines_4.4.1        lattice_0.22-6      \n [79] survival_3.6-4       tidyselect_1.2.1     miniUI_0.1.1.1      \n [82] knitr_1.48           reformulas_0.4.0     arrayhelpers_1.1-0  \n [85] gridExtra_2.3        V8_6.0.1             stats4_4.4.1        \n [88] xfun_0.52            rstanarm_2.32.1      bridgesampling_1.1-2\n [91] matrixStats_1.4.1    DT_0.33              rstan_2.32.6        \n [94] stringi_1.8.4        yaml_2.3.10          boot_1.3-30         \n [97] evaluate_1.0.0       codetools_0.2-20     cli_3.6.4           \n[100] RcppParallel_5.1.9   Rdpack_2.6.2         shinythemes_1.2.0   \n[103] xtable_1.8-4         munsell_0.5.1        jquerylib_0.1.4     \n[106] coda_0.19-4.1        svUnit_1.0.6         parallel_4.4.1      \n[109] rstantools_2.4.0     dygraphs_1.1.1.6     Brobdingnag_1.2-9   \n[112] lme4_1.1-36          mvtnorm_1.3-1        scales_1.3.0        \n[115] xts_0.14.1           rlang_1.1.5          shinyjs_2.1.0"
  }
]