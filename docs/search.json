[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi!\nMy name is Steven Mesquiti! I’m a first-year in the Department of Psychology at Princeton University advised by Erik Nook.\nI study how we can use Natural Language Processing techniques and Artifical Intelligence to explain, predict, and improve people’s Mental Health.\nBefore Princeton, I worked as a Lab Manager for Emily Falk in the Communication Neuroscience Lab at the Univeristy of Pennsylvania. Along the way, I have worked with excessively generous scientists like Jamie Pennebaker, Lyle Ungar, & Angela Duckworth pursing questions at the nexus of Computer Science, Computational Linguistics, and Psychology\nThis is my blog for PSY-504. You can find various assignments that I complete on this website."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html",
    "title": "Priors and Predicitive Checks",
    "section": "",
    "text": "During the first Bayes Lab you considered exploratory data analysis, compared default brms with lm(), and extracted posteriors after fitting models. You summarized posterior distributions and also generated a distribution of predictions using these posterior draws.\n\nDuring the second Bayes lab, you looked at the different types of distributions that are relevant for Bayesian analysis, including priors.\nDuring today’s lab, you will go into prior predictive checks and some HMC diagnostics. While we look at the simple linear modeling case, this workflow is relevant for all Bayesian models."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#setup-packages-and-data",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#setup-packages-and-data",
    "title": "Priors and Predicitive Checks",
    "section": "Setup: Packages and data",
    "text": "Setup: Packages and data\nLoad the primary packages.\n\nlibrary(pacman)\npacman::p_load(tidyverse, brms, tidybayes,\n               ggdist,bayesplot,moderndive,faux,GGally,ggmcmc,install = T)\nset.seed(42)\n\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nThis time we’ll be taking data from the moderndive package. We want the evals data set.\n\ndata(evals, package = \"moderndive\")\n\nThe evals data were originally in the paper by Hamermesh and Parker (2005; https://doi.org/10.1016/j.econedurev.2004.07.013). You can learn more about the data like this:\n\n?moderndive::evals\n\nYou can learn even more information about the data from https://www.openintro.org/data/index.php?data=evals.\nAnyway, we need to subset the data.\n\nevals94 &lt;- evals %&gt;% \n  group_by(prof_ID) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\nglimpse(evals94) |&gt; \n  DT::datatable()\n\nRows: 94\nColumns: 14\n$ ID           &lt;int&gt; 1, 5, 8, 10, 18, 24, 31, 36, 43, 50, 60, 63, 68, 75, 79, …\n$ prof_ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ score        &lt;dbl&gt; 4.7, 4.6, 4.1, 4.5, 4.8, 4.4, 4.4, 3.4, 4.8, 4.0, 3.6, 4.…\n$ age          &lt;int&gt; 36, 59, 51, 40, 31, 62, 33, 51, 33, 47, 35, 37, 42, 49, 3…\n$ bty_avg      &lt;dbl&gt; 5.000, 3.000, 3.333, 3.167, 7.333, 5.500, 4.167, 4.000, 4…\n$ gender       &lt;fct&gt; female, male, male, female, female, male, female, female,…\n$ ethnicity    &lt;fct&gt; minority, not minority, not minority, not minority, not m…\n$ language     &lt;fct&gt; english, english, english, english, english, english, eng…\n$ rank         &lt;fct&gt; tenure track, tenured, tenured, tenured, tenure track, te…\n$ pic_outfit   &lt;fct&gt; not formal, not formal, not formal, not formal, not forma…\n$ pic_color    &lt;fct&gt; color, color, color, color, color, color, color, color, c…\n$ cls_did_eval &lt;int&gt; 24, 17, 55, 40, 42, 182, 33, 25, 48, 16, 18, 30, 28, 30, …\n$ cls_students &lt;int&gt; 43, 20, 55, 46, 48, 282, 41, 41, 60, 19, 25, 34, 40, 36, …\n$ cls_level    &lt;fct&gt; upper, upper, upper, upper, upper, upper, upper, upper, u…"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#intercept-only-model",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#intercept-only-model",
    "title": "Priors and Predicitive Checks",
    "section": "Intercept-only model",
    "text": "Intercept-only model\nLet’s start by fitting an intercept-only model\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???},\n\\end{align}\n\\]\nwhere \\(\\beta_0\\) is the same as the unconditional population mean, and the population standard deviation is \\(\\sigma\\). Our next task will be choosing our priors.\n\nQuestion 1: Why have we left some of the specification above unfilled / with questions marks at this point?\n[[Answer: We have left some of the specifcation abvoe unfilled because we have not ovserved the data yet and therefore do not have priors]]\n\n\nVisualize possible prior distributions.\nIn this exercise, we’ll choose the priors together. Let’s start with prior on \\(\\beta_0\\). Below are a few candidate distributions visualized with ggdist and friends.\n\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(8, 2)),\n  prior(normal(5.5, 2))\n) %&gt;% \n  parse_dist() %&gt;% \n\n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper boundaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL) + plot_aes\n\n\n\n\n\n\n\n\nThe red lines in the figures (shown at x=1 and x=10) represent the lower and upper boundaries for the beauty ratings scale used in the study. With the simple intercept model, setting a prior on the intercept parameter is the same as setting a prior on the expected mean in observation space.\nNow let’s visualize a few potential priors for \\(\\sigma\\).\n\nc(\n  prior(exponential(1)), \n  prior(normal(0, 1), lb = 0), \n  prior(normal(2, 0.3), lb = 0)\n) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  xlab(expression(italic(p)(sigma))) +\n  ylab(NULL) + plot_aes\n\n\n\n\n\n\n\n\n\nQuestion 2: Given that \\(\\sigma\\) refers to the standard deviation, are these three priors theoretically possible? If yes, give an example of a theoretically impossible prior for \\(\\sigma\\).\n[[Answer: Yes, these three priors are theoretically possible. A theoretically impossible prior for sigma would be a normal distribution with a mean of 0 and a standard deviation of 1. This is because the standard deviation cannot be negative.]]\n\n\n\nPrior-predictive checks (by hand).\nNote: It’s possible we’ll need the truncnorm::rtruncnorm() function in this section. Once we have candidate priors for both \\(\\beta_0\\) and \\(\\sigma\\), we can simulate values from those priors and plot the implied distributions.\n\n# how many distributions do you want?\nn &lt;- 50\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n\n\n\n\n\n\n\n\nThe simulated values constitute predictions that are made using our prior beliefs (a prior is set for beta0 and another for sigma) When you check if these predictions (prior predictive) make sense or not, it is called the prior predictive check. The point of the prior predictive check is to iterate on specifying the priors until the prior predictive is sensible/satisfactory.\n(Again, the red boundaries denote that the only possible bty_avg values are between 1 and 10.)\n\nQuestion 3: Can Explain what the section of the previous command, before ggplot is doing?\n[[This code generates a dataset of normal distribution densities across a range of bty_avg values for n randomly sampled (mean = beta0, sd = sigma) parameter pairs.]]\n\n\nQuestion 4: The prior predictive above is for one combination of our candidate priors. Why don’t you also try the \\(\\beta_0\\) prior centered at 8, along with the \\(\\sigma\\) prior centered at 2? What do you observe? Among these two , which would you pick? And why? (Optional: try others too if you’d like)\n[[The second prior (with (_0) centered at 8 and () centered at 2) shifts the predictions higher and makes them more spread out; I would pick the one that best matches what I expect in the real data—if I expect higher and more variable values, I’d go with this new one.]]\n\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the simulation reproducible?\n# set.seed(1)\n\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 8, sd = 2),\n       sigma = rnorm(n = n, mean = 2, sd = 0.3)) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n\n\n\n\n\n\n\n\n\n\n\nFit the model that you prefer\nWe should practice writing out our model equation with our priors of choice:\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(8,\\ 2) \\\\\n\\sigma & \\sim \\operatorname{Normal}(2,\\ 0.3)\n\\end{align}\n\\]\nLet’s fit a model with our priors of choice.\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit9b.rds\")\n\nif (!file.exists(model_path)) {\nfit9.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  # make sure we're settled on our priors \n  # we don't need to use these; they're placeholders\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma)\n)\n  saveRDS(fit9.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit9.b &lt;- readRDS(model_path)\n}\n\nCheck the model summary.\n\nsummary(fit9.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.63      0.17     4.29     4.95 1.00     2964     2310\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.60      0.12     1.39     1.85 1.00     3568     2545\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we might do a posterior predictive check to see how well our model describes the data.\n\nset.seed(1)\npp_check(fit9.b, ndraws = 100) +\n  ggtitle(\"posterior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit9.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\")  +\n  ggtitle(\"posterior predictive check\") + plot_aes\n\n\n\n\n\n\n\n\nOur simple Gaussian model doesn’t do a great job respecting the lower and upper boundaries, but this is about as good as it gets when you’re in Gaussian land. On the whole, the model did a pretty okay reproducing the gross features of the distribution of the sample data.\n\nQuestion 5: To ensure you’ve understood things well, can you write below about the difference between the prior predictive check and the posterior predictive check? How do they differ in their objectives?\n[[The prior predictive check is used to evaluate the plausibility of the prior distributions before observing the data, while the posterior predictive check is used to evaluate how well the model fits the observed data after accounting for the priors and likelihood. The former focuses on the prior beliefs, while the latter focuses on the model’s performance with actual data.]]"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#prior-predictive-checks-by-sample_prior-only",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#prior-predictive-checks-by-sample_prior-only",
    "title": "Priors and Predicitive Checks",
    "section": "Prior-predictive checks (by sample_prior = \"only\")",
    "text": "Prior-predictive checks (by sample_prior = \"only\")\nWe can also sample from the prior predictive distribution from brm() itself. To do so, we use the sample_prior argument, which has the following options:\n\n\"no\", which is the default, and does not sample from the prior;\n\"yes\",, which will sample from both the prior and the posterior; and\n\"only\", which will only sample from the prior.\n\nLet’s set sample_prior = \"only\".\n\n# check to see if we want to use other priors\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit10b.rds\")\n\nif (!file.exists(model_path)) {\nfit10.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  # we can set our seed, too!\n  seed = 1\n)\nsaveRDS(fit10.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit10.b &lt;- readRDS(model_path)\n}\n\nDid you notice how we used the seed argument? This makes the results reproducible.\nNow the summary() function only returns summaries for the priors, NOT the posterior.\n\nsummary(fit10.b)  # this summarizes the prior\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     5.50      1.00     3.61     7.49 1.00     1876     1938\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.00      1.01     0.03     3.63 1.00     1957     1424\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe as_draws_df() function also returns draws from the prior.\n\nas_draws_df(fit10.b) %&gt;% \n  head() |&gt; \n  DT::datatable()\n\n\n\n\n\nHere’s how we might use that as_draws_df() output to make a similar plot to the one we made before.\n\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the results reproducible?\n# set.seed(1)\n\nas_draws_df(fit10.b) %&gt;% \n  \n  # subset\n  slice_sample(n = n) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  # notice we're defining the mean by b_Intercept\n  mutate(density = dnorm(x = bty_avg, mean = b_Intercept, sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = bty_avg, y = density, \n             # notice we're grouping by .draw\n             group = .draw)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n\n\n\n\n\n\n\n\nWe can also use functions like pp_check() to compare the prior to the sample data.\n\nset.seed(1)\npp_check(fit10.b, ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit10.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#univariable-predictor-model",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#univariable-predictor-model",
    "title": "Priors and Predicitive Checks",
    "section": "Univariable predictor model",
    "text": "Univariable predictor model\nNow we’ll add gender as the sole predictor in the model,\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{gender}_i \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\beta_1 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???}.\n\\end{align}\n\\]\nLet’s try these same set of \\(\\beta_0\\) priors\n\n# change as needed\n\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(7, 0.5)),\n  prior(normal(5.5, 2))\n) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper bondaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL) + plot_aes\n\n\n\n\n\n\n\n\nNow we update our by-hand prior predictive simulation to accomodate \\(\\beta_0\\) and \\(\\beta_1\\).\n\nn &lt;- 50\n\nset.seed(1)\n\ntibble(iter = 1:n,\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       # notice our new line\n       beta1 = rnorm(n = n, mean = 0, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %&gt;% \n  # we have a new expand_grid() line\n  # make sure everyone understands this coding scheme\n  expand_grid(gendermale = 0:1) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  # notice the updated mean formula\n  mutate(density = dnorm(x = bty_avg, \n                         mean = beta0 + beta1 * gendermale, \n                         sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~ and~italic(p)(beta[1])~and~italic(p)(sigma))) +\n  facet_wrap(~ gendermale, labeller = label_both) + plot_aes\n\n\n\n\n\n\n\n\nBefore we fit the model, let’s practice the sample_prior = \"only\" approach.\n\n# check to see if we want to use other priors\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit11b.rds'\n\nif (!file.exists(model_path)) {\n\nfit11.b = brm(\n  data = evals94,\n  family = gaussian,\n  # notice the 0 + Intercept syntax\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  seed = 2\n)\nsaveRDS(fit11.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit11.b &lt;- readRDS(model_path)\n}\n\nCheck the prior summary.\n\nsummary(fit11.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      5.46      1.00     3.53     7.43 1.00     3147     2820\ngendermale     0.02      0.99    -1.91     1.99 1.00     4109     2946\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.02      0.98     0.03     3.66 1.00     3149     1903\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompare the prior with the data with pp_check().\n\nset.seed(1)\npp_check(fit11.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit11.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\n\nThere isn’t a great grouped histogram option for pp_check(), so we experimented with type = \"freqpoly_grouped\" instead.\nIf we wanted, we could also use the predict() function to simulate bty_avg values from the priors.\n\n# walk through this slowly\n\nset.seed(1)\n\npredict(fit11.b,\n        summary = FALSE,\n        ndraws = 5) %&gt;% \n  str()\n\n num [1:5, 1:94] 5.37 7.82 3.87 4.89 6.06 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\n\n# customize the predictor grid, as desired\nnd &lt;- tibble(gender = rep(c(\"female\", \"male\"), each = 50)) %&gt;% \n  # this will make it easier to connect the nd data to the predict() output\n  mutate(row = 1:n())\n\nset.seed(1)\n\npredict(fit11.b,\n        newdata = nd,\n        summary = FALSE,\n        ndraws = 5) %&gt;% \n  data.frame() %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(row = str_remove(name, \"X\") %&gt;% as.double()) %&gt;% \n  left_join(nd, by = \"row\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 0.5, boundary = 1) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  facet_grid(draw ~ gender, labeller = label_both) + plot_aes\n\n\n\n\n\n\n\n\nOnce we’ve settled on our priors, we should once again practice writing out the full model equation:\n\\[\n\\begin{align} \\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i & = \\beta_0 + \\beta_1 \\cdot \\text{gender(male)}_i \\\\ \\beta_0 & \\sim \\operatorname{Normal}(5.46,\\ 1.00) \\\\ \\beta_1 & \\sim \\operatorname{Normal}(0.02,\\ 0.99) \\\\ \\sigma & \\sim \\operatorname{Normal}(1.02,\\ 0.98) \\end{align}\n\\]\nOkay, let’s fit the real model.\n\n# check to see if we want to use other priors\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit12b.rds'\n\nif (!file.exists(model_path)) {\n\nfit12.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  \n  # yes, you can set your seed for your posteriors, too\n  # this makes the results reproducible\n  seed = 3\n)\nsaveRDS(fit12.b,model_path)\n} else {\n  fit12.b &lt;- readRDS(model_path)\n}\n\nCheck the model summary.\n\nsummary(fit12.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      4.93      0.24     4.48     5.40 1.00     2134     2345\ngendermale    -0.54      0.31    -1.14     0.06 1.00     2050     2218\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.59      0.12     1.38     1.83 1.00     2431     2371\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHow does the posterior-predictive check look?\n\nset.seed(1)\npp_check(fit12.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12)) +\n  ggtitle(\"posterior predictive check\") + plot_aes\n\n\n\n\n\n\n\nset.seed(2)\npp_check(fit12.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n\n\n\n\n\n\n\n\n\nQuestion 6: Does the posterior predictive check look satsifactory to you?\n[[The posterior predictive check looks satisfactory to me. The model seems to be able to capture the distribution of the data well, and the red lines marking the boundaries are respected.]]\n\n\n\n\n\n\nNote\n\n\n\nFor more on prior predictive checks, see McElreath (from Chapter 4), and Solomon Kurz’s brms/tidverse implementations as well.\nFor a comprehensive guide to set priors for a given situation, look at reccomendations made by the Stan team https://github.com/stan-dev/stan/wiki/prior-choice-recommendations\nThey generally recommend against uniform priors on \\(\\beta\\) and \\(\\sigma\\) parameters. This is based on a general principle that you should not use a prior that places an artificial boundary on a parameter.\nE.g. \\(\\sigma\\) parameters have natural lower boundaries at zero, but they don’t have upper boundaries. Thus, a uniform prior adds an unnatural upper boundary. A better prior would be something that is weakly informative"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#references",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#references",
    "title": "Priors and Predicitive Checks",
    "section": "References",
    "text": "References\nHamermesh, D. S., & Parker, A. (2005). Beauty in the classroom: Instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24(4), 369-376. https://doi.org/10.1016/j.econedurev.2004.07.013\nKurz, A. S. (2023). Statistical Rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.4.0). https://bookdown.org/content/4857/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#session-information",
    "href": "posts/Lab-10/Bayes_Lab_3_1_Priors and predictive checks.html#session-information",
    "title": "Priors and Predicitive Checks",
    "section": "Session information",
    "text": "Session information\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggmcmc_1.5.1.1   GGally_2.2.1     faux_1.2.2       moderndive_0.7.0\n [5] bayesplot_1.11.1 ggdist_3.3.2     tidybayes_3.0.7  brms_2.21.0     \n [9] Rcpp_1.0.13      lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.4      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  pacman_0.5.1    \n\nloaded via a namespace (and not attached):\n  [1] infer_1.0.7          RColorBrewer_1.1-3   tensorA_0.36.2.1    \n  [4] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [7] estimability_1.5.1   farver_2.1.2         nloptr_2.1.1        \n [10] rmarkdown_2.28       vctrs_0.6.5          minqa_1.2.8         \n [13] base64enc_0.1-3      janitor_2.2.1        htmltools_0.5.8.1   \n [16] distributional_0.5.0 curl_5.2.2           broom_1.0.7         \n [19] sass_0.4.9           StanHeaders_2.32.10  bslib_0.8.0         \n [22] htmlwidgets_1.6.4    plyr_1.8.9           emmeans_1.10.7      \n [25] zoo_1.8-12           cachem_1.1.0         igraph_2.0.3        \n [28] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n [31] colourpicker_1.3.0   Matrix_1.7-0         R6_2.5.1            \n [34] fastmap_1.2.0        rbibutils_2.3        shiny_1.9.1         \n [37] snakecase_0.11.1     digest_0.6.37        numDeriv_2016.8-1.1 \n [40] colorspace_2.1-1     crosstalk_1.2.1      labeling_0.4.3      \n [43] fansi_1.0.6          timechange_0.3.0     abind_1.4-5         \n [46] compiler_4.4.1       withr_3.0.1          backports_1.5.0     \n [49] inline_0.3.19        shinystan_2.6.0      ggstats_0.9.0       \n [52] QuickJSR_1.3.1       pkgbuild_1.4.4       MASS_7.3-60.2       \n [55] gtools_3.9.5         loo_2.8.0            tools_4.4.1         \n [58] httpuv_1.6.15        threejs_0.3.3        glue_1.8.0          \n [61] nlme_3.1-164         promises_1.3.0       grid_4.4.1          \n [64] checkmate_2.3.2      reshape2_1.4.4       generics_0.1.3      \n [67] operator.tools_1.6.3 gtable_0.3.5         tzdb_0.4.0          \n [70] formula.tools_1.7.1  hms_1.1.3            utf8_1.2.4          \n [73] pillar_1.9.0         markdown_1.13        posterior_1.6.0     \n [76] later_1.3.2          splines_4.4.1        lattice_0.22-6      \n [79] survival_3.6-4       tidyselect_1.2.1     miniUI_0.1.1.1      \n [82] knitr_1.48           reformulas_0.4.0     arrayhelpers_1.1-0  \n [85] gridExtra_2.3        V8_6.0.1             stats4_4.4.1        \n [88] xfun_0.52            rstanarm_2.32.1      bridgesampling_1.1-2\n [91] matrixStats_1.4.1    DT_0.33              rstan_2.32.6        \n [94] stringi_1.8.4        yaml_2.3.10          boot_1.3-30         \n [97] evaluate_1.0.0       codetools_0.2-20     cli_3.6.4           \n[100] RcppParallel_5.1.9   Rdpack_2.6.2         shinythemes_1.2.0   \n[103] xtable_1.8-4         munsell_0.5.1        jquerylib_0.1.4     \n[106] coda_0.19-4.1        svUnit_1.0.6         parallel_4.4.1      \n[109] rstantools_2.4.0     dygraphs_1.1.1.6     Brobdingnag_1.2-9   \n[112] lme4_1.1-36          mvtnorm_1.3-1        scales_1.3.0        \n[115] xts_0.14.1           rlang_1.1.5          shinyjs_2.1.0"
  },
  {
    "objectID": "posts/Lab-11/Missing_Data_Lab_Questions.html",
    "href": "posts/Lab-11/Missing_Data_Lab_Questions.html",
    "title": "Missings Data Lab",
    "section": "",
    "text": "Missing data is a common problem and dealing with it appropriately is extremely important. Ignoring the missing data points or filling them incorrectly may cause the models to work in unexpected ways and cause the predictions and inferences to be biased.\n\nlibrary(pacman)\npacman::p_load(tidyverse,mice,naniar,devtools,patchwork,gghalves,skimr,install = T)\n\n#### define plot objects and stuff\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\n\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nLe’ts consider built-in dataset ‘airquality’ in R as a sample dataset.\n\n# Load the airquality dataset\ndata(\"airquality\")\n\ndata &lt;- airquality\n\ndata |&gt; \n  head() |&gt; \n  DT::datatable()\n\n\n\n\n\n\nQuestion 1:\n\nExamine this dataset for missing values. While there are many ways to do this, the skim function from the library ‘skimr’ is elegant;\n\n\ndata |&gt; \n  skim() |&gt; \n  DT::datatable()\n\n\n\n\n\n\nuse the nanair package to visualize missing values\n\n\ndata |&gt; \ngg_miss_var(show_pct = TRUE) +\n  labs(title = \"Missing Data by Variable in our dataset\") + \n  scale_fill_manual(values = palette) +\n  plot_aes\n\n\n\n\n\n\n\n\n\neven though it’s hard to confirm based on visualizations alone, what do your visualizations lead you to believe about the missing data being MCAR, MAR, or MNAR?\n\n\n\n\n\n\n\n\nMAR: Missing at Random\n\n\n\n\n\nCarry out Little’s statistical test to evaluate MCAR and report results.\n\n\n\n\n\n\n\nP-value &lt; 0.05: Rejects the MCAR hypothesis; data may be MAR or NMAR.\n\n\n\n\nmcar_test(data) |&gt; \n  DT::datatable()\n\n\n\n\n\n\nCreating a binary indicator for missingness allows you to test whether the presence of missing data is related to observed data.\n\nFor instance, you can create a dummy variable: 1 = Missing; 0 = Observed.\nNext you can conduct a chi-square test or t-test:\n\nChi-square: Compare proportions of missingness across groups.\nT-test: Compare means of (other) observed variables with missingness indicators.\n\n\n\n\ndata = data |&gt; \n  mutate(Ozone_missing = ifelse(is.na(Ozone), 1, 0)) |&gt; \n  group_by(Ozone_missing) \n\ndata |&gt; \n  head() |&gt;\n  DT::datatable()\n\n\n\n\n\n\nlibrary(purrr)\nlibrary(broom)\n\n# Specify which variables to compare\nvars_to_test &lt;- c(\"Solar.R\", \"Wind\", \"Temp\")\n\n# Build a summary tibble with group means and t-test results\nt_summary &lt;- map_dfr(vars_to_test, function(v) {\n  # 1) Compute the mean of variable 'v' by Ozone_missing\n  means &lt;- data |&gt; \n    group_by(Ozone_missing)|&gt;\n    summarise(mean_val = mean(.data[[v]], na.rm = TRUE), .groups = \"drop\")\n  mean0 &lt;- means$mean_val[means$Ozone_missing == 0]\n  mean1 &lt;- means$mean_val[means$Ozone_missing == 1]\n\n  # 2) Run a Welch two‐sample t-test comparing groups\n  tt &lt;- t.test(as.formula(paste(v, \"~ Ozone_missing\")), data = data, na.action = na.omit)\n  tt_tidy &lt;- broom::tidy(tt)\n\n  # 3) Return one row with all results\n  tibble(\n    variable      = v,\n    mean_obs      = mean0,\n    mean_missing  = mean1,\n    mean_diff     = mean1 - mean0,\n    t_statistic   = tt_tidy$statistic,\n    df            = tt_tidy$parameter,\n    p_value       = tt_tidy$p.value,\n    conf_low      = tt_tidy$conf.low,\n    conf_high     = tt_tidy$conf.high\n  )\n})\n\n# View the organized results\nt_summary |&gt; \n  DT::datatable()\n\n\n\n\n\n\n\nQuestion 2:\nCreate new and appropriately named datasets that are based on airquality for each of the following ways of fixing the dataset:\n\n# 1) Original data\norig_data &lt;- data|&gt;\n  dplyr::select(Ozone)|&gt;\n  mutate(method = \"Original\")\n\n# 2) Listwise deletion\nlistwise_data &lt;- data|&gt;\n  drop_na(Ozone)|&gt;\n dplyr::select(Ozone)|&gt;\n  mutate(method = \"Listwise Deletion\")\n\n# 3) Mean imputation\nmean_imp_data &lt;- data|&gt;\n  mutate(Ozone = ifelse(is.na(Ozone), mean(Ozone, na.rm = TRUE), Ozone))|&gt;\n dplyr:: select(Ozone)|&gt;\n  mutate(method = \"Mean Imputation\")\n\n# 4) Regression imputation (deterministic)\nmethods_reg &lt;- make.method(data)\nmethods_reg[c(\"Ozone\",\"Solar.R\",\"Wind\",\"Temp\")] &lt;- \"norm.predict\"\npred_reg &lt;- make.predictorMatrix(data)\npred_reg[,    \"Ozone_missing\"] &lt;- 0\npred_reg[\"Ozone_missing\", ]   &lt;- 0\nimp_reg &lt;- mice(data,\n                method          = methods_reg,\n                predictorMatrix = pred_reg,\n                m               = 1,\n                maxit           = 10,\n                seed            = 123,\n                printFlag       = FALSE)\nreg_data &lt;- complete(imp_reg, 1)|&gt;\n dplyr::select(Ozone)|&gt;\n  mutate(method = \"Regression Imputation\")\n\n# 5) Stochastic regression imputation\nmethods_stoch &lt;- make.method(data)\nmethods_stoch[c(\"Ozone\",\"Solar.R\",\"Wind\",\"Temp\")] &lt;- \"norm\"\npred_stoch &lt;- make.predictorMatrix(data)\npred_stoch[,    \"Ozone_missing\"] &lt;- 0\npred_stoch[\"Ozone_missing\", ]   &lt;- 0\nimp_stoch &lt;- mice(data,\n                  method          = methods_stoch,\n                  predictorMatrix = pred_stoch,\n                  m               = 1,\n                  maxit           = 10,\n                  seed            = 123,\n                  printFlag       = FALSE)\nstoch_data &lt;- complete(imp_stoch, 1)|&gt;\n  dplyr::select(Ozone)|&gt;\n  mutate(method = \"Stochastic Regression\")\n\n# 6) Multiple imputation via predictive mean matching\nimp_pmm &lt;- mice(data[, c(\"Ozone\",\"Solar.R\",\"Wind\",\"Temp\",\"Month\",\"Day\")],\n                method    = \"pmm\",\n                m         = 5,\n                seed      = 42,\n                printFlag = FALSE)\npmm_data &lt;- complete(imp_pmm, 1)|&gt;\n dplyr::select(Ozone)|&gt;\n  mutate(method = \"PMM Imputation\")\n\n# 7) Combine all datasets\nplot_data &lt;- bind_rows(\n  orig_data,\n  listwise_data,\n  mean_imp_data,\n  reg_data,\n  stoch_data,\n  pmm_data\n)\n\n\n# 8) Plot density overlays\nggplot(plot_data, aes(x = Ozone, color = method, fill = method)) +\n  geom_density(alpha = 0.4, adjust = 1) +\n  labs(\n    title = \"Density of Ozone: Original vs. Imputation Methods\",\n    x     = \"Ozone\",\n    y     = \"Density\"\n  ) +\n  scale_color_manual(values = palette) +\n  scale_fill_manual(values = palette) +\n  plot_aes\n\n\n\n\n\n\n\n\nWhat do you observe?\n\n\n\n\n\n\nThe density plots show how the different imputation methods affect the distribution of the Ozone variable slightly.\n\n\n\n\n\nOf course, each dataset you produced will lead to different modeling results, but we won’t go into that in today’s lab."
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html",
    "title": "Lab 4: Multinomial Regression",
    "section": "",
    "text": "Lab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#data",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#data",
    "title": "Lab 4: Multinomial Regression",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\nlibrary(pacman)\npacman::p_load(nnet,car,tidyverse,emmeans,ggeffects,knitr,patchwork,broom,parameters,easystats,install = T)\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\n\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")\n\nvoter_data |&gt; \n  head() |&gt; \n  DT::datatable()"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#lrt",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#lrt",
    "title": "Lab 4: Multinomial Regression",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\nvoter_model_expanded |&gt; \n  tidy(conf.int = TRUE )|&gt; \n  mutate(across(where(is.numeric), round, 3)) |&gt; \nDT::datatable(options = list(pageLength = 10, scrollX = TRUE))"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-political-group---emmeans",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-political-group---emmeans",
    "title": "Lab 4: Multinomial Regression",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n#Get estimated marginal means from the model\n\n#using \nmultinomial_id&lt;- emmeans(voter_model_expanded, ~ pol_ident_new|voter_category)\n\n\ncoefs = contrast(regrid(multinomial_id, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\")  \n\ncontrast = sporadic - (rarely/never):\n pol_ident_new estimate     SE df t.ratio p.value\n Dem             0.9613 0.0701 28  13.722  &lt;.0001\n Indep           0.5909 0.0773 28   7.643  &lt;.0001\n Other           0.0782 0.0868 28   0.902  0.7475\n Rep             0.8832 0.0844 28  10.469  &lt;.0001\n\ncontrast = always - (rarely/never):\n pol_ident_new estimate     SE df t.ratio p.value\n Dem             0.4797 0.0738 28   6.498  &lt;.0001\n Indep          -0.0494 0.0838 28  -0.590  0.8999\n Other          -0.8353 0.1100 28  -7.577  &lt;.0001\n Rep             0.3269 0.0890 28   3.672  0.0037\n\nResults are averaged over the levels of: race, gender, income_cat, educ \nResults are given on the log (not the response) scale. \nP value adjustment: dunnettx method for 4 tests"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-of-education---emmeans",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#marginal-effects-of-education---emmeans",
    "title": "Lab 4: Multinomial Regression",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n#Enter code\nmultinomial_edu &lt;- emmeans(voter_model_expanded, ~ educ|voter_category)\n\n\ncoefs = contrast(regrid(multinomial_edu, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\") \n\ncontrast = sporadic - (rarely/never):\n educ                estimate     SE df t.ratio p.value\n College                0.986 0.0764 28  12.904  &lt;.0001\n High school or less    0.187 0.0691 28   2.705  0.0313\n Some college           0.707 0.0744 28   9.512  &lt;.0001\n\ncontrast = always - (rarely/never):\n educ                estimate     SE df t.ratio p.value\n College                0.477 0.0800 28   5.960  &lt;.0001\n High school or less   -0.711 0.0800 28  -8.883  &lt;.0001\n Some college           0.167 0.0791 28   2.114  0.1117\n\nResults are averaged over the levels of: race, gender, income_cat, pol_ident_new \nResults are given on the log (not the response) scale. \nP value adjustment: dunnettx method for 3 tests \n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\npredictions &lt;- ggemmeans(voter_model_expanded, terms = c(\"age_centered\", \"pol_ident_new\"))\n\n# Create the plot with facets for each party ID category.\nggplot(predictions, aes(x = x, y = predicted, fill = response.level)) +\n  geom_area() +\n  geom_rug(sides = \"b\", position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"\\nAge\",\n    y = \"Predicted Probability\\n\",\n    title = \"Predicted Probabilities of Voting Frequency by Age and Party ID\"\n  ) +\n  facet_wrap(~ group, labeller = label_both) +  # Facet by Party ID\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"RARELY OR NEVER VOTE\", \"SOMETIMES VOTE\", \"ALMOST ALWAYS VOTE\"),\n    breaks = c(\"rarely/never\", \"sporadic\", \"always\")\n  ) +\n  plot_aes\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\npredictions &lt;- ggemmeans(voter_model_expanded, terms = c(\"educ\"))\n\n# Create the plot with facets for each party ID category.\nggplot(predictions, aes(x = x, y = predicted, fill = response.level)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.7) +  # Use stat = \"identity\" for bar heights\n  labs(\n    x = \"\\nEducation\",\n    y = \"Predicted Probability\\n\",\n    title = \"Predicted Probabilities of Voting Frequency by Age and Party ID\"\n  ) +\n  facet_wrap(~ group, labeller = label_both) +  # Facet by Party ID\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"RARELY OR NEVER VOTE\", \"SOMETIMES VOTE\", \"ALMOST ALWAYS VOTE\"),\n    breaks = c(\"rarely/never\", \"sporadic\", \"always\")\n  ) +\n  plot_aes"
  },
  {
    "objectID": "posts/Lab-4/Lab4_multinom_Questions-1.html#write-up",
    "href": "posts/Lab-4/Lab4_multinom_Questions-1.html#write-up",
    "title": "Lab 4: Multinomial Regression",
    "section": "Write-up",
    "text": "Write-up\n\nAge: The older people get the less likely they are to note vote, regardless of political idenitiy. However, younger, Independents are the least likely to vote the Education: People with a college education are more likely to sometimes and always vote that than counterpart, and less likely to never vote. On the other hand, people with a high school education or less are more likely to not engage in voting than their counterparts\n\n\nDifferences between political groups and voting behavior - Emmeans\n\nmulti_an &lt;- emmeans(voter_model_expanded, ~  pol_ident_new|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\nmulti_an &lt;- emmeans(voter_model_expanded, ~ educ|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\nEnter your interpretation here: &gt; The contrast analysis reveals significant differences in voting frequency based on educational attainment. Individuals with a high school education or less are less likely to vote sporadically or always compared to those with a college education. Those with some college education show mixed results; they are less likely to vote always compared to college graduates but more likely to vote sporadically than those with a high school education or less. Overall, these findings highlight the significant impact of education on voting behavior, indicating that higher educational attainment is associated with increased likelihood of electoral participation."
  },
  {
    "objectID": "posts/Lab-2/index.html",
    "href": "posts/Lab-2/index.html",
    "title": "Lab 2: Logistic Regression",
    "section": "",
    "text": "Assignment requirements:\n\nIf you are using Github (recommended), make sure to commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages, and share the link to your assignment with me. If not, you can also send me the rmd & rendered file via Canvas.\nIn this assignment, you will not need to code from scratch. Rather, you’ll need to fill in code where needed. This assignment has a logisitic regression implementation for a scenario from EDA down to model comparison (and would be useful for whenever you may encounter such a situation in the future).\nI want the assignments to begin reflecting a bit more of how you’d be doing things on your own, where you have some prior knowledge and you figure other things out (by referring to documentation, etc.) . In addition to the rmd, I also want you to submit to me notes of anything new that you learn while finishing the assignment. And any pain-points, and we’ll discuss more.\n\nNote:\n\nIf you are fitting a model, display the model output in a neatly formatted table. (The gt tidy and kable functions can help!). Modelsummary also looks good(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)\nMake sure that your plots are clearly labeled – for all axes, titles, etc."
  },
  {
    "objectID": "posts/Lab-2/index.html#data-general-social-survey",
    "href": "posts/Lab-2/index.html#data-general-social-survey",
    "title": "Lab 2: Logistic Regression",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "posts/Lab-2/index.html#eda",
    "href": "posts/Lab-2/index.html#eda",
    "title": "Lab 2: Logistic Regression",
    "section": "EDA",
    "text": "EDA\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\nFill in the “____” below to encode the binary variable\n\ndata &lt;- read.csv(\"gss2016.csv\")\n\ndata = data |&gt; \n  mutate(mass_trans_spend_right = if_else(natmass == \"About right\", 1, 0))\n\ndata |&gt; \n  DT::datatable()\n\n\n\n\n\n\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if needed\nmass_spend_long &lt;- mass_spend_summary %&gt;%\n  mutate(category = if_else(mass_trans_spend_right == 1, \"About right\", \"Not right\")) \n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\nggplot(mass_spend_summary, aes(x = factor(mass_trans_spend_right), y = proportion, fill = factor(mass_trans_spend_right))) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"),\n                   labels = c(\"Not right\", \"About right\")) +\n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Response\",\n       y = \"Proportion\",\n       fill = \"Spending View\") +\n  scale_x_discrete(labels = c(\"Not right\", \"About right\")) +\n  plot_aes\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                           levels = c(\"Extremely liberal\", \"Liberal\", \"Slightly liberal\", \n                                      \"Moderate\", \"Slghtly conservative\", \"Conservative\", \n                                      \"Extrmly conservative\"),\n                           ordered = TRUE))\n\n\nMake a plot of the distribution of polviews\n\n\n#Get proportions, format, and produce a plot like you did previously for mass_trans_spend_right\n\npol_view_summary &lt;- data %&gt;%\n  count(polviews) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(pol_view_summary, aes(x = polviews, y = proportion, fill = polviews)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = palette) +  # Removed extra parenthesis\n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Response\",\n       y = \"Proportion\",\n       fill = \"Spending View\") +\n  plot_aes\n\n\n\n\n\n\n\n\n\nWhich political view occurs most frequently in this data set?\n_____\n\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\n\n\ndata |&gt; \n  group_by(polviews) |&gt; \n  summarize(prop_satisfied = mean(mass_trans_spend_right), na.rn = T) |&gt; \n  ggplot(aes(x = polviews, y = prop_satisfied, fill = polviews)) + \n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = palette_condition) +  \n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Political Views\",\n       y = \"Proportion\\nSatisfied with Spending\",\n       fill = \"Spending View\") +\n  plot_aes\n\n\n\n\n\n\n\n\nThe more conservative one’s political views are the more they think the amount of spending on mass transportation is correct.\n\nWe’d like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as “89 or older”.\n\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values “89 or older” with a single value.\n\n\ndata = data |&gt; \n  mutate(age = if_else(age == \"89 or older\", \"89\", age)) |&gt; \n  mutate(age = as.numeric(age))\n\n\nPlot the frequency distribution of age.\n\n\ndata |&gt; \n  ggplot(aes(x = age)) + \n  geom_density(binwidth = 5, fill = \"#6195C6\") + \n  labs(title = \"Frequency Distribution of Age\",\n       x = \"Age\",\n       y = \"Frequency\") +\n  plot_aes"
  },
  {
    "objectID": "posts/Lab-2/index.html#logistic-regression",
    "href": "posts/Lab-2/index.html#logistic-regression",
    "title": "Lab 2: Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLet’s start by fitting a logistic regression model with just the intercept\n\n\nintercept_only_model &lt;- glm(mass_trans_spend_right ~ 1, data = data, family = binomial(link = \"logit\"))\n\nintercept_only_model %&gt;% \n  tidy() %&gt;%\n  DT::datatable()\n\n\n\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two–what is the basic thing this probability tells us about?\n\n\nb0 &lt;- coef(intercept_only_model)[\"(Intercept)\"]\n\n# Logistic transformation of the intercept (log-odds to probability)\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) \n\n# Compute the 95% confidence intervals on the log-odds scale\nci_lower &lt;- b0 - 1.96 * 0.0393685\nci_upper &lt;- b0 + 1.96 * 0.0393685\n\n# Transform the confidence intervals into probabilities\np_lower &lt;- exp(ci_lower) / (1 + exp(ci_lower))\np_upper &lt;- exp(ci_upper) / (1 + exp(ci_upper))\n\n# Print results\ncat(\"Intercept (probability):\", round(b0_transformed, 3), \"\\n\")\n\nIntercept (probability): 0.53 \n\ncat(\"95% CI (probability): [\", round(p_lower, 3), \",\", round(p_upper, 3), \"]\\n\")\n\n95% CI (probability): [ 0.51 , 0.549 ]\n\n\n\nThe the baseline probability of supporting the policy is 53%.\n\n\nNow let’s fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\n\ndata &lt;- data |&gt; \n  mutate(sex = as.factor(sex)) \ndata$sex &lt;- relevel(data$sex, ref = \"Male\")\n\n\nm1 &lt;- glm(mass_trans_spend_right ~ age +sex + sei10, data = data, family = binomial(link = \"logit\"))\n\nm1 %&gt;% \n  tidy() %&gt;%\n  DT::datatable()\n\n\n\n\n\n\nConsider the relationship between sex and one’s opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\nlist(\n  \"Model Coefficients\" = m1 %&gt;% tidy(),\n  \"Exponentiated Coefficients\" = m1 %&gt;% tidy(exponentiate = TRUE)\n) %&gt;%\n  purrr::map(DT::datatable)\n\n$`Model Coefficients`\n\n$`Exponentiated Coefficients`\n\n# Calculate confidence intervals for sexFemale coefficient\nbsex &lt;- coef(m1)[\"sexFemale\"]\nci_lower_lo &lt;- bsex - 1.96 * 0.0798020\nci_upper_lo &lt;- bsex + 1.96 * 0.0798020\n\n# Convert to odds ratios and calculate confidence intervals\nci_lower_or &lt;- exp(bsex - 1.96 * 0.0798020)\nci_upper_or &lt;- exp(bsex + 1.96 * 0.0798020)\n\n# Output the results\nlist(\n  \"CI for log-odds\" = c(ci_lower_lo, ci_upper_lo),\n  \"CI for Odds Ratio\" = c(ci_lower_or, ci_upper_or)\n)\n\n$`CI for log-odds`\n sexFemale  sexFemale \n0.09933194 0.41215578 \n\n$`CI for Odds Ratio`\nsexFemale sexFemale \n 1.104433  1.510070 \n\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\n\nemm_sex \n\n sex     prob     SE  df asymp.LCL asymp.UCL\n Male   0.495 0.0147 Inf     0.467     0.524\n Female 0.559 0.0133 Inf     0.533     0.585\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nIf you did this right, you’ll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n# Get the coefficient for age\nb_age &lt;- coef(m1)[\"age\"]\n\n# Compute the 95% CI for the coefficient of age in log-odds\nage_se &lt;- summary(m1)$coefficients[\"age\", \"Std. Error\"]\nci_lower_log_odds &lt;- b_age - 1.96 * age_se\nci_upper_log_odds &lt;- b_age + 1.96 * age_se\n\n# Convert log-odds to odds ratio (OR) by applying the logistic transformation\nor_age &lt;- exp(b_age)\n\n# Compute the 95% CI for the odds ratio\nci_lower_or &lt;- exp(ci_lower_log_odds)\nci_upper_or &lt;- exp(ci_upper_log_odds)\n\n# Create a data frame with the results\nresult_df &lt;- data.frame(\n  Metric = c(\"Coefficient (log-odds)\", \"95% CI for log-odds\", \"Odds Ratio\", \"95% CI for Odds Ratio\"),\n  Estimate = c(b_age, paste(round(ci_lower_log_odds, 3), \"to\", round(ci_upper_log_odds, 3)),\n               round(or_age, 3), paste(round(ci_lower_or, 3), \"to\", round(ci_upper_or, 3)))\n)\n\n# Display the results in an interactive datatable\nDT::datatable(result_df, options = list(pageLength = 5))\n\n\n\n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by -0.0062, holding all other variables constant. The odds ratio is 0.994, which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.994, or approximately 0.6% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one’s opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. ß\n\n\nbses &lt;- coef(m1)[\"sei10\"]\n\n\n# Compute the 95% CI for the coefficient of age in log-odds\nses_se &lt;- summary(m1)$coefficients[\"sei10\", \"Std. Error\"]\nci_lower_log_odds &lt;- bses - 1.96 * age_se\nci_upper_log_odds &lt;- bses + 1.96 * age_se\n\n# Convert log-odds to odds ratio (OR) by applying the logistic transformation\nor_age &lt;- exp(bses)\n\n# Compute the 95% CI for the odds ratio\nci_lower_or &lt;- exp(ci_lower_log_odds)\nci_upper_or &lt;- exp(ci_upper_log_odds)\n\n# Create a data frame with the results\nresult_df &lt;- data.frame(\n  Metric = c(\"Coefficient (log-odds)\", \"95% CI for log-odds\", \"Odds Ratio\", \"95% CI for Odds Ratio\"),\n  Estimate = c(b_age, paste(round(ci_lower_log_odds, 3), \"to\", round(ci_upper_log_odds, 3)),\n               round(or_age, 3), paste(round(ci_lower_or, 3), \"to\", round(ci_upper_or, 3)))\n)\n\n# Display the results in an interactive datatable\nDT::datatable(result_df, options = list(pageLength = 5))\n\n\n\n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0107, -0.0017]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998])."
  },
  {
    "objectID": "posts/Lab-2/index.html#marginal-effects",
    "href": "posts/Lab-2/index.html#marginal-effects",
    "title": "Lab 2: Logistic Regression",
    "section": "Marginal effects",
    "text": "Marginal effects\n\nLet’s examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\navg_comparisons(m1, comparison = \"difference\") %&gt;% \n  DT::datatable()\n\n\n\n\n\n\nThe marginal effect of age is -0.0015 (95% CI [-0.0026, -0.0004]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other factors constant (p = 0.0066).\nThe marginal effect of SES is -0.0015 (95% CI [-0.0023, -0.0007]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other variables constant.\nThe marginal effect for being female compared to male is 0.0631 (95% CI [0.0263, 0.1000]). This indicates that females are, on average, about 6.31 percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant."
  },
  {
    "objectID": "posts/Lab-2/index.html#model-comparison",
    "href": "posts/Lab-2/index.html#model-comparison",
    "title": "Lab 2: Logistic Regression",
    "section": "Model comparison",
    "text": "Model comparison\n\nNow let’s see whether a person’s political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\nfit2 &lt;- glm(mass_trans_spend_right ~age + sex + sei10, data = data, family = binomial(link = \"logit\"))\nfit3 &lt;- glm(mass_trans_spend_right ~ polviews + age + sex + sei10, data = data, family = binomial(link = \"logit\"))\n\ntest_likelihoodratio(fit2, fit3) %&gt;% kable()\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes. The model with polviews is significantly better than the model without it, as indicated by the likelihood ratio test (p &lt; 0.001)."
  },
  {
    "objectID": "posts/Lab-2/index.html#visualization",
    "href": "posts/Lab-2/index.html#visualization",
    "title": "Lab 2: Logistic Regression",
    "section": "Visualization",
    "text": "Visualization\n\nLet’s plot the results\nWe next use the model to produce visualizations:\n\nGiven the code below, interpet what is being plotted:\n\npol_plot : people that are extremely conservative are more likely to support mass transit spending\nsex_plot : women are more likely to support mass transit spending than men\nses_plot: people of lower ses are more likely to support mass transit spending\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nadjust the various settings in your plot to make it look professional.\nYou can use ggeffects to get the predicted probabilities for these models.\n\n\n\n\n\nlibrary(ggeffects)\n\n\n# Load the gridExtra package for arranging plots\nlibrary(gridExtra)\n\n# Plot for political views\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = palette) +\n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  plot_aes\n\n# Plot for sex\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  plot_aes\n\n# Plot for socioeconomic status\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\nses_plot &lt;- ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"red4\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"red4\", alpha = 0.2) +\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  plot_aes + theme(legend.position = \"none\")\n\n# Arrange the plots using grid.arrange\ngrid.arrange(pol_plot, sex_plot, ses_plot, ncol = 1)"
  },
  {
    "objectID": "posts/Lab-2/index.html#model-assumptions",
    "href": "posts/Lab-2/index.html#model-assumptions",
    "title": "Lab 2: Logistic Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nIs the logistic model a good choice for this data?\n\n\nbinned_residuals(fit2)\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: No, because only 86% of the residuals are inside the error bounds (~95% or higher would be good)."
  },
  {
    "objectID": "posts/Lab-2/index.html#model-fit",
    "href": "posts/Lab-2/index.html#model-fit",
    "title": "Lab 2: Logistic Regression",
    "section": "Model fit",
    "text": "Model fit\n\nCalculate the \\(R^2\\) for this model\n\n\nr2_mcfadden(fit2)\n\n# R2 for Generalized Linear Regression\n       R2: 0.010\n  adj. R2: 0.009\n\n\n\nR2 interpretation: The model accounts for 0.01% of the variance in the outcome variable, which is very low.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\nbinned_residuals(fit2, term=\"sei10\")\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\nbinned_residuals(fit2, term=\"age\")\n\nOk: About 98% of the residuals are inside the error bounds.\n\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYes, there are a few predictors that stick out. The residuals for the socioeconomic index (sei10) are not evenly distributed across the bins, indicating a non-linear relationship. To improve model fit, we could consider transforming the variable or using a different model that can capture non-linear relationships."
  },
  {
    "objectID": "posts/Lab-2/index.html#testing-polviews",
    "href": "posts/Lab-2/index.html#testing-polviews",
    "title": "Lab 2: Logistic Regression",
    "section": "Testing Polviews",
    "text": "Testing Polviews\n\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 0.37 times more likely to support mass transit spending compared to extremely liberals and 0.46 times more likely to support mass transit than liberals.\n\nExtreme liberals are 2.70 times more likely to support spending compared to conservatives, 2.53 times compared to moderates, and 2.34 times compared to slightly conservatives.\n\nExtremely conservatives are 3.82 times less likely to support mass spending than liberals and 2.08 times less likely than slightly liberals.\n\nLiberals are 2.03 times more likely to support spending than moderates and 1.88 times more likely than slightly conservatives.\n\n\nHow These Numbers Were Derived The reported odds ratios in the original output describe how much less likely a group is to support spending compared to another group. To express how much more likely one group is compared to another, we compute the inverse of the odds ratio:\n\n[ = ]"
  },
  {
    "objectID": "posts/Lab-2/index.html#conclusion",
    "href": "posts/Lab-2/index.html#conclusion",
    "title": "Lab 2: Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nPolitical views have the strongest effect on the dependent variable, given the largest deviance reduction. Age and sex also have a significant impact, with similar deviance reductions. Socioeconomic status (sei10) matters but has a smaller effect compared to other predictors.\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "posts/final-project/Pokemon_GPT_Simulator.html",
    "href": "posts/final-project/Pokemon_GPT_Simulator.html",
    "title": "**Stats-blog**",
    "section": "",
    "text": "from openai import OpenAI, RateLimitError, APIError, APITimeoutError\nimport pandas as pd \nfrom tqdm.notebook import tqdm\nfrom dotenv import load_dotenv\nimport re\nimport numpy as np\nimport json\nimport argparse\nimport random\nimport time\nimport os\nimport ast\n\n\n%cd \"/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project\"\n\n/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project\n\n\n/Users/sm9518/Library/Python/3.9/lib/python/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n\n\n\nload_dotenv(\"/Users/sm9518/Desktop/Article-Summarizer/.env\") # where i keep my API key... \napi_key = os.getenv(\"OPENAI_API_KEY\")\nif api_key:\n    print(\"API Key loaded successfully!\\n:)\")\nelse:\n    raise ValueError(\"API Key not found.\\nMake sure it is set in the .env file.\")\nmodel=\"gpt-3.5-turbo\" # set model. we dont need anything fancy for this task.\ntemperature=0 # set temp to be rather determinisitic \nSEED = random.seed(42) # set seed for reproducibility\n\nAPI Key loaded successfully!\n:)\n\n\n\ndf = pd.read_csv('/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project/data/pokedex.csv', index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\nname\nheight\nweight\nhp\nattack\ndefense\ns_attack\ns_defense\nspeed\ntype\nevo_set\ninfo\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nbulbasaur\n7\n69\n45\n49\n49\n65\n65\n45\n{grass,poison}\n1\nA strange seed was planted on its back at birt...\n\n\n2\nivysaur\n10\n130\n60\n62\n63\n80\n80\n60\n{grass,poison}\n1\nWhen the bulb on its back grows large, it appe...\n\n\n3\nvenusaur\n20\n1000\n80\n82\n83\n100\n100\n80\n{grass,poison}\n1\nThe plant blooms when it is absorbing solar en...\n\n\n4\ncharmander\n6\n85\n39\n52\n43\n60\n50\n65\n{fire}\n2\nObviously prefers hot places. When it rains, s...\n\n\n5\ncharmeleon\n11\n190\n58\n64\n58\n80\n65\n80\n{fire}\n2\nWhen it swings its burning tail, it elevates t...\n\n\n\n\n\n\n\n\nOG_pokedex = df.iloc[:151].copy() # take the OG 151 pokemon\n\n# build the 20‐matchups per Pokémon\nmatchups = []\nfor challenger in OG_pokedex['name']:\n    pool = [p for p in OG_pokedex['name'] if p != challenger]\n    opponents = random.sample(pool, 20) # give them 20 challengers\n    for opponent in opponents:\n        matchups.append({'challenger': challenger, 'opponent': opponent})\nmatchups_df = pd.DataFrame(matchups)\n\n\n\n# merge challenger metadata\nmatchups_with_meta = (\n    matchups_df\n    .merge(\n        OG_pokedex.add_suffix('_challenger'),\n        left_on='challenger',\n        right_on='name_challenger',\n        how='left'\n    )\n    # drop the redundant name_challenger column if you like\n    .drop(columns=['name_challenger'])\n    # merge opponent metadata\n    .merge(\n        OG_pokedex.add_suffix('_opponent'),\n        left_on='opponent',\n        right_on='name_opponent',\n        how='left'\n    )\n    .drop(columns=['name_opponent'])\n)\n\n# now every row has both challenger_* and opponent_* columns\nmatchups_with_meta.head()\n\n\n\n\n\n\n\n\nchallenger\nopponent\nheight_challenger\nweight_challenger\nhp_challenger\nattack_challenger\ndefense_challenger\ns_attack_challenger\ns_defense_challenger\nspeed_challenger\n...\nweight_opponent\nhp_opponent\nattack_opponent\ndefense_opponent\ns_attack_opponent\ns_defense_opponent\nspeed_opponent\ntype_opponent\nevo_set_opponent\ninfo_opponent\n\n\n\n\n0\nbulbasaur\nnidorina\n7\n69\n45\n49\n49\n65\n65\n45\n...\n200\n70\n62\n67\n55\n55\n56\n{poison}\n12\nThe female's horn develops slowly. Prefers phy...\n\n\n1\nbulbasaur\nwartortle\n7\n69\n45\n49\n49\n65\n65\n45\n...\n225\n59\n63\n80\n65\n80\n58\n{water}\n3\nOften hides in water to stalk unwary prey. For...\n\n\n2\nbulbasaur\ntentacool\n7\n69\n45\n49\n49\n65\n65\n45\n...\n455\n40\n40\n35\n50\n100\n70\n{water,poison}\n30\nDrifts in shallow seas. Anglers who hook them ...\n\n\n3\nbulbasaur\nkadabra\n7\n69\n45\n49\n49\n65\n65\n45\n...\n565\n40\n35\n30\n120\n70\n105\n{psychic}\n27\nIt emits special alpha waves from its body tha...\n\n\n4\nbulbasaur\narcanine\n7\n69\n45\n49\n49\n65\n65\n45\n...\n1550\n90\n110\n80\n100\n80\n95\n{fire}\n25\nA POKéMON that has been admired since the past...\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n# randomly sample 10% to make sure it works before deploying at scale\n#matchups_with_meta = matchups_with_meta.sample(frac=0.1,random_state=SEED)\n#matchups_with_meta.shape\n\n(302, 24)\n\n\n\n# Initialize OpenAI client\nclient = OpenAI()\n# ---- Utility Functions ---- #\n\ndef safe_parse_types(val):\n    if isinstance(val, list):\n        return val\n    try:\n        return ast.literal_eval(val)\n    except Exception:\n        return [str(val)]\n\ndef format_pokemon_stats(name, row, suffix):\n    types = safe_parse_types(row[f'type{suffix}'])\n    return (\n        f\"{name.title()}:\\n\"\n        f\"- Type: {', '.join(types)}\\n\"\n        f\"- HP: {row[f'hp{suffix}']}\\n\"\n        f\"- Attack: {row[f'attack{suffix}']}\\n\"\n        f\"- Defense: {row[f'defense{suffix}']}\\n\"\n        f\"- Special Attack: {row[f's_attack{suffix}']}\\n\"\n        f\"- Special Defense: {row[f's_defense{suffix}']}\\n\"\n        f\"- Speed: {row[f'speed{suffix}']}\\n\"\n    )\n\n# ---- API Interaction ---- #\n\ndef get_completion(prompt):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature\n    )\n    return response.choices[0].message.content.strip()\n\ndef get_response(prompt):\n    try:\n        return get_completion(prompt)\n    except RateLimitError as e:\n        retry_time = getattr(e, 'retry_after', 30)\n        print(f\"Rate limit exceeded. Retrying in {retry_time} seconds...\")\n        time.sleep(retry_time)\n        return get_response(prompt)\n    except APIError as e:\n        print(f\"API error occurred: {e}. Retrying in 30 seconds...\")\n        time.sleep(30)\n        return get_response(prompt)\n    except APITimeoutError as e:\n        print(f\"Request timed out: {e}. Retrying in 10 seconds...\")\n        time.sleep(10)\n        return get_response(prompt)\n    except Exception as e:\n        print(f\"Unexpected error: {e}. Retrying in 10 seconds...\")\n        time.sleep(10)\n        return get_response(prompt)\n\n# ---- Simulate One Battle ---- #\n\ndef simulate_battle(row):\n    p1_stats = format_pokemon_stats(row['challenger'], row, '_challenger')\n    p2_stats = format_pokemon_stats(row['opponent'], row, '_opponent')\n\n    prompt = (\n        \"Based on the stats, which Pokémon would win a one-on-one battle?\\n\\n\"\n        f\"{p1_stats}\\nVS\\n\\n{p2_stats}\\n\\n\"\n        \"Only respond with the name of the winning Pokémon.\"\n    )\n\n    response = get_response(prompt)\n    return response.lower()\n\n# ---- Run All Simulations ---- #\n\n# This should be your DataFrame containing all matchups\n# matchups_with_meta = pd.read_csv(...)  # Load your data here\n\nresults = []\n\nfor idx, row in tqdm(matchups_with_meta.iterrows(), total=len(matchups_with_meta), desc=\"Simulating battles\"):\n    print(f\"Simulating battle {idx + 1} of {len(matchups_with_meta)}: {row['challenger']} vs {row['opponent']}\")\n    winner = simulate_battle(row)\n    results.append({\n        \"challenger\": row['challenger'],\n        \"opponent\": row['opponent'],\n        \"winner\": winner\n    })\n    time.sleep(1.5)  # Respect rate limits\n\n# ---- Save Results ---- #\n\nresults_df = pd.DataFrame(results)\nmatchups_with_results = matchups_with_meta.merge(\n    results_df,\n    on=[\"challenger\", \"opponent\"],\n    how=\"left\"\n)\nmatchups_with_results.head()\nmatchups_with_results.to_csv(f\"/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project/data/pokemon_battle_results_{model}_{SEED}_{temperature}.csv\", index=False)\nprint(f\"\\nDone! Winners saved to pokemon_battle_results_{model}_{SEED}_{temperature}.csv.\")\n\n\n\n\nSimulating battle 1 of 3020: bulbasaur vs nidorina\nSimulating battle 2 of 3020: bulbasaur vs wartortle\nSimulating battle 3 of 3020: bulbasaur vs tentacool\nSimulating battle 4 of 3020: bulbasaur vs kadabra\nSimulating battle 5 of 3020: bulbasaur vs arcanine\nSimulating battle 6 of 3020: bulbasaur vs vulpix\nSimulating battle 7 of 3020: bulbasaur vs sandslash\nSimulating battle 8 of 3020: bulbasaur vs kabutops\nSimulating battle 9 of 3020: bulbasaur vs arbok\nSimulating battle 10 of 3020: bulbasaur vs weezing\nSimulating battle 11 of 3020: bulbasaur vs caterpie\nSimulating battle 12 of 3020: bulbasaur vs blastoise\nSimulating battle 13 of 3020: bulbasaur vs pikachu\nSimulating battle 14 of 3020: bulbasaur vs primeape\nSimulating battle 15 of 3020: bulbasaur vs poliwhirl\nSimulating battle 16 of 3020: bulbasaur vs lapras\nSimulating battle 17 of 3020: bulbasaur vs zapdos\nSimulating battle 18 of 3020: bulbasaur vs meowth\nSimulating battle 19 of 3020: bulbasaur vs koffing\nSimulating battle 20 of 3020: bulbasaur vs growlithe\nSimulating battle 21 of 3020: ivysaur vs horsea\nSimulating battle 22 of 3020: ivysaur vs tentacruel\nSimulating battle 23 of 3020: ivysaur vs venusaur\nSimulating battle 24 of 3020: ivysaur vs golbat\nSimulating battle 25 of 3020: ivysaur vs weezing\nSimulating battle 26 of 3020: ivysaur vs muk\nSimulating battle 27 of 3020: ivysaur vs zubat\nSimulating battle 28 of 3020: ivysaur vs primeape\nSimulating battle 29 of 3020: ivysaur vs grimer\nSimulating battle 30 of 3020: ivysaur vs sandslash\nSimulating battle 31 of 3020: ivysaur vs pikachu\nSimulating battle 32 of 3020: ivysaur vs kingler\nSimulating battle 33 of 3020: ivysaur vs raichu\nSimulating battle 34 of 3020: ivysaur vs haunter\nSimulating battle 35 of 3020: ivysaur vs shellder\nSimulating battle 36 of 3020: ivysaur vs bellsprout\nSimulating battle 37 of 3020: ivysaur vs weedle\nSimulating battle 38 of 3020: ivysaur vs seaking\nSimulating battle 39 of 3020: ivysaur vs omastar\nSimulating battle 40 of 3020: ivysaur vs nidorino\nSimulating battle 41 of 3020: venusaur vs krabby\nSimulating battle 42 of 3020: venusaur vs fearow\nSimulating battle 43 of 3020: venusaur vs snorlax\nSimulating battle 44 of 3020: venusaur vs ponyta\nSimulating battle 45 of 3020: venusaur vs gengar\nSimulating battle 46 of 3020: venusaur vs dragonite\nSimulating battle 47 of 3020: venusaur vs dugtrio\nSimulating battle 48 of 3020: venusaur vs rattata\nSimulating battle 49 of 3020: venusaur vs weedle\nSimulating battle 50 of 3020: venusaur vs poliwag\nSimulating battle 51 of 3020: venusaur vs golem\nSimulating battle 52 of 3020: venusaur vs poliwhirl\nSimulating battle 53 of 3020: venusaur vs sandshrew\nSimulating battle 54 of 3020: venusaur vs kingler\nSimulating battle 55 of 3020: venusaur vs tentacruel\nSimulating battle 56 of 3020: venusaur vs goldeen\nSimulating battle 57 of 3020: venusaur vs onix\nSimulating battle 58 of 3020: venusaur vs oddish\nSimulating battle 59 of 3020: venusaur vs drowzee\nSimulating battle 60 of 3020: venusaur vs gastly\nSimulating battle 61 of 3020: charmander vs golduck\nSimulating battle 62 of 3020: charmander vs weepinbell\nSimulating battle 63 of 3020: charmander vs raticate\nSimulating battle 64 of 3020: charmander vs vileplume\nSimulating battle 65 of 3020: charmander vs omanyte\nSimulating battle 66 of 3020: charmander vs kadabra\nSimulating battle 67 of 3020: charmander vs oddish\nSimulating battle 68 of 3020: charmander vs staryu\nSimulating battle 69 of 3020: charmander vs kingler\nSimulating battle 70 of 3020: charmander vs victreebel\nSimulating battle 71 of 3020: charmander vs articuno\nSimulating battle 72 of 3020: charmander vs growlithe\nSimulating battle 73 of 3020: charmander vs dodrio\nSimulating battle 74 of 3020: charmander vs pidgey\nSimulating battle 75 of 3020: charmander vs poliwag\nSimulating battle 76 of 3020: charmander vs caterpie\nSimulating battle 77 of 3020: charmander vs magneton\nSimulating battle 78 of 3020: charmander vs cubone\nSimulating battle 79 of 3020: charmander vs pidgeot\nSimulating battle 80 of 3020: charmander vs mankey\nSimulating battle 81 of 3020: charmeleon vs dratini\nSimulating battle 82 of 3020: charmeleon vs magneton\nSimulating battle 83 of 3020: charmeleon vs mankey\nSimulating battle 84 of 3020: charmeleon vs magikarp\nSimulating battle 85 of 3020: charmeleon vs exeggutor\nSimulating battle 86 of 3020: charmeleon vs seaking\nSimulating battle 87 of 3020: charmeleon vs ninetales\nSimulating battle 88 of 3020: charmeleon vs bellsprout\nSimulating battle 89 of 3020: charmeleon vs vulpix\nSimulating battle 90 of 3020: charmeleon vs alakazam\nSimulating battle 91 of 3020: charmeleon vs zapdos\nSimulating battle 92 of 3020: charmeleon vs omastar\nSimulating battle 93 of 3020: charmeleon vs mew\nSimulating battle 94 of 3020: charmeleon vs rhyhorn\nSimulating battle 95 of 3020: charmeleon vs cubone\nSimulating battle 96 of 3020: charmeleon vs gengar\nSimulating battle 97 of 3020: charmeleon vs growlithe\nSimulating battle 98 of 3020: charmeleon vs ditto\nSimulating battle 99 of 3020: charmeleon vs tauros\nSimulating battle 100 of 3020: charmeleon vs pikachu\nSimulating battle 101 of 3020: charizard vs kakuna\nSimulating battle 102 of 3020: charizard vs nidorina\nSimulating battle 103 of 3020: charizard vs zubat\nSimulating battle 104 of 3020: charizard vs golbat\nSimulating battle 105 of 3020: charizard vs weezing\nSimulating battle 106 of 3020: charizard vs pidgeot\nSimulating battle 107 of 3020: charizard vs voltorb\nSimulating battle 108 of 3020: charizard vs kingler\nSimulating battle 109 of 3020: charizard vs starmie\nSimulating battle 110 of 3020: charizard vs porygon\nSimulating battle 111 of 3020: charizard vs machop\nSimulating battle 112 of 3020: charizard vs snorlax\nSimulating battle 113 of 3020: charizard vs venusaur\nSimulating battle 114 of 3020: charizard vs nidoqueen\nSimulating battle 115 of 3020: charizard vs omastar\nSimulating battle 116 of 3020: charizard vs weepinbell\nSimulating battle 117 of 3020: charizard vs muk\nSimulating battle 118 of 3020: charizard vs ponyta\nSimulating battle 119 of 3020: charizard vs chansey\nSimulating battle 120 of 3020: charizard vs goldeen\nSimulating battle 121 of 3020: squirtle vs bulbasaur\nSimulating battle 122 of 3020: squirtle vs bellsprout\nSimulating battle 123 of 3020: squirtle vs gyarados\nSimulating battle 124 of 3020: squirtle vs parasect\nSimulating battle 125 of 3020: squirtle vs lapras\nSimulating battle 126 of 3020: squirtle vs nidoran-f\nSimulating battle 127 of 3020: squirtle vs rapidash\nSimulating battle 128 of 3020: squirtle vs meowth\nSimulating battle 129 of 3020: squirtle vs zubat\nSimulating battle 130 of 3020: squirtle vs hypno\nSimulating battle 131 of 3020: squirtle vs oddish\nSimulating battle 132 of 3020: squirtle vs kabuto\nSimulating battle 133 of 3020: squirtle vs porygon\nSimulating battle 134 of 3020: squirtle vs doduo\nSimulating battle 135 of 3020: squirtle vs pinsir\nSimulating battle 136 of 3020: squirtle vs charmeleon\nSimulating battle 137 of 3020: squirtle vs nidorina\nSimulating battle 138 of 3020: squirtle vs gengar\nSimulating battle 139 of 3020: squirtle vs slowbro\nSimulating battle 140 of 3020: squirtle vs abra\nSimulating battle 141 of 3020: wartortle vs pidgey\nSimulating battle 142 of 3020: wartortle vs abra\nSimulating battle 143 of 3020: wartortle vs dratini\nSimulating battle 144 of 3020: wartortle vs fearow\nSimulating battle 145 of 3020: wartortle vs ekans\nSimulating battle 146 of 3020: wartortle vs magmar\nSimulating battle 147 of 3020: wartortle vs rattata\nSimulating battle 148 of 3020: wartortle vs omanyte\nSimulating battle 149 of 3020: wartortle vs nidoking\nSimulating battle 150 of 3020: wartortle vs scyther\nSimulating battle 151 of 3020: wartortle vs aerodactyl\nSimulating battle 152 of 3020: wartortle vs gloom\nSimulating battle 153 of 3020: wartortle vs bellsprout\nSimulating battle 154 of 3020: wartortle vs porygon\nSimulating battle 155 of 3020: wartortle vs weezing\nSimulating battle 156 of 3020: wartortle vs mankey\nSimulating battle 157 of 3020: wartortle vs kabuto\nSimulating battle 158 of 3020: wartortle vs persian\nSimulating battle 159 of 3020: wartortle vs magnemite\nSimulating battle 160 of 3020: wartortle vs cubone\nSimulating battle 161 of 3020: blastoise vs hypno\nSimulating battle 162 of 3020: blastoise vs tangela\nSimulating battle 163 of 3020: blastoise vs vaporeon\nSimulating battle 164 of 3020: blastoise vs seadra\nSimulating battle 165 of 3020: blastoise vs nidoran-m\nSimulating battle 166 of 3020: blastoise vs alakazam\nSimulating battle 167 of 3020: blastoise vs arcanine\nSimulating battle 168 of 3020: blastoise vs pidgeot\nSimulating battle 169 of 3020: blastoise vs grimer\nSimulating battle 170 of 3020: blastoise vs charizard\nSimulating battle 171 of 3020: blastoise vs snorlax\nSimulating battle 172 of 3020: blastoise vs poliwag\nSimulating battle 173 of 3020: blastoise vs growlithe\nSimulating battle 174 of 3020: blastoise vs ivysaur\nSimulating battle 175 of 3020: blastoise vs raticate\nSimulating battle 176 of 3020: blastoise vs pidgeotto\nSimulating battle 177 of 3020: blastoise vs rattata\nSimulating battle 178 of 3020: blastoise vs caterpie\nSimulating battle 179 of 3020: blastoise vs seel\nSimulating battle 180 of 3020: blastoise vs eevee\nSimulating battle 181 of 3020: caterpie vs poliwrath\nSimulating battle 182 of 3020: caterpie vs tentacruel\nSimulating battle 183 of 3020: caterpie vs magmar\nSimulating battle 184 of 3020: caterpie vs mankey\nSimulating battle 185 of 3020: caterpie vs kabuto\nSimulating battle 186 of 3020: caterpie vs clefairy\nSimulating battle 187 of 3020: caterpie vs dragonair\nSimulating battle 188 of 3020: caterpie vs dragonite\nSimulating battle 189 of 3020: caterpie vs scyther\nSimulating battle 190 of 3020: caterpie vs kadabra\nSimulating battle 191 of 3020: caterpie vs hitmonlee\nSimulating battle 192 of 3020: caterpie vs diglett\nSimulating battle 193 of 3020: caterpie vs raichu\nSimulating battle 194 of 3020: caterpie vs rhydon\nSimulating battle 195 of 3020: caterpie vs gastly\nSimulating battle 196 of 3020: caterpie vs weezing\nSimulating battle 197 of 3020: caterpie vs hitmonchan\nSimulating battle 198 of 3020: caterpie vs starmie\nSimulating battle 199 of 3020: caterpie vs beedrill\nSimulating battle 200 of 3020: caterpie vs sandshrew\nSimulating battle 201 of 3020: metapod vs pidgeotto\nSimulating battle 202 of 3020: metapod vs marowak\nSimulating battle 203 of 3020: metapod vs grimer\nSimulating battle 204 of 3020: metapod vs nidoran-f\nSimulating battle 205 of 3020: metapod vs alakazam\nSimulating battle 206 of 3020: metapod vs dugtrio\nSimulating battle 207 of 3020: metapod vs diglett\nSimulating battle 208 of 3020: metapod vs omastar\nSimulating battle 209 of 3020: metapod vs horsea\nSimulating battle 210 of 3020: metapod vs vulpix\nSimulating battle 211 of 3020: metapod vs weezing\nSimulating battle 212 of 3020: metapod vs venonat\nSimulating battle 213 of 3020: metapod vs tentacruel\nSimulating battle 214 of 3020: metapod vs staryu\nSimulating battle 215 of 3020: metapod vs spearow\nSimulating battle 216 of 3020: metapod vs kangaskhan\nSimulating battle 217 of 3020: metapod vs aerodactyl\nSimulating battle 218 of 3020: metapod vs sandshrew\nSimulating battle 219 of 3020: metapod vs kakuna\nSimulating battle 220 of 3020: metapod vs kabuto\nSimulating battle 221 of 3020: butterfree vs charmander\nSimulating battle 222 of 3020: butterfree vs pikachu\nSimulating battle 223 of 3020: butterfree vs poliwrath\nSimulating battle 224 of 3020: butterfree vs gloom\nSimulating battle 225 of 3020: butterfree vs hitmonlee\nSimulating battle 226 of 3020: butterfree vs magmar\nSimulating battle 227 of 3020: butterfree vs electabuzz\nSimulating battle 228 of 3020: butterfree vs mankey\nSimulating battle 229 of 3020: butterfree vs cubone\nSimulating battle 230 of 3020: butterfree vs pidgeotto\nSimulating battle 231 of 3020: butterfree vs kingler\nSimulating battle 232 of 3020: butterfree vs bulbasaur\nSimulating battle 233 of 3020: butterfree vs electrode\nSimulating battle 234 of 3020: butterfree vs bellsprout\nSimulating battle 235 of 3020: butterfree vs goldeen\nSimulating battle 236 of 3020: butterfree vs graveler\nSimulating battle 237 of 3020: butterfree vs weezing\nSimulating battle 238 of 3020: butterfree vs articuno\nSimulating battle 239 of 3020: butterfree vs zubat\nSimulating battle 240 of 3020: butterfree vs diglett\nSimulating battle 241 of 3020: weedle vs ponyta\nSimulating battle 242 of 3020: weedle vs primeape\nSimulating battle 243 of 3020: weedle vs pidgey\nSimulating battle 244 of 3020: weedle vs mewtwo\nSimulating battle 245 of 3020: weedle vs kabuto\nSimulating battle 246 of 3020: weedle vs pidgeotto\nSimulating battle 247 of 3020: weedle vs magneton\nSimulating battle 248 of 3020: weedle vs kakuna\nSimulating battle 249 of 3020: weedle vs mew\nSimulating battle 250 of 3020: weedle vs jynx\nSimulating battle 251 of 3020: weedle vs gyarados\nSimulating battle 252 of 3020: weedle vs porygon\nSimulating battle 253 of 3020: weedle vs golbat\nSimulating battle 254 of 3020: weedle vs ditto\nSimulating battle 255 of 3020: weedle vs fearow\nSimulating battle 256 of 3020: weedle vs venomoth\nSimulating battle 257 of 3020: weedle vs rattata\nSimulating battle 258 of 3020: weedle vs poliwrath\nSimulating battle 259 of 3020: weedle vs marowak\nSimulating battle 260 of 3020: weedle vs nidoran-m\nSimulating battle 261 of 3020: kakuna vs dratini\nSimulating battle 262 of 3020: kakuna vs alakazam\nSimulating battle 263 of 3020: kakuna vs mewtwo\nSimulating battle 264 of 3020: kakuna vs metapod\nSimulating battle 265 of 3020: kakuna vs fearow\nSimulating battle 266 of 3020: kakuna vs koffing\nSimulating battle 267 of 3020: kakuna vs mew\nSimulating battle 268 of 3020: kakuna vs moltres\nSimulating battle 269 of 3020: kakuna vs jolteon\nSimulating battle 270 of 3020: kakuna vs magneton\nSimulating battle 271 of 3020: kakuna vs machamp\nSimulating battle 272 of 3020: kakuna vs psyduck\nSimulating battle 273 of 3020: kakuna vs abra\nSimulating battle 274 of 3020: kakuna vs bellsprout\nSimulating battle 275 of 3020: kakuna vs exeggutor\nSimulating battle 276 of 3020: kakuna vs clefairy\nSimulating battle 277 of 3020: kakuna vs rapidash\nSimulating battle 278 of 3020: kakuna vs seaking\nSimulating battle 279 of 3020: kakuna vs raticate\nSimulating battle 280 of 3020: kakuna vs venusaur\nSimulating battle 281 of 3020: beedrill vs seaking\nSimulating battle 282 of 3020: beedrill vs moltres\nSimulating battle 283 of 3020: beedrill vs sandshrew\nSimulating battle 284 of 3020: beedrill vs raticate\nSimulating battle 285 of 3020: beedrill vs omastar\nSimulating battle 286 of 3020: beedrill vs mankey\nSimulating battle 287 of 3020: beedrill vs lapras\nSimulating battle 288 of 3020: beedrill vs bellsprout\nSimulating battle 289 of 3020: beedrill vs clefairy\nSimulating battle 290 of 3020: beedrill vs cloyster\nSimulating battle 291 of 3020: beedrill vs rattata\nSimulating battle 292 of 3020: beedrill vs kadabra\nSimulating battle 293 of 3020: beedrill vs drowzee\nSimulating battle 294 of 3020: beedrill vs geodude\nSimulating battle 295 of 3020: beedrill vs golbat\nSimulating battle 296 of 3020: beedrill vs tangela\nSimulating battle 297 of 3020: beedrill vs kabutops\nSimulating battle 298 of 3020: beedrill vs slowpoke\nSimulating battle 299 of 3020: beedrill vs porygon\nSimulating battle 300 of 3020: beedrill vs venusaur\nSimulating battle 301 of 3020: pidgey vs snorlax\nSimulating battle 302 of 3020: pidgey vs rapidash\nSimulating battle 303 of 3020: pidgey vs sandslash\nSimulating battle 304 of 3020: pidgey vs clefable\nSimulating battle 305 of 3020: pidgey vs bellsprout\nSimulating battle 306 of 3020: pidgey vs nidoqueen\nSimulating battle 307 of 3020: pidgey vs nidoran-f\nSimulating battle 308 of 3020: pidgey vs zubat\nSimulating battle 309 of 3020: pidgey vs victreebel\nSimulating battle 310 of 3020: pidgey vs geodude\nSimulating battle 311 of 3020: pidgey vs golduck\nSimulating battle 312 of 3020: pidgey vs muk\nSimulating battle 313 of 3020: pidgey vs psyduck\nSimulating battle 314 of 3020: pidgey vs lapras\nSimulating battle 315 of 3020: pidgey vs pinsir\nSimulating battle 316 of 3020: pidgey vs machop\nSimulating battle 317 of 3020: pidgey vs kakuna\nSimulating battle 318 of 3020: pidgey vs pikachu\nSimulating battle 319 of 3020: pidgey vs weezing\nSimulating battle 320 of 3020: pidgey vs tentacool\nSimulating battle 321 of 3020: pidgeotto vs butterfree\nSimulating battle 322 of 3020: pidgeotto vs bulbasaur\nSimulating battle 323 of 3020: pidgeotto vs dewgong\nSimulating battle 324 of 3020: pidgeotto vs clefairy\nSimulating battle 325 of 3020: pidgeotto vs bellsprout\nSimulating battle 326 of 3020: pidgeotto vs oddish\nSimulating battle 327 of 3020: pidgeotto vs kangaskhan\nSimulating battle 328 of 3020: pidgeotto vs snorlax\nSimulating battle 329 of 3020: pidgeotto vs rhyhorn\nSimulating battle 330 of 3020: pidgeotto vs zapdos\nSimulating battle 331 of 3020: pidgeotto vs venusaur\nSimulating battle 332 of 3020: pidgeotto vs nidorina\nSimulating battle 333 of 3020: pidgeotto vs spearow\nSimulating battle 334 of 3020: pidgeotto vs wigglytuff\nSimulating battle 335 of 3020: pidgeotto vs kabutops\nSimulating battle 336 of 3020: pidgeotto vs caterpie\nSimulating battle 337 of 3020: pidgeotto vs drowzee\nSimulating battle 338 of 3020: pidgeotto vs mew\nSimulating battle 339 of 3020: pidgeotto vs jigglypuff\nSimulating battle 340 of 3020: pidgeotto vs rhydon\nSimulating battle 341 of 3020: pidgeot vs nidoking\nSimulating battle 342 of 3020: pidgeot vs metapod\nSimulating battle 343 of 3020: pidgeot vs slowbro\nSimulating battle 344 of 3020: pidgeot vs onix\nSimulating battle 345 of 3020: pidgeot vs haunter\nSimulating battle 346 of 3020: pidgeot vs golduck\nSimulating battle 347 of 3020: pidgeot vs alakazam\nSimulating battle 348 of 3020: pidgeot vs sandslash\nSimulating battle 349 of 3020: pidgeot vs gastly\nSimulating battle 350 of 3020: pidgeot vs zapdos\nSimulating battle 351 of 3020: pidgeot vs hitmonlee\nSimulating battle 352 of 3020: pidgeot vs zubat\nSimulating battle 353 of 3020: pidgeot vs poliwrath\nSimulating battle 354 of 3020: pidgeot vs oddish\nSimulating battle 355 of 3020: pidgeot vs parasect\nSimulating battle 356 of 3020: pidgeot vs hitmonchan\nSimulating battle 357 of 3020: pidgeot vs squirtle\nSimulating battle 358 of 3020: pidgeot vs dewgong\nSimulating battle 359 of 3020: pidgeot vs weepinbell\nSimulating battle 360 of 3020: pidgeot vs golbat\nSimulating battle 361 of 3020: rattata vs nidoran-f\nSimulating battle 362 of 3020: rattata vs kingler\nSimulating battle 363 of 3020: rattata vs caterpie\nSimulating battle 364 of 3020: rattata vs mr-mime\nSimulating battle 365 of 3020: rattata vs growlithe\nSimulating battle 366 of 3020: rattata vs persian\nSimulating battle 367 of 3020: rattata vs seaking\nSimulating battle 368 of 3020: rattata vs cloyster\nSimulating battle 369 of 3020: rattata vs slowbro\nSimulating battle 370 of 3020: rattata vs poliwag\nSimulating battle 371 of 3020: rattata vs arcanine\nSimulating battle 372 of 3020: rattata vs squirtle\nSimulating battle 373 of 3020: rattata vs dugtrio\nSimulating battle 374 of 3020: rattata vs cubone\nSimulating battle 375 of 3020: rattata vs seel\nSimulating battle 376 of 3020: rattata vs tentacruel\nSimulating battle 377 of 3020: rattata vs pidgeot\nSimulating battle 378 of 3020: rattata vs ditto\nSimulating battle 379 of 3020: rattata vs omastar\nSimulating battle 380 of 3020: rattata vs wartortle\nSimulating battle 381 of 3020: raticate vs nidoqueen\nSimulating battle 382 of 3020: raticate vs machamp\nSimulating battle 383 of 3020: raticate vs parasect\nSimulating battle 384 of 3020: raticate vs mewtwo\nSimulating battle 385 of 3020: raticate vs bellsprout\nSimulating battle 386 of 3020: raticate vs caterpie\nSimulating battle 387 of 3020: raticate vs nidoran-f\nSimulating battle 388 of 3020: raticate vs chansey\nSimulating battle 389 of 3020: raticate vs shellder\nSimulating battle 390 of 3020: raticate vs magneton\nSimulating battle 391 of 3020: raticate vs ditto\nSimulating battle 392 of 3020: raticate vs voltorb\nSimulating battle 393 of 3020: raticate vs dragonite\nSimulating battle 394 of 3020: raticate vs diglett\nSimulating battle 395 of 3020: raticate vs machoke\nSimulating battle 396 of 3020: raticate vs butterfree\nSimulating battle 397 of 3020: raticate vs bulbasaur\nSimulating battle 398 of 3020: raticate vs jolteon\nSimulating battle 399 of 3020: raticate vs omastar\nSimulating battle 400 of 3020: raticate vs meowth\nSimulating battle 401 of 3020: spearow vs onix\nSimulating battle 402 of 3020: spearow vs rhydon\nSimulating battle 403 of 3020: spearow vs pidgeot\nSimulating battle 404 of 3020: spearow vs seel\nSimulating battle 405 of 3020: spearow vs magneton\nSimulating battle 406 of 3020: spearow vs nidorino\nSimulating battle 407 of 3020: spearow vs rapidash\nSimulating battle 408 of 3020: spearow vs lapras\nSimulating battle 409 of 3020: spearow vs magnemite\nSimulating battle 410 of 3020: spearow vs hitmonlee\nSimulating battle 411 of 3020: spearow vs dodrio\nSimulating battle 412 of 3020: spearow vs marowak\nSimulating battle 413 of 3020: spearow vs ponyta\nSimulating battle 414 of 3020: spearow vs snorlax\nSimulating battle 415 of 3020: spearow vs nidoking\nSimulating battle 416 of 3020: spearow vs dugtrio\nSimulating battle 417 of 3020: spearow vs koffing\nSimulating battle 418 of 3020: spearow vs kingler\nSimulating battle 419 of 3020: spearow vs paras\nSimulating battle 420 of 3020: spearow vs dratini\nSimulating battle 421 of 3020: fearow vs slowpoke\nSimulating battle 422 of 3020: fearow vs marowak\nSimulating battle 423 of 3020: fearow vs aerodactyl\nSimulating battle 424 of 3020: fearow vs bulbasaur\nSimulating battle 425 of 3020: fearow vs graveler\nSimulating battle 426 of 3020: fearow vs golduck\nSimulating battle 427 of 3020: fearow vs rhydon\nSimulating battle 428 of 3020: fearow vs mewtwo\nSimulating battle 429 of 3020: fearow vs doduo\nSimulating battle 430 of 3020: fearow vs starmie\nSimulating battle 431 of 3020: fearow vs kangaskhan\nSimulating battle 432 of 3020: fearow vs mankey\nSimulating battle 433 of 3020: fearow vs ditto\nSimulating battle 434 of 3020: fearow vs scyther\nSimulating battle 435 of 3020: fearow vs vileplume\nSimulating battle 436 of 3020: fearow vs ekans\nSimulating battle 437 of 3020: fearow vs geodude\nSimulating battle 438 of 3020: fearow vs eevee\nSimulating battle 439 of 3020: fearow vs dewgong\nSimulating battle 440 of 3020: fearow vs pikachu\nSimulating battle 441 of 3020: ekans vs poliwrath\nSimulating battle 442 of 3020: ekans vs magnemite\nSimulating battle 443 of 3020: ekans vs arcanine\nSimulating battle 444 of 3020: ekans vs meowth\nSimulating battle 445 of 3020: ekans vs jigglypuff\nSimulating battle 446 of 3020: ekans vs squirtle\nSimulating battle 447 of 3020: ekans vs butterfree\nSimulating battle 448 of 3020: ekans vs kadabra\nSimulating battle 449 of 3020: ekans vs scyther\nSimulating battle 450 of 3020: ekans vs rattata\nSimulating battle 451 of 3020: ekans vs goldeen\nSimulating battle 452 of 3020: ekans vs lickitung\nSimulating battle 453 of 3020: ekans vs dragonite\nSimulating battle 454 of 3020: ekans vs dugtrio\nSimulating battle 455 of 3020: ekans vs voltorb\nSimulating battle 456 of 3020: ekans vs tauros\nSimulating battle 457 of 3020: ekans vs cubone\nSimulating battle 458 of 3020: ekans vs ivysaur\nSimulating battle 459 of 3020: ekans vs nidoran-f\nSimulating battle 460 of 3020: ekans vs weezing\nSimulating battle 461 of 3020: arbok vs growlithe\nSimulating battle 462 of 3020: arbok vs parasect\nSimulating battle 463 of 3020: arbok vs vaporeon\nSimulating battle 464 of 3020: arbok vs staryu\nSimulating battle 465 of 3020: arbok vs weedle\nSimulating battle 466 of 3020: arbok vs articuno\nSimulating battle 467 of 3020: arbok vs alakazam\nSimulating battle 468 of 3020: arbok vs nidorino\nSimulating battle 469 of 3020: arbok vs goldeen\nSimulating battle 470 of 3020: arbok vs clefable\nSimulating battle 471 of 3020: arbok vs porygon\nSimulating battle 472 of 3020: arbok vs zapdos\nSimulating battle 473 of 3020: arbok vs farfetchd\nSimulating battle 474 of 3020: arbok vs kangaskhan\nSimulating battle 475 of 3020: arbok vs lapras\nSimulating battle 476 of 3020: arbok vs rhyhorn\nSimulating battle 477 of 3020: arbok vs aerodactyl\nSimulating battle 478 of 3020: arbok vs horsea\nSimulating battle 479 of 3020: arbok vs golbat\nSimulating battle 480 of 3020: arbok vs scyther\nSimulating battle 481 of 3020: pikachu vs seadra\nSimulating battle 482 of 3020: pikachu vs machamp\nSimulating battle 483 of 3020: pikachu vs alakazam\nSimulating battle 484 of 3020: pikachu vs tentacool\nSimulating battle 485 of 3020: pikachu vs jolteon\nSimulating battle 486 of 3020: pikachu vs magmar\nSimulating battle 487 of 3020: pikachu vs abra\nSimulating battle 488 of 3020: pikachu vs tangela\nSimulating battle 489 of 3020: pikachu vs raticate\nSimulating battle 490 of 3020: pikachu vs graveler\nSimulating battle 491 of 3020: pikachu vs poliwrath\nSimulating battle 492 of 3020: pikachu vs victreebel\nSimulating battle 493 of 3020: pikachu vs dewgong\nSimulating battle 494 of 3020: pikachu vs farfetchd\nSimulating battle 495 of 3020: pikachu vs kabuto\nSimulating battle 496 of 3020: pikachu vs spearow\nSimulating battle 497 of 3020: pikachu vs vulpix\nSimulating battle 498 of 3020: pikachu vs wigglytuff\nSimulating battle 499 of 3020: pikachu vs poliwhirl\nSimulating battle 500 of 3020: pikachu vs voltorb\nSimulating battle 501 of 3020: raichu vs zubat\nSimulating battle 502 of 3020: raichu vs mankey\nSimulating battle 503 of 3020: raichu vs pidgeotto\nSimulating battle 504 of 3020: raichu vs lickitung\nSimulating battle 505 of 3020: raichu vs hitmonlee\nSimulating battle 506 of 3020: raichu vs seel\nSimulating battle 507 of 3020: raichu vs kabuto\nSimulating battle 508 of 3020: raichu vs starmie\nSimulating battle 509 of 3020: raichu vs pidgey\nSimulating battle 510 of 3020: raichu vs psyduck\nSimulating battle 511 of 3020: raichu vs koffing\nSimulating battle 512 of 3020: raichu vs electrode\nSimulating battle 513 of 3020: raichu vs mew\nSimulating battle 514 of 3020: raichu vs charizard\nSimulating battle 515 of 3020: raichu vs dragonite\nSimulating battle 516 of 3020: raichu vs kingler\nSimulating battle 517 of 3020: raichu vs jynx\nSimulating battle 518 of 3020: raichu vs ivysaur\nSimulating battle 519 of 3020: raichu vs gastly\nSimulating battle 520 of 3020: raichu vs rapidash\nSimulating battle 521 of 3020: sandshrew vs electrode\nSimulating battle 522 of 3020: sandshrew vs koffing\nSimulating battle 523 of 3020: sandshrew vs omastar\nSimulating battle 524 of 3020: sandshrew vs kabutops\nSimulating battle 525 of 3020: sandshrew vs growlithe\nSimulating battle 526 of 3020: sandshrew vs magmar\nSimulating battle 527 of 3020: sandshrew vs victreebel\nSimulating battle 528 of 3020: sandshrew vs chansey\nSimulating battle 529 of 3020: sandshrew vs wartortle\nSimulating battle 530 of 3020: sandshrew vs grimer\nSimulating battle 531 of 3020: sandshrew vs marowak\nSimulating battle 532 of 3020: sandshrew vs gloom\nSimulating battle 533 of 3020: sandshrew vs starmie\nSimulating battle 534 of 3020: sandshrew vs nidoking\nSimulating battle 535 of 3020: sandshrew vs omanyte\nSimulating battle 536 of 3020: sandshrew vs squirtle\nSimulating battle 537 of 3020: sandshrew vs exeggcute\nSimulating battle 538 of 3020: sandshrew vs moltres\nSimulating battle 539 of 3020: sandshrew vs fearow\nSimulating battle 540 of 3020: sandshrew vs rhyhorn\nSimulating battle 541 of 3020: sandslash vs clefable\nSimulating battle 542 of 3020: sandslash vs staryu\nSimulating battle 543 of 3020: sandslash vs venonat\nSimulating battle 544 of 3020: sandslash vs weedle\nSimulating battle 545 of 3020: sandslash vs machamp\nSimulating battle 546 of 3020: sandslash vs kingler\nSimulating battle 547 of 3020: sandslash vs dodrio\nSimulating battle 548 of 3020: sandslash vs mankey\nSimulating battle 549 of 3020: sandslash vs goldeen\nSimulating battle 550 of 3020: sandslash vs grimer\nSimulating battle 551 of 3020: sandslash vs tentacruel\nSimulating battle 552 of 3020: sandslash vs koffing\nSimulating battle 553 of 3020: sandslash vs machop\nSimulating battle 554 of 3020: sandslash vs spearow\nSimulating battle 555 of 3020: sandslash vs mr-mime\nSimulating battle 556 of 3020: sandslash vs charmeleon\nSimulating battle 557 of 3020: sandslash vs kabuto\nSimulating battle 558 of 3020: sandslash vs kakuna\nSimulating battle 559 of 3020: sandslash vs cloyster\nSimulating battle 560 of 3020: sandslash vs arcanine\nSimulating battle 561 of 3020: nidoran-f vs pidgeot\nSimulating battle 562 of 3020: nidoran-f vs metapod\nSimulating battle 563 of 3020: nidoran-f vs wartortle\nSimulating battle 564 of 3020: nidoran-f vs alakazam\nSimulating battle 565 of 3020: nidoran-f vs persian\nSimulating battle 566 of 3020: nidoran-f vs charizard\nSimulating battle 567 of 3020: nidoran-f vs zubat\nSimulating battle 568 of 3020: nidoran-f vs abra\nSimulating battle 569 of 3020: nidoran-f vs nidoking\nSimulating battle 570 of 3020: nidoran-f vs scyther\nSimulating battle 571 of 3020: nidoran-f vs nidoqueen\nSimulating battle 572 of 3020: nidoran-f vs moltres\nSimulating battle 573 of 3020: nidoran-f vs primeape\nSimulating battle 574 of 3020: nidoran-f vs starmie\nSimulating battle 575 of 3020: nidoran-f vs machoke\nSimulating battle 576 of 3020: nidoran-f vs drowzee\nSimulating battle 577 of 3020: nidoran-f vs gloom\nSimulating battle 578 of 3020: nidoran-f vs oddish\nSimulating battle 579 of 3020: nidoran-f vs magnemite\nSimulating battle 580 of 3020: nidoran-f vs sandslash\nSimulating battle 581 of 3020: nidorina vs mewtwo\nSimulating battle 582 of 3020: nidorina vs squirtle\nSimulating battle 583 of 3020: nidorina vs magnemite\nSimulating battle 584 of 3020: nidorina vs dragonite\nSimulating battle 585 of 3020: nidorina vs krabby\nSimulating battle 586 of 3020: nidorina vs exeggutor\nSimulating battle 587 of 3020: nidorina vs meowth\nSimulating battle 588 of 3020: nidorina vs raticate\nSimulating battle 589 of 3020: nidorina vs kadabra\nSimulating battle 590 of 3020: nidorina vs sandshrew\nSimulating battle 591 of 3020: nidorina vs slowpoke\nSimulating battle 592 of 3020: nidorina vs nidoran-m\nSimulating battle 593 of 3020: nidorina vs moltres\nSimulating battle 594 of 3020: nidorina vs metapod\nSimulating battle 595 of 3020: nidorina vs shellder\nSimulating battle 596 of 3020: nidorina vs omanyte\nSimulating battle 597 of 3020: nidorina vs rhyhorn\nSimulating battle 598 of 3020: nidorina vs drowzee\nSimulating battle 599 of 3020: nidorina vs pidgeot\nSimulating battle 600 of 3020: nidorina vs lapras\nSimulating battle 601 of 3020: nidoqueen vs muk\nSimulating battle 602 of 3020: nidoqueen vs charmander\nSimulating battle 603 of 3020: nidoqueen vs koffing\nSimulating battle 604 of 3020: nidoqueen vs pinsir\nSimulating battle 605 of 3020: nidoqueen vs sandslash\nSimulating battle 606 of 3020: nidoqueen vs rhydon\nSimulating battle 607 of 3020: nidoqueen vs gengar\nSimulating battle 608 of 3020: nidoqueen vs seaking\nSimulating battle 609 of 3020: nidoqueen vs zubat\nSimulating battle 610 of 3020: nidoqueen vs chansey\nSimulating battle 611 of 3020: nidoqueen vs parasect\nSimulating battle 612 of 3020: nidoqueen vs jolteon\nSimulating battle 613 of 3020: nidoqueen vs victreebel\nSimulating battle 614 of 3020: nidoqueen vs omastar\nSimulating battle 615 of 3020: nidoqueen vs electabuzz\nSimulating battle 616 of 3020: nidoqueen vs starmie\nSimulating battle 617 of 3020: nidoqueen vs weepinbell\nSimulating battle 618 of 3020: nidoqueen vs doduo\nSimulating battle 619 of 3020: nidoqueen vs kadabra\nSimulating battle 620 of 3020: nidoqueen vs ekans\nSimulating battle 621 of 3020: nidoran-m vs tentacruel\nSimulating battle 622 of 3020: nidoran-m vs seadra\nSimulating battle 623 of 3020: nidoran-m vs kadabra\nSimulating battle 624 of 3020: nidoran-m vs staryu\nSimulating battle 625 of 3020: nidoran-m vs dratini\nSimulating battle 626 of 3020: nidoran-m vs kingler\nSimulating battle 627 of 3020: nidoran-m vs grimer\nSimulating battle 628 of 3020: nidoran-m vs wartortle\nSimulating battle 629 of 3020: nidoran-m vs tauros\nSimulating battle 630 of 3020: nidoran-m vs dodrio\nSimulating battle 631 of 3020: nidoran-m vs venonat\nSimulating battle 632 of 3020: nidoran-m vs magmar\nSimulating battle 633 of 3020: nidoran-m vs mankey\nSimulating battle 634 of 3020: nidoran-m vs gastly\nSimulating battle 635 of 3020: nidoran-m vs machamp\nSimulating battle 636 of 3020: nidoran-m vs muk\nSimulating battle 637 of 3020: nidoran-m vs tentacool\nSimulating battle 638 of 3020: nidoran-m vs articuno\nSimulating battle 639 of 3020: nidoran-m vs venusaur\nSimulating battle 640 of 3020: nidoran-m vs vaporeon\nSimulating battle 641 of 3020: nidorino vs diglett\nSimulating battle 642 of 3020: nidorino vs fearow\nSimulating battle 643 of 3020: nidorino vs abra\nSimulating battle 644 of 3020: nidorino vs hitmonlee\nSimulating battle 645 of 3020: nidorino vs pinsir\nSimulating battle 646 of 3020: nidorino vs articuno\nSimulating battle 647 of 3020: nidorino vs scyther\nSimulating battle 648 of 3020: nidorino vs horsea\nSimulating battle 649 of 3020: nidorino vs charmeleon\nSimulating battle 650 of 3020: nidorino vs arbok\nSimulating battle 651 of 3020: nidorino vs ponyta\nSimulating battle 652 of 3020: nidorino vs growlithe\nSimulating battle 653 of 3020: nidorino vs marowak\nSimulating battle 654 of 3020: nidorino vs kadabra\nSimulating battle 655 of 3020: nidorino vs slowbro\nSimulating battle 656 of 3020: nidorino vs mewtwo\nSimulating battle 657 of 3020: nidorino vs drowzee\nSimulating battle 658 of 3020: nidorino vs snorlax\nSimulating battle 659 of 3020: nidorino vs porygon\nSimulating battle 660 of 3020: nidorino vs shellder\nSimulating battle 661 of 3020: nidoking vs weezing\nSimulating battle 662 of 3020: nidoking vs aerodactyl\nSimulating battle 663 of 3020: nidoking vs seel\nSimulating battle 664 of 3020: nidoking vs gastly\nSimulating battle 665 of 3020: nidoking vs goldeen\nSimulating battle 666 of 3020: nidoking vs victreebel\nSimulating battle 667 of 3020: nidoking vs slowbro\nSimulating battle 668 of 3020: nidoking vs machop\nSimulating battle 669 of 3020: nidoking vs poliwhirl\nSimulating battle 670 of 3020: nidoking vs nidoqueen\nSimulating battle 671 of 3020: nidoking vs dugtrio\nSimulating battle 672 of 3020: nidoking vs magneton\nSimulating battle 673 of 3020: nidoking vs omastar\nSimulating battle 674 of 3020: nidoking vs venomoth\nSimulating battle 675 of 3020: nidoking vs primeape\nSimulating battle 676 of 3020: nidoking vs electabuzz\nSimulating battle 677 of 3020: nidoking vs tentacool\nSimulating battle 678 of 3020: nidoking vs flareon\nSimulating battle 679 of 3020: nidoking vs geodude\nSimulating battle 680 of 3020: nidoking vs raichu\nSimulating battle 681 of 3020: clefairy vs dugtrio\nSimulating battle 682 of 3020: clefairy vs ponyta\nSimulating battle 683 of 3020: clefairy vs poliwag\nSimulating battle 684 of 3020: clefairy vs gengar\nSimulating battle 685 of 3020: clefairy vs parasect\nSimulating battle 686 of 3020: clefairy vs slowpoke\nSimulating battle 687 of 3020: clefairy vs charmander\nSimulating battle 688 of 3020: clefairy vs omanyte\nSimulating battle 689 of 3020: clefairy vs nidorino\nSimulating battle 690 of 3020: clefairy vs tentacool\nSimulating battle 691 of 3020: clefairy vs butterfree\nSimulating battle 692 of 3020: clefairy vs kakuna\nSimulating battle 693 of 3020: clefairy vs snorlax\nSimulating battle 694 of 3020: clefairy vs golem\nSimulating battle 695 of 3020: clefairy vs pinsir\nSimulating battle 696 of 3020: clefairy vs sandshrew\nSimulating battle 697 of 3020: clefairy vs dragonair\nSimulating battle 698 of 3020: clefairy vs geodude\nSimulating battle 699 of 3020: clefairy vs mr-mime\nSimulating battle 700 of 3020: clefairy vs jynx\nSimulating battle 701 of 3020: clefable vs tangela\nSimulating battle 702 of 3020: clefable vs muk\nSimulating battle 703 of 3020: clefable vs venomoth\nSimulating battle 704 of 3020: clefable vs kakuna\nSimulating battle 705 of 3020: clefable vs machop\nSimulating battle 706 of 3020: clefable vs jynx\nSimulating battle 707 of 3020: clefable vs nidorina\nSimulating battle 708 of 3020: clefable vs pidgeotto\nSimulating battle 709 of 3020: clefable vs cubone\nSimulating battle 710 of 3020: clefable vs pinsir\nSimulating battle 711 of 3020: clefable vs rattata\nSimulating battle 712 of 3020: clefable vs dragonite\nSimulating battle 713 of 3020: clefable vs wigglytuff\nSimulating battle 714 of 3020: clefable vs moltres\nSimulating battle 715 of 3020: clefable vs slowpoke\nSimulating battle 716 of 3020: clefable vs fearow\nSimulating battle 717 of 3020: clefable vs alakazam\nSimulating battle 718 of 3020: clefable vs nidoqueen\nSimulating battle 719 of 3020: clefable vs articuno\nSimulating battle 720 of 3020: clefable vs lickitung\nSimulating battle 721 of 3020: vulpix vs arcanine\nSimulating battle 722 of 3020: vulpix vs jolteon\nSimulating battle 723 of 3020: vulpix vs kingler\nSimulating battle 724 of 3020: vulpix vs seadra\nSimulating battle 725 of 3020: vulpix vs kangaskhan\nSimulating battle 726 of 3020: vulpix vs rapidash\nSimulating battle 727 of 3020: vulpix vs rhyhorn\nSimulating battle 728 of 3020: vulpix vs slowbro\nSimulating battle 729 of 3020: vulpix vs dratini\nSimulating battle 730 of 3020: vulpix vs pidgey\nSimulating battle 731 of 3020: vulpix vs raichu\nSimulating battle 732 of 3020: vulpix vs golduck\nSimulating battle 733 of 3020: vulpix vs mankey\nSimulating battle 734 of 3020: vulpix vs bellsprout\nSimulating battle 735 of 3020: vulpix vs spearow\nSimulating battle 736 of 3020: vulpix vs golbat\nSimulating battle 737 of 3020: vulpix vs abra\nSimulating battle 738 of 3020: vulpix vs paras\nSimulating battle 739 of 3020: vulpix vs snorlax\nSimulating battle 740 of 3020: vulpix vs raticate\nSimulating battle 741 of 3020: ninetales vs golbat\nSimulating battle 742 of 3020: ninetales vs bulbasaur\nSimulating battle 743 of 3020: ninetales vs hitmonlee\nSimulating battle 744 of 3020: ninetales vs seadra\nSimulating battle 745 of 3020: ninetales vs mr-mime\nSimulating battle 746 of 3020: ninetales vs golem\nSimulating battle 747 of 3020: ninetales vs blastoise\nSimulating battle 748 of 3020: ninetales vs poliwhirl\nSimulating battle 749 of 3020: ninetales vs graveler\nSimulating battle 750 of 3020: ninetales vs geodude\nSimulating battle 751 of 3020: ninetales vs goldeen\nSimulating battle 752 of 3020: ninetales vs rattata\nSimulating battle 753 of 3020: ninetales vs bellsprout\nSimulating battle 754 of 3020: ninetales vs meowth\nSimulating battle 755 of 3020: ninetales vs weezing\nSimulating battle 756 of 3020: ninetales vs nidorina\nSimulating battle 757 of 3020: ninetales vs kabutops\nSimulating battle 758 of 3020: ninetales vs arcanine\nSimulating battle 759 of 3020: ninetales vs wigglytuff\nSimulating battle 760 of 3020: ninetales vs weepinbell\nSimulating battle 761 of 3020: jigglypuff vs vulpix\nSimulating battle 762 of 3020: jigglypuff vs rattata\nSimulating battle 763 of 3020: jigglypuff vs pidgey\nSimulating battle 764 of 3020: jigglypuff vs gloom\nSimulating battle 765 of 3020: jigglypuff vs slowbro\nSimulating battle 766 of 3020: jigglypuff vs dratini\nSimulating battle 767 of 3020: jigglypuff vs graveler\nSimulating battle 768 of 3020: jigglypuff vs tangela\nSimulating battle 769 of 3020: jigglypuff vs nidoran-m\nSimulating battle 770 of 3020: jigglypuff vs starmie\nSimulating battle 771 of 3020: jigglypuff vs slowpoke\nSimulating battle 772 of 3020: jigglypuff vs marowak\nSimulating battle 773 of 3020: jigglypuff vs victreebel\nSimulating battle 774 of 3020: jigglypuff vs gyarados\nSimulating battle 775 of 3020: jigglypuff vs kabuto\nSimulating battle 776 of 3020: jigglypuff vs tauros\nSimulating battle 777 of 3020: jigglypuff vs spearow\nSimulating battle 778 of 3020: jigglypuff vs metapod\nSimulating battle 779 of 3020: jigglypuff vs rhydon\nSimulating battle 780 of 3020: jigglypuff vs doduo\nSimulating battle 781 of 3020: wigglytuff vs machop\nSimulating battle 782 of 3020: wigglytuff vs squirtle\nSimulating battle 783 of 3020: wigglytuff vs arbok\nSimulating battle 784 of 3020: wigglytuff vs poliwag\nSimulating battle 785 of 3020: wigglytuff vs dragonite\nSimulating battle 786 of 3020: wigglytuff vs charizard\nSimulating battle 787 of 3020: wigglytuff vs weepinbell\nSimulating battle 788 of 3020: wigglytuff vs metapod\nSimulating battle 789 of 3020: wigglytuff vs paras\nSimulating battle 790 of 3020: wigglytuff vs mr-mime\nSimulating battle 791 of 3020: wigglytuff vs vaporeon\nSimulating battle 792 of 3020: wigglytuff vs kangaskhan\nSimulating battle 793 of 3020: wigglytuff vs tentacruel\nSimulating battle 794 of 3020: wigglytuff vs venonat\nSimulating battle 795 of 3020: wigglytuff vs mew\nSimulating battle 796 of 3020: wigglytuff vs chansey\nSimulating battle 797 of 3020: wigglytuff vs pinsir\nSimulating battle 798 of 3020: wigglytuff vs cloyster\nSimulating battle 799 of 3020: wigglytuff vs hitmonlee\nSimulating battle 800 of 3020: wigglytuff vs dewgong\nSimulating battle 801 of 3020: zubat vs doduo\nSimulating battle 802 of 3020: zubat vs sandshrew\nSimulating battle 803 of 3020: zubat vs oddish\nSimulating battle 804 of 3020: zubat vs seel\nSimulating battle 805 of 3020: zubat vs hitmonchan\nSimulating battle 806 of 3020: zubat vs tauros\nSimulating battle 807 of 3020: zubat vs graveler\nSimulating battle 808 of 3020: zubat vs cubone\nSimulating battle 809 of 3020: zubat vs aerodactyl\nSimulating battle 810 of 3020: zubat vs caterpie\nSimulating battle 811 of 3020: zubat vs goldeen\nSimulating battle 812 of 3020: zubat vs ekans\nSimulating battle 813 of 3020: zubat vs magneton\nSimulating battle 814 of 3020: zubat vs machop\nSimulating battle 815 of 3020: zubat vs nidorina\nSimulating battle 816 of 3020: zubat vs marowak\nSimulating battle 817 of 3020: zubat vs eevee\nSimulating battle 818 of 3020: zubat vs bulbasaur\nSimulating battle 819 of 3020: zubat vs kabuto\nSimulating battle 820 of 3020: zubat vs staryu\nSimulating battle 821 of 3020: golbat vs hitmonchan\nSimulating battle 822 of 3020: golbat vs kakuna\nSimulating battle 823 of 3020: golbat vs diglett\nSimulating battle 824 of 3020: golbat vs vaporeon\nSimulating battle 825 of 3020: golbat vs gengar\nSimulating battle 826 of 3020: golbat vs magikarp\nSimulating battle 827 of 3020: golbat vs kangaskhan\nSimulating battle 828 of 3020: golbat vs psyduck\nSimulating battle 829 of 3020: golbat vs weepinbell\nSimulating battle 830 of 3020: golbat vs aerodactyl\nSimulating battle 831 of 3020: golbat vs nidoking\nSimulating battle 832 of 3020: golbat vs graveler\nSimulating battle 833 of 3020: golbat vs tangela\nSimulating battle 834 of 3020: golbat vs magmar\nSimulating battle 835 of 3020: golbat vs nidoran-m\nSimulating battle 836 of 3020: golbat vs wartortle\nSimulating battle 837 of 3020: golbat vs abra\nSimulating battle 838 of 3020: golbat vs zubat\nSimulating battle 839 of 3020: golbat vs magnemite\nSimulating battle 840 of 3020: golbat vs snorlax\nSimulating battle 841 of 3020: oddish vs charmander\nSimulating battle 842 of 3020: oddish vs snorlax\nSimulating battle 843 of 3020: oddish vs hitmonlee\nSimulating battle 844 of 3020: oddish vs arbok\nSimulating battle 845 of 3020: oddish vs arcanine\nSimulating battle 846 of 3020: oddish vs nidorina\nSimulating battle 847 of 3020: oddish vs staryu\nSimulating battle 848 of 3020: oddish vs nidoqueen\nSimulating battle 849 of 3020: oddish vs wigglytuff\nSimulating battle 850 of 3020: oddish vs magikarp\nSimulating battle 851 of 3020: oddish vs golem\nSimulating battle 852 of 3020: oddish vs ditto\nSimulating battle 853 of 3020: oddish vs victreebel\nSimulating battle 854 of 3020: oddish vs lickitung\nSimulating battle 855 of 3020: oddish vs electabuzz\nSimulating battle 856 of 3020: oddish vs mr-mime\nSimulating battle 857 of 3020: oddish vs kadabra\nSimulating battle 858 of 3020: oddish vs goldeen\nSimulating battle 859 of 3020: oddish vs ninetales\nSimulating battle 860 of 3020: oddish vs voltorb\nSimulating battle 861 of 3020: gloom vs diglett\nSimulating battle 862 of 3020: gloom vs ditto\nSimulating battle 863 of 3020: gloom vs clefairy\nSimulating battle 864 of 3020: gloom vs pidgeot\nSimulating battle 865 of 3020: gloom vs tentacool\nSimulating battle 866 of 3020: gloom vs lickitung\nSimulating battle 867 of 3020: gloom vs muk\nSimulating battle 868 of 3020: gloom vs lapras\nSimulating battle 869 of 3020: gloom vs weepinbell\nSimulating battle 870 of 3020: gloom vs bulbasaur\nSimulating battle 871 of 3020: gloom vs geodude\nSimulating battle 872 of 3020: gloom vs rapidash\nSimulating battle 873 of 3020: gloom vs mewtwo\nSimulating battle 874 of 3020: gloom vs pinsir\nSimulating battle 875 of 3020: gloom vs jigglypuff\nSimulating battle 876 of 3020: gloom vs horsea\nSimulating battle 877 of 3020: gloom vs omastar\nSimulating battle 878 of 3020: gloom vs electabuzz\nSimulating battle 879 of 3020: gloom vs shellder\nSimulating battle 880 of 3020: gloom vs dewgong\nSimulating battle 881 of 3020: vileplume vs snorlax\nSimulating battle 882 of 3020: vileplume vs kabutops\nSimulating battle 883 of 3020: vileplume vs krabby\nSimulating battle 884 of 3020: vileplume vs goldeen\nSimulating battle 885 of 3020: vileplume vs doduo\nSimulating battle 886 of 3020: vileplume vs diglett\nSimulating battle 887 of 3020: vileplume vs abra\nSimulating battle 888 of 3020: vileplume vs dragonair\nSimulating battle 889 of 3020: vileplume vs voltorb\nSimulating battle 890 of 3020: vileplume vs poliwhirl\nSimulating battle 891 of 3020: vileplume vs hitmonchan\nSimulating battle 892 of 3020: vileplume vs butterfree\nSimulating battle 893 of 3020: vileplume vs farfetchd\nSimulating battle 894 of 3020: vileplume vs scyther\nSimulating battle 895 of 3020: vileplume vs kingler\nSimulating battle 896 of 3020: vileplume vs jigglypuff\nSimulating battle 897 of 3020: vileplume vs tauros\nSimulating battle 898 of 3020: vileplume vs caterpie\nSimulating battle 899 of 3020: vileplume vs nidorino\nSimulating battle 900 of 3020: vileplume vs gyarados\nSimulating battle 901 of 3020: paras vs seel\nSimulating battle 902 of 3020: paras vs raichu\nSimulating battle 903 of 3020: paras vs tangela\nSimulating battle 904 of 3020: paras vs flareon\nSimulating battle 905 of 3020: paras vs goldeen\nSimulating battle 906 of 3020: paras vs charmander\nSimulating battle 907 of 3020: paras vs vulpix\nSimulating battle 908 of 3020: paras vs hitmonlee\nSimulating battle 909 of 3020: paras vs wigglytuff\nSimulating battle 910 of 3020: paras vs raticate\nSimulating battle 911 of 3020: paras vs mr-mime\nSimulating battle 912 of 3020: paras vs bellsprout\nSimulating battle 913 of 3020: paras vs grimer\nSimulating battle 914 of 3020: paras vs exeggutor\nSimulating battle 915 of 3020: paras vs spearow\nSimulating battle 916 of 3020: paras vs omanyte\nSimulating battle 917 of 3020: paras vs kingler\nSimulating battle 918 of 3020: paras vs farfetchd\nSimulating battle 919 of 3020: paras vs magmar\nSimulating battle 920 of 3020: paras vs kabuto\nSimulating battle 921 of 3020: parasect vs caterpie\nSimulating battle 922 of 3020: parasect vs pidgeot\nSimulating battle 923 of 3020: parasect vs poliwrath\nSimulating battle 924 of 3020: parasect vs graveler\nSimulating battle 925 of 3020: parasect vs poliwag\nSimulating battle 926 of 3020: parasect vs arbok\nSimulating battle 927 of 3020: parasect vs chansey\nSimulating battle 928 of 3020: parasect vs raichu\nSimulating battle 929 of 3020: parasect vs kangaskhan\nSimulating battle 930 of 3020: parasect vs oddish\nSimulating battle 931 of 3020: parasect vs rapidash\nSimulating battle 932 of 3020: parasect vs wartortle\nSimulating battle 933 of 3020: parasect vs butterfree\nSimulating battle 934 of 3020: parasect vs dodrio\nSimulating battle 935 of 3020: parasect vs beedrill\nSimulating battle 936 of 3020: parasect vs ponyta\nSimulating battle 937 of 3020: parasect vs haunter\nSimulating battle 938 of 3020: parasect vs hypno\nSimulating battle 939 of 3020: parasect vs rhydon\nSimulating battle 940 of 3020: parasect vs ninetales\nSimulating battle 941 of 3020: venonat vs kadabra\nSimulating battle 942 of 3020: venonat vs porygon\nSimulating battle 943 of 3020: venonat vs hitmonchan\nSimulating battle 944 of 3020: venonat vs moltres\nSimulating battle 945 of 3020: venonat vs parasect\nSimulating battle 946 of 3020: venonat vs gloom\nSimulating battle 947 of 3020: venonat vs vileplume\nSimulating battle 948 of 3020: venonat vs spearow\nSimulating battle 949 of 3020: venonat vs kingler\nSimulating battle 950 of 3020: venonat vs abra\nSimulating battle 951 of 3020: venonat vs magikarp\nSimulating battle 952 of 3020: venonat vs mew\nSimulating battle 953 of 3020: venonat vs vulpix\nSimulating battle 954 of 3020: venonat vs poliwhirl\nSimulating battle 955 of 3020: venonat vs staryu\nSimulating battle 956 of 3020: venonat vs machoke\nSimulating battle 957 of 3020: venonat vs seaking\nSimulating battle 958 of 3020: venonat vs venusaur\nSimulating battle 959 of 3020: venonat vs starmie\nSimulating battle 960 of 3020: venonat vs graveler\nSimulating battle 961 of 3020: venomoth vs kabutops\nSimulating battle 962 of 3020: venomoth vs zubat\nSimulating battle 963 of 3020: venomoth vs rattata\nSimulating battle 964 of 3020: venomoth vs kangaskhan\nSimulating battle 965 of 3020: venomoth vs shellder\nSimulating battle 966 of 3020: venomoth vs rapidash\nSimulating battle 967 of 3020: venomoth vs weezing\nSimulating battle 968 of 3020: venomoth vs machop\nSimulating battle 969 of 3020: venomoth vs goldeen\nSimulating battle 970 of 3020: venomoth vs slowpoke\nSimulating battle 971 of 3020: venomoth vs meowth\nSimulating battle 972 of 3020: venomoth vs voltorb\nSimulating battle 973 of 3020: venomoth vs electabuzz\nSimulating battle 974 of 3020: venomoth vs sandslash\nSimulating battle 975 of 3020: venomoth vs poliwrath\nSimulating battle 976 of 3020: venomoth vs kingler\nSimulating battle 977 of 3020: venomoth vs dragonair\nSimulating battle 978 of 3020: venomoth vs haunter\nSimulating battle 979 of 3020: venomoth vs dragonite\nSimulating battle 980 of 3020: venomoth vs ponyta\nSimulating battle 981 of 3020: diglett vs ponyta\nSimulating battle 982 of 3020: diglett vs charizard\nSimulating battle 983 of 3020: diglett vs exeggutor\nSimulating battle 984 of 3020: diglett vs tentacool\nSimulating battle 985 of 3020: diglett vs venusaur\nSimulating battle 986 of 3020: diglett vs moltres\nSimulating battle 987 of 3020: diglett vs weedle\nSimulating battle 988 of 3020: diglett vs magikarp\nSimulating battle 989 of 3020: diglett vs graveler\nSimulating battle 990 of 3020: diglett vs poliwag\nSimulating battle 991 of 3020: diglett vs gastly\nSimulating battle 992 of 3020: diglett vs growlithe\nSimulating battle 993 of 3020: diglett vs venomoth\nSimulating battle 994 of 3020: diglett vs machop\nSimulating battle 995 of 3020: diglett vs clefable\nSimulating battle 996 of 3020: diglett vs pikachu\nSimulating battle 997 of 3020: diglett vs metapod\nSimulating battle 998 of 3020: diglett vs magnemite\nSimulating battle 999 of 3020: diglett vs tangela\nSimulating battle 1000 of 3020: diglett vs blastoise\nSimulating battle 1001 of 3020: dugtrio vs mewtwo\nSimulating battle 1002 of 3020: dugtrio vs onix\nSimulating battle 1003 of 3020: dugtrio vs nidoking\nSimulating battle 1004 of 3020: dugtrio vs arbok\nSimulating battle 1005 of 3020: dugtrio vs ponyta\nSimulating battle 1006 of 3020: dugtrio vs dodrio\nSimulating battle 1007 of 3020: dugtrio vs lickitung\nSimulating battle 1008 of 3020: dugtrio vs vileplume\nSimulating battle 1009 of 3020: dugtrio vs persian\nSimulating battle 1010 of 3020: dugtrio vs kabuto\nSimulating battle 1011 of 3020: dugtrio vs porygon\nSimulating battle 1012 of 3020: dugtrio vs gyarados\nSimulating battle 1013 of 3020: dugtrio vs victreebel\nSimulating battle 1014 of 3020: dugtrio vs oddish\nSimulating battle 1015 of 3020: dugtrio vs machoke\nSimulating battle 1016 of 3020: dugtrio vs electabuzz\nSimulating battle 1017 of 3020: dugtrio vs grimer\nSimulating battle 1018 of 3020: dugtrio vs nidorina\nSimulating battle 1019 of 3020: dugtrio vs starmie\nSimulating battle 1020 of 3020: dugtrio vs raticate\nSimulating battle 1021 of 3020: meowth vs vulpix\nSimulating battle 1022 of 3020: meowth vs arcanine\nSimulating battle 1023 of 3020: meowth vs exeggutor\nSimulating battle 1024 of 3020: meowth vs articuno\nSimulating battle 1025 of 3020: meowth vs onix\nSimulating battle 1026 of 3020: meowth vs arbok\nSimulating battle 1027 of 3020: meowth vs charmander\nSimulating battle 1028 of 3020: meowth vs bellsprout\nSimulating battle 1029 of 3020: meowth vs omastar\nSimulating battle 1030 of 3020: meowth vs nidoran-m\nSimulating battle 1031 of 3020: meowth vs goldeen\nSimulating battle 1032 of 3020: meowth vs drowzee\nSimulating battle 1033 of 3020: meowth vs mew\nSimulating battle 1034 of 3020: meowth vs kingler\nSimulating battle 1035 of 3020: meowth vs hypno\nSimulating battle 1036 of 3020: meowth vs sandslash\nSimulating battle 1037 of 3020: meowth vs poliwhirl\nSimulating battle 1038 of 3020: meowth vs mr-mime\nSimulating battle 1039 of 3020: meowth vs squirtle\nSimulating battle 1040 of 3020: meowth vs zapdos\nSimulating battle 1041 of 3020: persian vs dodrio\nSimulating battle 1042 of 3020: persian vs growlithe\nSimulating battle 1043 of 3020: persian vs pidgeotto\nSimulating battle 1044 of 3020: persian vs staryu\nSimulating battle 1045 of 3020: persian vs slowpoke\nSimulating battle 1046 of 3020: persian vs hitmonlee\nSimulating battle 1047 of 3020: persian vs nidorina\nSimulating battle 1048 of 3020: persian vs clefable\nSimulating battle 1049 of 3020: persian vs butterfree\nSimulating battle 1050 of 3020: persian vs caterpie\nSimulating battle 1051 of 3020: persian vs tauros\nSimulating battle 1052 of 3020: persian vs pikachu\nSimulating battle 1053 of 3020: persian vs poliwrath\nSimulating battle 1054 of 3020: persian vs omastar\nSimulating battle 1055 of 3020: persian vs clefairy\nSimulating battle 1056 of 3020: persian vs electrode\nSimulating battle 1057 of 3020: persian vs goldeen\nSimulating battle 1058 of 3020: persian vs drowzee\nSimulating battle 1059 of 3020: persian vs kabuto\nSimulating battle 1060 of 3020: persian vs koffing\nSimulating battle 1061 of 3020: psyduck vs wigglytuff\nSimulating battle 1062 of 3020: psyduck vs lickitung\nSimulating battle 1063 of 3020: psyduck vs raichu\nSimulating battle 1064 of 3020: psyduck vs pinsir\nSimulating battle 1065 of 3020: psyduck vs hitmonlee\nSimulating battle 1066 of 3020: psyduck vs tentacruel\nSimulating battle 1067 of 3020: psyduck vs blastoise\nSimulating battle 1068 of 3020: psyduck vs drowzee\nSimulating battle 1069 of 3020: psyduck vs primeape\nSimulating battle 1070 of 3020: psyduck vs kangaskhan\nSimulating battle 1071 of 3020: psyduck vs poliwrath\nSimulating battle 1072 of 3020: psyduck vs gengar\nSimulating battle 1073 of 3020: psyduck vs kabutops\nSimulating battle 1074 of 3020: psyduck vs haunter\nSimulating battle 1075 of 3020: psyduck vs pidgey\nSimulating battle 1076 of 3020: psyduck vs exeggutor\nSimulating battle 1077 of 3020: psyduck vs tentacool\nSimulating battle 1078 of 3020: psyduck vs venomoth\nSimulating battle 1079 of 3020: psyduck vs nidoran-m\nSimulating battle 1080 of 3020: psyduck vs goldeen\nSimulating battle 1081 of 3020: golduck vs arbok\nSimulating battle 1082 of 3020: golduck vs mankey\nSimulating battle 1083 of 3020: golduck vs charizard\nSimulating battle 1084 of 3020: golduck vs weedle\nSimulating battle 1085 of 3020: golduck vs dewgong\nSimulating battle 1086 of 3020: golduck vs kadabra\nSimulating battle 1087 of 3020: golduck vs nidorino\nSimulating battle 1088 of 3020: golduck vs moltres\nSimulating battle 1089 of 3020: golduck vs persian\nSimulating battle 1090 of 3020: golduck vs pidgeot\nSimulating battle 1091 of 3020: golduck vs snorlax\nSimulating battle 1092 of 3020: golduck vs psyduck\nSimulating battle 1093 of 3020: golduck vs primeape\nSimulating battle 1094 of 3020: golduck vs poliwhirl\nSimulating battle 1095 of 3020: golduck vs seel\nSimulating battle 1096 of 3020: golduck vs ninetales\nSimulating battle 1097 of 3020: golduck vs bulbasaur\nSimulating battle 1098 of 3020: golduck vs tentacool\nSimulating battle 1099 of 3020: golduck vs nidoking\nSimulating battle 1100 of 3020: golduck vs kabuto\nSimulating battle 1101 of 3020: mankey vs machop\nSimulating battle 1102 of 3020: mankey vs vileplume\nSimulating battle 1103 of 3020: mankey vs nidoran-f\nSimulating battle 1104 of 3020: mankey vs squirtle\nSimulating battle 1105 of 3020: mankey vs nidoking\nSimulating battle 1106 of 3020: mankey vs charmander\nSimulating battle 1107 of 3020: mankey vs haunter\nSimulating battle 1108 of 3020: mankey vs poliwrath\nSimulating battle 1109 of 3020: mankey vs doduo\nSimulating battle 1110 of 3020: mankey vs charmeleon\nSimulating battle 1111 of 3020: mankey vs bellsprout\nSimulating battle 1112 of 3020: mankey vs kakuna\nSimulating battle 1113 of 3020: mankey vs nidorino\nSimulating battle 1114 of 3020: mankey vs koffing\nSimulating battle 1115 of 3020: mankey vs flareon\nSimulating battle 1116 of 3020: mankey vs nidorina\nSimulating battle 1117 of 3020: mankey vs pidgeotto\nSimulating battle 1118 of 3020: mankey vs scyther\nSimulating battle 1119 of 3020: mankey vs horsea\nSimulating battle 1120 of 3020: mankey vs gengar\nSimulating battle 1121 of 3020: primeape vs eevee\nSimulating battle 1122 of 3020: primeape vs sandslash\nSimulating battle 1123 of 3020: primeape vs seadra\nSimulating battle 1124 of 3020: primeape vs gyarados\nSimulating battle 1125 of 3020: primeape vs growlithe\nSimulating battle 1126 of 3020: primeape vs butterfree\nSimulating battle 1127 of 3020: primeape vs jolteon\nSimulating battle 1128 of 3020: primeape vs slowpoke\nSimulating battle 1129 of 3020: primeape vs seaking\nSimulating battle 1130 of 3020: primeape vs wartortle\nSimulating battle 1131 of 3020: primeape vs pidgey\nSimulating battle 1132 of 3020: primeape vs jynx\nSimulating battle 1133 of 3020: primeape vs cubone\nSimulating battle 1134 of 3020: primeape vs rhyhorn\nSimulating battle 1135 of 3020: primeape vs pinsir\nSimulating battle 1136 of 3020: primeape vs kangaskhan\nSimulating battle 1137 of 3020: primeape vs rattata\nSimulating battle 1138 of 3020: primeape vs spearow\nSimulating battle 1139 of 3020: primeape vs doduo\nSimulating battle 1140 of 3020: primeape vs ninetales\nSimulating battle 1141 of 3020: growlithe vs pidgeotto\nSimulating battle 1142 of 3020: growlithe vs nidorino\nSimulating battle 1143 of 3020: growlithe vs tentacool\nSimulating battle 1144 of 3020: growlithe vs mew\nSimulating battle 1145 of 3020: growlithe vs aerodactyl\nSimulating battle 1146 of 3020: growlithe vs dodrio\nSimulating battle 1147 of 3020: growlithe vs kingler\nSimulating battle 1148 of 3020: growlithe vs porygon\nSimulating battle 1149 of 3020: growlithe vs ponyta\nSimulating battle 1150 of 3020: growlithe vs goldeen\nSimulating battle 1151 of 3020: growlithe vs lapras\nSimulating battle 1152 of 3020: growlithe vs rhydon\nSimulating battle 1153 of 3020: growlithe vs raichu\nSimulating battle 1154 of 3020: growlithe vs nidorina\nSimulating battle 1155 of 3020: growlithe vs snorlax\nSimulating battle 1156 of 3020: growlithe vs mankey\nSimulating battle 1157 of 3020: growlithe vs seadra\nSimulating battle 1158 of 3020: growlithe vs poliwag\nSimulating battle 1159 of 3020: growlithe vs hitmonchan\nSimulating battle 1160 of 3020: growlithe vs grimer\nSimulating battle 1161 of 3020: arcanine vs goldeen\nSimulating battle 1162 of 3020: arcanine vs cubone\nSimulating battle 1163 of 3020: arcanine vs lickitung\nSimulating battle 1164 of 3020: arcanine vs pikachu\nSimulating battle 1165 of 3020: arcanine vs magneton\nSimulating battle 1166 of 3020: arcanine vs rhyhorn\nSimulating battle 1167 of 3020: arcanine vs machoke\nSimulating battle 1168 of 3020: arcanine vs hypno\nSimulating battle 1169 of 3020: arcanine vs wigglytuff\nSimulating battle 1170 of 3020: arcanine vs scyther\nSimulating battle 1171 of 3020: arcanine vs pidgeot\nSimulating battle 1172 of 3020: arcanine vs arbok\nSimulating battle 1173 of 3020: arcanine vs fearow\nSimulating battle 1174 of 3020: arcanine vs rhydon\nSimulating battle 1175 of 3020: arcanine vs nidoking\nSimulating battle 1176 of 3020: arcanine vs articuno\nSimulating battle 1177 of 3020: arcanine vs pidgey\nSimulating battle 1178 of 3020: arcanine vs zapdos\nSimulating battle 1179 of 3020: arcanine vs seel\nSimulating battle 1180 of 3020: arcanine vs nidoran-m\nSimulating battle 1181 of 3020: poliwag vs hitmonchan\nSimulating battle 1182 of 3020: poliwag vs gastly\nSimulating battle 1183 of 3020: poliwag vs weezing\nSimulating battle 1184 of 3020: poliwag vs kakuna\nSimulating battle 1185 of 3020: poliwag vs graveler\nSimulating battle 1186 of 3020: poliwag vs magnemite\nSimulating battle 1187 of 3020: poliwag vs sandshrew\nSimulating battle 1188 of 3020: poliwag vs dragonite\nSimulating battle 1189 of 3020: poliwag vs lapras\nSimulating battle 1190 of 3020: poliwag vs golduck\nSimulating battle 1191 of 3020: poliwag vs wigglytuff\nSimulating battle 1192 of 3020: poliwag vs electabuzz\nSimulating battle 1193 of 3020: poliwag vs growlithe\nSimulating battle 1194 of 3020: poliwag vs sandslash\nSimulating battle 1195 of 3020: poliwag vs cloyster\nSimulating battle 1196 of 3020: poliwag vs articuno\nSimulating battle 1197 of 3020: poliwag vs drowzee\nSimulating battle 1198 of 3020: poliwag vs nidorina\nSimulating battle 1199 of 3020: poliwag vs tentacruel\nSimulating battle 1200 of 3020: poliwag vs dragonair\nSimulating battle 1201 of 3020: poliwhirl vs growlithe\nSimulating battle 1202 of 3020: poliwhirl vs rhyhorn\nSimulating battle 1203 of 3020: poliwhirl vs zapdos\nSimulating battle 1204 of 3020: poliwhirl vs articuno\nSimulating battle 1205 of 3020: poliwhirl vs squirtle\nSimulating battle 1206 of 3020: poliwhirl vs weepinbell\nSimulating battle 1207 of 3020: poliwhirl vs wartortle\nSimulating battle 1208 of 3020: poliwhirl vs parasect\nSimulating battle 1209 of 3020: poliwhirl vs victreebel\nSimulating battle 1210 of 3020: poliwhirl vs magnemite\nSimulating battle 1211 of 3020: poliwhirl vs grimer\nSimulating battle 1212 of 3020: poliwhirl vs cloyster\nSimulating battle 1213 of 3020: poliwhirl vs ivysaur\nSimulating battle 1214 of 3020: poliwhirl vs vulpix\nSimulating battle 1215 of 3020: poliwhirl vs moltres\nSimulating battle 1216 of 3020: poliwhirl vs cubone\nSimulating battle 1217 of 3020: poliwhirl vs pidgeot\nSimulating battle 1218 of 3020: poliwhirl vs arbok\nSimulating battle 1219 of 3020: poliwhirl vs porygon\nSimulating battle 1220 of 3020: poliwhirl vs mankey\nSimulating battle 1221 of 3020: poliwrath vs krabby\nSimulating battle 1222 of 3020: poliwrath vs koffing\nSimulating battle 1223 of 3020: poliwrath vs goldeen\nSimulating battle 1224 of 3020: poliwrath vs muk\nSimulating battle 1225 of 3020: poliwrath vs zubat\nSimulating battle 1226 of 3020: poliwrath vs drowzee\nSimulating battle 1227 of 3020: poliwrath vs magnemite\nSimulating battle 1228 of 3020: poliwrath vs dodrio\nSimulating battle 1229 of 3020: poliwrath vs dratini\nSimulating battle 1230 of 3020: poliwrath vs fearow\nSimulating battle 1231 of 3020: poliwrath vs kakuna\nSimulating battle 1232 of 3020: poliwrath vs wigglytuff\nSimulating battle 1233 of 3020: poliwrath vs weedle\nSimulating battle 1234 of 3020: poliwrath vs spearow\nSimulating battle 1235 of 3020: poliwrath vs victreebel\nSimulating battle 1236 of 3020: poliwrath vs kangaskhan\nSimulating battle 1237 of 3020: poliwrath vs weezing\nSimulating battle 1238 of 3020: poliwrath vs magmar\nSimulating battle 1239 of 3020: poliwrath vs lickitung\nSimulating battle 1240 of 3020: poliwrath vs mankey\nSimulating battle 1241 of 3020: abra vs eevee\nSimulating battle 1242 of 3020: abra vs nidorina\nSimulating battle 1243 of 3020: abra vs shellder\nSimulating battle 1244 of 3020: abra vs rhydon\nSimulating battle 1245 of 3020: abra vs nidoran-f\nSimulating battle 1246 of 3020: abra vs geodude\nSimulating battle 1247 of 3020: abra vs magmar\nSimulating battle 1248 of 3020: abra vs flareon\nSimulating battle 1249 of 3020: abra vs slowbro\nSimulating battle 1250 of 3020: abra vs butterfree\nSimulating battle 1251 of 3020: abra vs primeape\nSimulating battle 1252 of 3020: abra vs exeggutor\nSimulating battle 1253 of 3020: abra vs beedrill\nSimulating battle 1254 of 3020: abra vs ivysaur\nSimulating battle 1255 of 3020: abra vs persian\nSimulating battle 1256 of 3020: abra vs slowpoke\nSimulating battle 1257 of 3020: abra vs golduck\nSimulating battle 1258 of 3020: abra vs clefable\nSimulating battle 1259 of 3020: abra vs machoke\nSimulating battle 1260 of 3020: abra vs golem\nSimulating battle 1261 of 3020: kadabra vs dodrio\nSimulating battle 1262 of 3020: kadabra vs nidoqueen\nSimulating battle 1263 of 3020: kadabra vs ivysaur\nSimulating battle 1264 of 3020: kadabra vs magikarp\nSimulating battle 1265 of 3020: kadabra vs rhydon\nSimulating battle 1266 of 3020: kadabra vs vileplume\nSimulating battle 1267 of 3020: kadabra vs nidoking\nSimulating battle 1268 of 3020: kadabra vs kingler\nSimulating battle 1269 of 3020: kadabra vs omanyte\nSimulating battle 1270 of 3020: kadabra vs arcanine\nSimulating battle 1271 of 3020: kadabra vs gyarados\nSimulating battle 1272 of 3020: kadabra vs zapdos\nSimulating battle 1273 of 3020: kadabra vs gastly\nSimulating battle 1274 of 3020: kadabra vs rattata\nSimulating battle 1275 of 3020: kadabra vs exeggutor\nSimulating battle 1276 of 3020: kadabra vs metapod\nSimulating battle 1277 of 3020: kadabra vs chansey\nSimulating battle 1278 of 3020: kadabra vs charmeleon\nSimulating battle 1279 of 3020: kadabra vs seaking\nSimulating battle 1280 of 3020: kadabra vs raticate\nSimulating battle 1281 of 3020: alakazam vs magneton\nSimulating battle 1282 of 3020: alakazam vs dragonite\nSimulating battle 1283 of 3020: alakazam vs rhyhorn\nSimulating battle 1284 of 3020: alakazam vs dragonair\nSimulating battle 1285 of 3020: alakazam vs marowak\nSimulating battle 1286 of 3020: alakazam vs lickitung\nSimulating battle 1287 of 3020: alakazam vs golem\nSimulating battle 1288 of 3020: alakazam vs nidorina\nSimulating battle 1289 of 3020: alakazam vs charizard\nSimulating battle 1290 of 3020: alakazam vs dodrio\nSimulating battle 1291 of 3020: alakazam vs gloom\nSimulating battle 1292 of 3020: alakazam vs seaking\nSimulating battle 1293 of 3020: alakazam vs gengar\nSimulating battle 1294 of 3020: alakazam vs ekans\nSimulating battle 1295 of 3020: alakazam vs chansey\nSimulating battle 1296 of 3020: alakazam vs sandslash\nSimulating battle 1297 of 3020: alakazam vs abra\nSimulating battle 1298 of 3020: alakazam vs cubone\nSimulating battle 1299 of 3020: alakazam vs flareon\nSimulating battle 1300 of 3020: alakazam vs spearow\nSimulating battle 1301 of 3020: machop vs exeggutor\nSimulating battle 1302 of 3020: machop vs magnemite\nSimulating battle 1303 of 3020: machop vs grimer\nSimulating battle 1304 of 3020: machop vs primeape\nSimulating battle 1305 of 3020: machop vs dewgong\nSimulating battle 1306 of 3020: machop vs gloom\nSimulating battle 1307 of 3020: machop vs raticate\nSimulating battle 1308 of 3020: machop vs ditto\nSimulating battle 1309 of 3020: machop vs nidorina\nSimulating battle 1310 of 3020: machop vs porygon\nSimulating battle 1311 of 3020: machop vs diglett\nSimulating battle 1312 of 3020: machop vs cloyster\nSimulating battle 1313 of 3020: machop vs ninetales\nSimulating battle 1314 of 3020: machop vs poliwhirl\nSimulating battle 1315 of 3020: machop vs sandshrew\nSimulating battle 1316 of 3020: machop vs machoke\nSimulating battle 1317 of 3020: machop vs dugtrio\nSimulating battle 1318 of 3020: machop vs vileplume\nSimulating battle 1319 of 3020: machop vs wigglytuff\nSimulating battle 1320 of 3020: machop vs paras\nSimulating battle 1321 of 3020: machoke vs tauros\nSimulating battle 1322 of 3020: machoke vs staryu\nSimulating battle 1323 of 3020: machoke vs moltres\nSimulating battle 1324 of 3020: machoke vs mewtwo\nSimulating battle 1325 of 3020: machoke vs horsea\nSimulating battle 1326 of 3020: machoke vs doduo\nSimulating battle 1327 of 3020: machoke vs magneton\nSimulating battle 1328 of 3020: machoke vs jigglypuff\nSimulating battle 1329 of 3020: machoke vs tangela\nSimulating battle 1330 of 3020: machoke vs pidgeot\nSimulating battle 1331 of 3020: machoke vs mr-mime\nSimulating battle 1332 of 3020: machoke vs kangaskhan\nSimulating battle 1333 of 3020: machoke vs slowpoke\nSimulating battle 1334 of 3020: machoke vs tentacool\nSimulating battle 1335 of 3020: machoke vs beedrill\nSimulating battle 1336 of 3020: machoke vs gastly\nSimulating battle 1337 of 3020: machoke vs lapras\nSimulating battle 1338 of 3020: machoke vs rattata\nSimulating battle 1339 of 3020: machoke vs magnemite\nSimulating battle 1340 of 3020: machoke vs seadra\nSimulating battle 1341 of 3020: machamp vs caterpie\nSimulating battle 1342 of 3020: machamp vs beedrill\nSimulating battle 1343 of 3020: machamp vs drowzee\nSimulating battle 1344 of 3020: machamp vs graveler\nSimulating battle 1345 of 3020: machamp vs raticate\nSimulating battle 1346 of 3020: machamp vs arbok\nSimulating battle 1347 of 3020: machamp vs lapras\nSimulating battle 1348 of 3020: machamp vs voltorb\nSimulating battle 1349 of 3020: machamp vs staryu\nSimulating battle 1350 of 3020: machamp vs mewtwo\nSimulating battle 1351 of 3020: machamp vs snorlax\nSimulating battle 1352 of 3020: machamp vs metapod\nSimulating battle 1353 of 3020: machamp vs seadra\nSimulating battle 1354 of 3020: machamp vs dragonair\nSimulating battle 1355 of 3020: machamp vs venomoth\nSimulating battle 1356 of 3020: machamp vs doduo\nSimulating battle 1357 of 3020: machamp vs scyther\nSimulating battle 1358 of 3020: machamp vs gyarados\nSimulating battle 1359 of 3020: machamp vs jigglypuff\nSimulating battle 1360 of 3020: machamp vs pidgey\nSimulating battle 1361 of 3020: bellsprout vs seadra\nSimulating battle 1362 of 3020: bellsprout vs sandshrew\nSimulating battle 1363 of 3020: bellsprout vs muk\nSimulating battle 1364 of 3020: bellsprout vs fearow\nSimulating battle 1365 of 3020: bellsprout vs lapras\nSimulating battle 1366 of 3020: bellsprout vs vileplume\nSimulating battle 1367 of 3020: bellsprout vs metapod\nSimulating battle 1368 of 3020: bellsprout vs kadabra\nSimulating battle 1369 of 3020: bellsprout vs tangela\nSimulating battle 1370 of 3020: bellsprout vs flareon\nSimulating battle 1371 of 3020: bellsprout vs jolteon\nSimulating battle 1372 of 3020: bellsprout vs zubat\nSimulating battle 1373 of 3020: bellsprout vs onix\nSimulating battle 1374 of 3020: bellsprout vs hypno\nSimulating battle 1375 of 3020: bellsprout vs geodude\nSimulating battle 1376 of 3020: bellsprout vs electrode\nSimulating battle 1377 of 3020: bellsprout vs hitmonlee\nSimulating battle 1378 of 3020: bellsprout vs grimer\nSimulating battle 1379 of 3020: bellsprout vs kakuna\nSimulating battle 1380 of 3020: bellsprout vs dewgong\nSimulating battle 1381 of 3020: weepinbell vs pidgeotto\nSimulating battle 1382 of 3020: weepinbell vs seel\nSimulating battle 1383 of 3020: weepinbell vs pikachu\nSimulating battle 1384 of 3020: weepinbell vs articuno\nSimulating battle 1385 of 3020: weepinbell vs voltorb\nSimulating battle 1386 of 3020: weepinbell vs geodude\nSimulating battle 1387 of 3020: weepinbell vs alakazam\nSimulating battle 1388 of 3020: weepinbell vs jigglypuff\nSimulating battle 1389 of 3020: weepinbell vs dewgong\nSimulating battle 1390 of 3020: weepinbell vs spearow\nSimulating battle 1391 of 3020: weepinbell vs mew\nSimulating battle 1392 of 3020: weepinbell vs vulpix\nSimulating battle 1393 of 3020: weepinbell vs cloyster\nSimulating battle 1394 of 3020: weepinbell vs magnemite\nSimulating battle 1395 of 3020: weepinbell vs exeggcute\nSimulating battle 1396 of 3020: weepinbell vs nidoking\nSimulating battle 1397 of 3020: weepinbell vs fearow\nSimulating battle 1398 of 3020: weepinbell vs zapdos\nSimulating battle 1399 of 3020: weepinbell vs krabby\nSimulating battle 1400 of 3020: weepinbell vs nidorino\nSimulating battle 1401 of 3020: victreebel vs flareon\nSimulating battle 1402 of 3020: victreebel vs arbok\nSimulating battle 1403 of 3020: victreebel vs weezing\nSimulating battle 1404 of 3020: victreebel vs ditto\nSimulating battle 1405 of 3020: victreebel vs gengar\nSimulating battle 1406 of 3020: victreebel vs charmeleon\nSimulating battle 1407 of 3020: victreebel vs magnemite\nSimulating battle 1408 of 3020: victreebel vs parasect\nSimulating battle 1409 of 3020: victreebel vs golduck\nSimulating battle 1410 of 3020: victreebel vs muk\nSimulating battle 1411 of 3020: victreebel vs magmar\nSimulating battle 1412 of 3020: victreebel vs diglett\nSimulating battle 1413 of 3020: victreebel vs growlithe\nSimulating battle 1414 of 3020: victreebel vs clefable\nSimulating battle 1415 of 3020: victreebel vs wigglytuff\nSimulating battle 1416 of 3020: victreebel vs raticate\nSimulating battle 1417 of 3020: victreebel vs ponyta\nSimulating battle 1418 of 3020: victreebel vs raichu\nSimulating battle 1419 of 3020: victreebel vs lapras\nSimulating battle 1420 of 3020: victreebel vs kabuto\nSimulating battle 1421 of 3020: tentacool vs flareon\nSimulating battle 1422 of 3020: tentacool vs caterpie\nSimulating battle 1423 of 3020: tentacool vs grimer\nSimulating battle 1424 of 3020: tentacool vs nidoking\nSimulating battle 1425 of 3020: tentacool vs krabby\nSimulating battle 1426 of 3020: tentacool vs wigglytuff\nSimulating battle 1427 of 3020: tentacool vs golbat\nSimulating battle 1428 of 3020: tentacool vs parasect\nSimulating battle 1429 of 3020: tentacool vs oddish\nSimulating battle 1430 of 3020: tentacool vs tangela\nSimulating battle 1431 of 3020: tentacool vs butterfree\nSimulating battle 1432 of 3020: tentacool vs hitmonchan\nSimulating battle 1433 of 3020: tentacool vs onix\nSimulating battle 1434 of 3020: tentacool vs poliwhirl\nSimulating battle 1435 of 3020: tentacool vs kangaskhan\nSimulating battle 1436 of 3020: tentacool vs geodude\nSimulating battle 1437 of 3020: tentacool vs horsea\nSimulating battle 1438 of 3020: tentacool vs poliwag\nSimulating battle 1439 of 3020: tentacool vs omanyte\nSimulating battle 1440 of 3020: tentacool vs poliwrath\nSimulating battle 1441 of 3020: tentacruel vs magnemite\nSimulating battle 1442 of 3020: tentacruel vs mr-mime\nSimulating battle 1443 of 3020: tentacruel vs diglett\nSimulating battle 1444 of 3020: tentacruel vs drowzee\nSimulating battle 1445 of 3020: tentacruel vs dragonair\nSimulating battle 1446 of 3020: tentacruel vs tangela\nSimulating battle 1447 of 3020: tentacruel vs staryu\nSimulating battle 1448 of 3020: tentacruel vs geodude\nSimulating battle 1449 of 3020: tentacruel vs kingler\nSimulating battle 1450 of 3020: tentacruel vs gyarados\nSimulating battle 1451 of 3020: tentacruel vs porygon\nSimulating battle 1452 of 3020: tentacruel vs koffing\nSimulating battle 1453 of 3020: tentacruel vs golbat\nSimulating battle 1454 of 3020: tentacruel vs meowth\nSimulating battle 1455 of 3020: tentacruel vs clefable\nSimulating battle 1456 of 3020: tentacruel vs alakazam\nSimulating battle 1457 of 3020: tentacruel vs kakuna\nSimulating battle 1458 of 3020: tentacruel vs electabuzz\nSimulating battle 1459 of 3020: tentacruel vs hypno\nSimulating battle 1460 of 3020: tentacruel vs snorlax\nSimulating battle 1461 of 3020: geodude vs sandshrew\nSimulating battle 1462 of 3020: geodude vs vaporeon\nSimulating battle 1463 of 3020: geodude vs nidoran-m\nSimulating battle 1464 of 3020: geodude vs tentacruel\nSimulating battle 1465 of 3020: geodude vs fearow\nSimulating battle 1466 of 3020: geodude vs golbat\nSimulating battle 1467 of 3020: geodude vs weepinbell\nSimulating battle 1468 of 3020: geodude vs seadra\nSimulating battle 1469 of 3020: geodude vs eevee\nSimulating battle 1470 of 3020: geodude vs ninetales\nSimulating battle 1471 of 3020: geodude vs chansey\nSimulating battle 1472 of 3020: geodude vs arbok\nSimulating battle 1473 of 3020: geodude vs primeape\nSimulating battle 1474 of 3020: geodude vs cloyster\nSimulating battle 1475 of 3020: geodude vs squirtle\nSimulating battle 1476 of 3020: geodude vs lickitung\nSimulating battle 1477 of 3020: geodude vs kakuna\nSimulating battle 1478 of 3020: geodude vs exeggutor\nSimulating battle 1479 of 3020: geodude vs gyarados\nSimulating battle 1480 of 3020: geodude vs hypno\nSimulating battle 1481 of 3020: graveler vs poliwhirl\nSimulating battle 1482 of 3020: graveler vs voltorb\nSimulating battle 1483 of 3020: graveler vs spearow\nSimulating battle 1484 of 3020: graveler vs hypno\nSimulating battle 1485 of 3020: graveler vs growlithe\nSimulating battle 1486 of 3020: graveler vs wartortle\nSimulating battle 1487 of 3020: graveler vs farfetchd\nSimulating battle 1488 of 3020: graveler vs raichu\nSimulating battle 1489 of 3020: graveler vs dewgong\nSimulating battle 1490 of 3020: graveler vs ninetales\nSimulating battle 1491 of 3020: graveler vs clefable\nSimulating battle 1492 of 3020: graveler vs caterpie\nSimulating battle 1493 of 3020: graveler vs geodude\nSimulating battle 1494 of 3020: graveler vs mr-mime\nSimulating battle 1495 of 3020: graveler vs horsea\nSimulating battle 1496 of 3020: graveler vs ivysaur\nSimulating battle 1497 of 3020: graveler vs charmeleon\nSimulating battle 1498 of 3020: graveler vs machop\nSimulating battle 1499 of 3020: graveler vs mankey\nSimulating battle 1500 of 3020: graveler vs jigglypuff\nSimulating battle 1501 of 3020: golem vs aerodactyl\nSimulating battle 1502 of 3020: golem vs porygon\nSimulating battle 1503 of 3020: golem vs weezing\nSimulating battle 1504 of 3020: golem vs nidoran-f\nSimulating battle 1505 of 3020: golem vs geodude\nSimulating battle 1506 of 3020: golem vs poliwhirl\nSimulating battle 1507 of 3020: golem vs slowpoke\nSimulating battle 1508 of 3020: golem vs nidoran-m\nSimulating battle 1509 of 3020: golem vs weedle\nSimulating battle 1510 of 3020: golem vs poliwrath\nSimulating battle 1511 of 3020: golem vs koffing\nSimulating battle 1512 of 3020: golem vs seaking\nSimulating battle 1513 of 3020: golem vs pidgeotto\nSimulating battle 1514 of 3020: golem vs magikarp\nSimulating battle 1515 of 3020: golem vs omastar\nSimulating battle 1516 of 3020: golem vs charmeleon\nSimulating battle 1517 of 3020: golem vs eevee\nSimulating battle 1518 of 3020: golem vs dragonite\nSimulating battle 1519 of 3020: golem vs vulpix\nSimulating battle 1520 of 3020: golem vs graveler\nSimulating battle 1521 of 3020: ponyta vs rhyhorn\nSimulating battle 1522 of 3020: ponyta vs bulbasaur\nSimulating battle 1523 of 3020: ponyta vs gastly\nSimulating battle 1524 of 3020: ponyta vs poliwrath\nSimulating battle 1525 of 3020: ponyta vs dragonair\nSimulating battle 1526 of 3020: ponyta vs lickitung\nSimulating battle 1527 of 3020: ponyta vs venonat\nSimulating battle 1528 of 3020: ponyta vs fearow\nSimulating battle 1529 of 3020: ponyta vs flareon\nSimulating battle 1530 of 3020: ponyta vs gengar\nSimulating battle 1531 of 3020: ponyta vs pidgeot\nSimulating battle 1532 of 3020: ponyta vs kabutops\nSimulating battle 1533 of 3020: ponyta vs lapras\nSimulating battle 1534 of 3020: ponyta vs snorlax\nSimulating battle 1535 of 3020: ponyta vs charizard\nSimulating battle 1536 of 3020: ponyta vs electrode\nSimulating battle 1537 of 3020: ponyta vs mr-mime\nSimulating battle 1538 of 3020: ponyta vs butterfree\nSimulating battle 1539 of 3020: ponyta vs hypno\nSimulating battle 1540 of 3020: ponyta vs alakazam\nSimulating battle 1541 of 3020: rapidash vs charmeleon\nSimulating battle 1542 of 3020: rapidash vs haunter\nSimulating battle 1543 of 3020: rapidash vs pidgeot\nSimulating battle 1544 of 3020: rapidash vs shellder\nSimulating battle 1545 of 3020: rapidash vs poliwrath\nSimulating battle 1546 of 3020: rapidash vs sandshrew\nSimulating battle 1547 of 3020: rapidash vs mewtwo\nSimulating battle 1548 of 3020: rapidash vs dewgong\nSimulating battle 1549 of 3020: rapidash vs clefairy\nSimulating battle 1550 of 3020: rapidash vs butterfree\nSimulating battle 1551 of 3020: rapidash vs gastly\nSimulating battle 1552 of 3020: rapidash vs kabutops\nSimulating battle 1553 of 3020: rapidash vs grimer\nSimulating battle 1554 of 3020: rapidash vs vileplume\nSimulating battle 1555 of 3020: rapidash vs staryu\nSimulating battle 1556 of 3020: rapidash vs jynx\nSimulating battle 1557 of 3020: rapidash vs parasect\nSimulating battle 1558 of 3020: rapidash vs pidgeotto\nSimulating battle 1559 of 3020: rapidash vs seaking\nSimulating battle 1560 of 3020: rapidash vs caterpie\nSimulating battle 1561 of 3020: slowpoke vs golem\nSimulating battle 1562 of 3020: slowpoke vs meowth\nSimulating battle 1563 of 3020: slowpoke vs butterfree\nSimulating battle 1564 of 3020: slowpoke vs metapod\nSimulating battle 1565 of 3020: slowpoke vs magneton\nSimulating battle 1566 of 3020: slowpoke vs magnemite\nSimulating battle 1567 of 3020: slowpoke vs eevee\nSimulating battle 1568 of 3020: slowpoke vs exeggutor\nSimulating battle 1569 of 3020: slowpoke vs kabutops\nSimulating battle 1570 of 3020: slowpoke vs scyther\nSimulating battle 1571 of 3020: slowpoke vs alakazam\nSimulating battle 1572 of 3020: slowpoke vs caterpie\nSimulating battle 1573 of 3020: slowpoke vs venomoth\nSimulating battle 1574 of 3020: slowpoke vs geodude\nSimulating battle 1575 of 3020: slowpoke vs haunter\nSimulating battle 1576 of 3020: slowpoke vs weedle\nSimulating battle 1577 of 3020: slowpoke vs seel\nSimulating battle 1578 of 3020: slowpoke vs weepinbell\nSimulating battle 1579 of 3020: slowpoke vs nidoran-m\nSimulating battle 1580 of 3020: slowpoke vs drowzee\nSimulating battle 1581 of 3020: slowbro vs chansey\nSimulating battle 1582 of 3020: slowbro vs cubone\nSimulating battle 1583 of 3020: slowbro vs tangela\nSimulating battle 1584 of 3020: slowbro vs voltorb\nSimulating battle 1585 of 3020: slowbro vs grimer\nSimulating battle 1586 of 3020: slowbro vs venonat\nSimulating battle 1587 of 3020: slowbro vs magikarp\nSimulating battle 1588 of 3020: slowbro vs drowzee\nSimulating battle 1589 of 3020: slowbro vs vaporeon\nSimulating battle 1590 of 3020: slowbro vs bellsprout\nSimulating battle 1591 of 3020: slowbro vs fearow\nSimulating battle 1592 of 3020: slowbro vs weezing\nSimulating battle 1593 of 3020: slowbro vs spearow\nSimulating battle 1594 of 3020: slowbro vs rhydon\nSimulating battle 1595 of 3020: slowbro vs parasect\nSimulating battle 1596 of 3020: slowbro vs kabutops\nSimulating battle 1597 of 3020: slowbro vs golem\nSimulating battle 1598 of 3020: slowbro vs doduo\nSimulating battle 1599 of 3020: slowbro vs sandshrew\nSimulating battle 1600 of 3020: slowbro vs dodrio\nSimulating battle 1601 of 3020: magnemite vs golem\nSimulating battle 1602 of 3020: magnemite vs slowpoke\nSimulating battle 1603 of 3020: magnemite vs horsea\nSimulating battle 1604 of 3020: magnemite vs rhyhorn\nSimulating battle 1605 of 3020: magnemite vs oddish\nSimulating battle 1606 of 3020: magnemite vs kangaskhan\nSimulating battle 1607 of 3020: magnemite vs cloyster\nSimulating battle 1608 of 3020: magnemite vs metapod\nSimulating battle 1609 of 3020: magnemite vs gastly\nSimulating battle 1610 of 3020: magnemite vs chansey\nSimulating battle 1611 of 3020: magnemite vs victreebel\nSimulating battle 1612 of 3020: magnemite vs beedrill\nSimulating battle 1613 of 3020: magnemite vs raticate\nSimulating battle 1614 of 3020: magnemite vs marowak\nSimulating battle 1615 of 3020: magnemite vs onix\nSimulating battle 1616 of 3020: magnemite vs eevee\nSimulating battle 1617 of 3020: magnemite vs zubat\nSimulating battle 1618 of 3020: magnemite vs wartortle\nSimulating battle 1619 of 3020: magnemite vs vulpix\nSimulating battle 1620 of 3020: magnemite vs tangela\nSimulating battle 1621 of 3020: magneton vs blastoise\nSimulating battle 1622 of 3020: magneton vs nidorino\nSimulating battle 1623 of 3020: magneton vs pidgeot\nSimulating battle 1624 of 3020: magneton vs poliwhirl\nSimulating battle 1625 of 3020: magneton vs onix\nSimulating battle 1626 of 3020: magneton vs gengar\nSimulating battle 1627 of 3020: magneton vs voltorb\nSimulating battle 1628 of 3020: magneton vs dratini\nSimulating battle 1629 of 3020: magneton vs wigglytuff\nSimulating battle 1630 of 3020: magneton vs seadra\nSimulating battle 1631 of 3020: magneton vs drowzee\nSimulating battle 1632 of 3020: magneton vs hypno\nSimulating battle 1633 of 3020: magneton vs kangaskhan\nSimulating battle 1634 of 3020: magneton vs raticate\nSimulating battle 1635 of 3020: magneton vs dragonair\nSimulating battle 1636 of 3020: magneton vs clefable\nSimulating battle 1637 of 3020: magneton vs porygon\nSimulating battle 1638 of 3020: magneton vs exeggutor\nSimulating battle 1639 of 3020: magneton vs magnemite\nSimulating battle 1640 of 3020: magneton vs tentacool\nSimulating battle 1641 of 3020: farfetchd vs kadabra\nSimulating battle 1642 of 3020: farfetchd vs nidorina\nSimulating battle 1643 of 3020: farfetchd vs squirtle\nSimulating battle 1644 of 3020: farfetchd vs venonat\nSimulating battle 1645 of 3020: farfetchd vs magikarp\nSimulating battle 1646 of 3020: farfetchd vs vaporeon\nSimulating battle 1647 of 3020: farfetchd vs electrode\nSimulating battle 1648 of 3020: farfetchd vs zapdos\nSimulating battle 1649 of 3020: farfetchd vs nidoqueen\nSimulating battle 1650 of 3020: farfetchd vs machamp\nSimulating battle 1651 of 3020: farfetchd vs machoke\nSimulating battle 1652 of 3020: farfetchd vs horsea\nSimulating battle 1653 of 3020: farfetchd vs golduck\nSimulating battle 1654 of 3020: farfetchd vs geodude\nSimulating battle 1655 of 3020: farfetchd vs pinsir\nSimulating battle 1656 of 3020: farfetchd vs meowth\nSimulating battle 1657 of 3020: farfetchd vs nidoran-m\nSimulating battle 1658 of 3020: farfetchd vs clefairy\nSimulating battle 1659 of 3020: farfetchd vs rattata\nSimulating battle 1660 of 3020: farfetchd vs seadra\nSimulating battle 1661 of 3020: doduo vs vileplume\nSimulating battle 1662 of 3020: doduo vs kangaskhan\nSimulating battle 1663 of 3020: doduo vs ekans\nSimulating battle 1664 of 3020: doduo vs magneton\nSimulating battle 1665 of 3020: doduo vs shellder\nSimulating battle 1666 of 3020: doduo vs pidgeotto\nSimulating battle 1667 of 3020: doduo vs aerodactyl\nSimulating battle 1668 of 3020: doduo vs kabuto\nSimulating battle 1669 of 3020: doduo vs graveler\nSimulating battle 1670 of 3020: doduo vs ponyta\nSimulating battle 1671 of 3020: doduo vs zubat\nSimulating battle 1672 of 3020: doduo vs gengar\nSimulating battle 1673 of 3020: doduo vs ditto\nSimulating battle 1674 of 3020: doduo vs growlithe\nSimulating battle 1675 of 3020: doduo vs nidoran-m\nSimulating battle 1676 of 3020: doduo vs meowth\nSimulating battle 1677 of 3020: doduo vs clefable\nSimulating battle 1678 of 3020: doduo vs poliwhirl\nSimulating battle 1679 of 3020: doduo vs tauros\nSimulating battle 1680 of 3020: doduo vs squirtle\nSimulating battle 1681 of 3020: dodrio vs gengar\nSimulating battle 1682 of 3020: dodrio vs snorlax\nSimulating battle 1683 of 3020: dodrio vs dragonair\nSimulating battle 1684 of 3020: dodrio vs drowzee\nSimulating battle 1685 of 3020: dodrio vs starmie\nSimulating battle 1686 of 3020: dodrio vs nidoking\nSimulating battle 1687 of 3020: dodrio vs ekans\nSimulating battle 1688 of 3020: dodrio vs pidgeotto\nSimulating battle 1689 of 3020: dodrio vs slowbro\nSimulating battle 1690 of 3020: dodrio vs exeggutor\nSimulating battle 1691 of 3020: dodrio vs jynx\nSimulating battle 1692 of 3020: dodrio vs flareon\nSimulating battle 1693 of 3020: dodrio vs hitmonchan\nSimulating battle 1694 of 3020: dodrio vs hitmonlee\nSimulating battle 1695 of 3020: dodrio vs dragonite\nSimulating battle 1696 of 3020: dodrio vs rattata\nSimulating battle 1697 of 3020: dodrio vs nidorino\nSimulating battle 1698 of 3020: dodrio vs magneton\nSimulating battle 1699 of 3020: dodrio vs seadra\nSimulating battle 1700 of 3020: dodrio vs vaporeon\nSimulating battle 1701 of 3020: seel vs shellder\nSimulating battle 1702 of 3020: seel vs nidorino\nSimulating battle 1703 of 3020: seel vs snorlax\nSimulating battle 1704 of 3020: seel vs parasect\nSimulating battle 1705 of 3020: seel vs nidoking\nSimulating battle 1706 of 3020: seel vs rhydon\nSimulating battle 1707 of 3020: seel vs gyarados\nSimulating battle 1708 of 3020: seel vs beedrill\nSimulating battle 1709 of 3020: seel vs nidoran-m\nSimulating battle 1710 of 3020: seel vs vaporeon\nSimulating battle 1711 of 3020: seel vs wigglytuff\nSimulating battle 1712 of 3020: seel vs rapidash\nSimulating battle 1713 of 3020: seel vs oddish\nSimulating battle 1714 of 3020: seel vs golbat\nSimulating battle 1715 of 3020: seel vs farfetchd\nSimulating battle 1716 of 3020: seel vs growlithe\nSimulating battle 1717 of 3020: seel vs tentacruel\nSimulating battle 1718 of 3020: seel vs spearow\nSimulating battle 1719 of 3020: seel vs alakazam\nSimulating battle 1720 of 3020: seel vs dugtrio\nSimulating battle 1721 of 3020: dewgong vs snorlax\nSimulating battle 1722 of 3020: dewgong vs victreebel\nSimulating battle 1723 of 3020: dewgong vs nidorino\nSimulating battle 1724 of 3020: dewgong vs rapidash\nSimulating battle 1725 of 3020: dewgong vs omanyte\nSimulating battle 1726 of 3020: dewgong vs arbok\nSimulating battle 1727 of 3020: dewgong vs gyarados\nSimulating battle 1728 of 3020: dewgong vs gloom\nSimulating battle 1729 of 3020: dewgong vs mewtwo\nSimulating battle 1730 of 3020: dewgong vs wigglytuff\nSimulating battle 1731 of 3020: dewgong vs grimer\nSimulating battle 1732 of 3020: dewgong vs moltres\nSimulating battle 1733 of 3020: dewgong vs metapod\nSimulating battle 1734 of 3020: dewgong vs wartortle\nSimulating battle 1735 of 3020: dewgong vs spearow\nSimulating battle 1736 of 3020: dewgong vs butterfree\nSimulating battle 1737 of 3020: dewgong vs dragonite\nSimulating battle 1738 of 3020: dewgong vs machamp\nSimulating battle 1739 of 3020: dewgong vs psyduck\nSimulating battle 1740 of 3020: dewgong vs dragonair\nSimulating battle 1741 of 3020: grimer vs lickitung\nSimulating battle 1742 of 3020: grimer vs wartortle\nSimulating battle 1743 of 3020: grimer vs magikarp\nSimulating battle 1744 of 3020: grimer vs kabutops\nSimulating battle 1745 of 3020: grimer vs graveler\nSimulating battle 1746 of 3020: grimer vs rapidash\nSimulating battle 1747 of 3020: grimer vs electabuzz\nSimulating battle 1748 of 3020: grimer vs abra\nSimulating battle 1749 of 3020: grimer vs marowak\nSimulating battle 1750 of 3020: grimer vs ponyta\nSimulating battle 1751 of 3020: grimer vs goldeen\nSimulating battle 1752 of 3020: grimer vs rattata\nSimulating battle 1753 of 3020: grimer vs pidgey\nSimulating battle 1754 of 3020: grimer vs zubat\nSimulating battle 1755 of 3020: grimer vs tangela\nSimulating battle 1756 of 3020: grimer vs staryu\nSimulating battle 1757 of 3020: grimer vs persian\nSimulating battle 1758 of 3020: grimer vs muk\nSimulating battle 1759 of 3020: grimer vs vulpix\nSimulating battle 1760 of 3020: grimer vs magnemite\nSimulating battle 1761 of 3020: muk vs magneton\nSimulating battle 1762 of 3020: muk vs shellder\nSimulating battle 1763 of 3020: muk vs cubone\nSimulating battle 1764 of 3020: muk vs nidoking\nSimulating battle 1765 of 3020: muk vs drowzee\nSimulating battle 1766 of 3020: muk vs eevee\nSimulating battle 1767 of 3020: muk vs zapdos\nSimulating battle 1768 of 3020: muk vs sandslash\nSimulating battle 1769 of 3020: muk vs poliwrath\nSimulating battle 1770 of 3020: muk vs starmie\nSimulating battle 1771 of 3020: muk vs nidoran-m\nSimulating battle 1772 of 3020: muk vs bellsprout\nSimulating battle 1773 of 3020: muk vs seadra\nSimulating battle 1774 of 3020: muk vs kadabra\nSimulating battle 1775 of 3020: muk vs vulpix\nSimulating battle 1776 of 3020: muk vs pikachu\nSimulating battle 1777 of 3020: muk vs weedle\nSimulating battle 1778 of 3020: muk vs graveler\nSimulating battle 1779 of 3020: muk vs voltorb\nSimulating battle 1780 of 3020: muk vs koffing\nSimulating battle 1781 of 3020: shellder vs kadabra\nSimulating battle 1782 of 3020: shellder vs zubat\nSimulating battle 1783 of 3020: shellder vs doduo\nSimulating battle 1784 of 3020: shellder vs dragonite\nSimulating battle 1785 of 3020: shellder vs magnemite\nSimulating battle 1786 of 3020: shellder vs venomoth\nSimulating battle 1787 of 3020: shellder vs magikarp\nSimulating battle 1788 of 3020: shellder vs eevee\nSimulating battle 1789 of 3020: shellder vs starmie\nSimulating battle 1790 of 3020: shellder vs slowpoke\nSimulating battle 1791 of 3020: shellder vs charizard\nSimulating battle 1792 of 3020: shellder vs arbok\nSimulating battle 1793 of 3020: shellder vs exeggcute\nSimulating battle 1794 of 3020: shellder vs lapras\nSimulating battle 1795 of 3020: shellder vs seaking\nSimulating battle 1796 of 3020: shellder vs poliwrath\nSimulating battle 1797 of 3020: shellder vs mankey\nSimulating battle 1798 of 3020: shellder vs mew\nSimulating battle 1799 of 3020: shellder vs gastly\nSimulating battle 1800 of 3020: shellder vs weedle\nSimulating battle 1801 of 3020: cloyster vs weedle\nSimulating battle 1802 of 3020: cloyster vs tentacruel\nSimulating battle 1803 of 3020: cloyster vs tauros\nSimulating battle 1804 of 3020: cloyster vs mr-mime\nSimulating battle 1805 of 3020: cloyster vs geodude\nSimulating battle 1806 of 3020: cloyster vs omastar\nSimulating battle 1807 of 3020: cloyster vs venusaur\nSimulating battle 1808 of 3020: cloyster vs sandslash\nSimulating battle 1809 of 3020: cloyster vs rhydon\nSimulating battle 1810 of 3020: cloyster vs clefairy\nSimulating battle 1811 of 3020: cloyster vs machamp\nSimulating battle 1812 of 3020: cloyster vs onix\nSimulating battle 1813 of 3020: cloyster vs marowak\nSimulating battle 1814 of 3020: cloyster vs butterfree\nSimulating battle 1815 of 3020: cloyster vs cubone\nSimulating battle 1816 of 3020: cloyster vs kakuna\nSimulating battle 1817 of 3020: cloyster vs dratini\nSimulating battle 1818 of 3020: cloyster vs zapdos\nSimulating battle 1819 of 3020: cloyster vs diglett\nSimulating battle 1820 of 3020: cloyster vs gengar\nSimulating battle 1821 of 3020: gastly vs snorlax\nSimulating battle 1822 of 3020: gastly vs geodude\nSimulating battle 1823 of 3020: gastly vs rattata\nSimulating battle 1824 of 3020: gastly vs voltorb\nSimulating battle 1825 of 3020: gastly vs lapras\nSimulating battle 1826 of 3020: gastly vs seadra\nSimulating battle 1827 of 3020: gastly vs aerodactyl\nSimulating battle 1828 of 3020: gastly vs tentacool\nSimulating battle 1829 of 3020: gastly vs nidoqueen\nSimulating battle 1830 of 3020: gastly vs nidorino\nSimulating battle 1831 of 3020: gastly vs pikachu\nSimulating battle 1832 of 3020: gastly vs exeggcute\nSimulating battle 1833 of 3020: gastly vs hypno\nSimulating battle 1834 of 3020: gastly vs dewgong\nSimulating battle 1835 of 3020: gastly vs articuno\nSimulating battle 1836 of 3020: gastly vs onix\nSimulating battle 1837 of 3020: gastly vs vulpix\nSimulating battle 1838 of 3020: gastly vs dugtrio\nSimulating battle 1839 of 3020: gastly vs ditto\nSimulating battle 1840 of 3020: gastly vs cubone\nSimulating battle 1841 of 3020: haunter vs gyarados\nSimulating battle 1842 of 3020: haunter vs metapod\nSimulating battle 1843 of 3020: haunter vs butterfree\nSimulating battle 1844 of 3020: haunter vs caterpie\nSimulating battle 1845 of 3020: haunter vs clefable\nSimulating battle 1846 of 3020: haunter vs seel\nSimulating battle 1847 of 3020: haunter vs scyther\nSimulating battle 1848 of 3020: haunter vs vaporeon\nSimulating battle 1849 of 3020: haunter vs goldeen\nSimulating battle 1850 of 3020: haunter vs jigglypuff\nSimulating battle 1851 of 3020: haunter vs eevee\nSimulating battle 1852 of 3020: haunter vs doduo\nSimulating battle 1853 of 3020: haunter vs magneton\nSimulating battle 1854 of 3020: haunter vs golbat\nSimulating battle 1855 of 3020: haunter vs exeggcute\nSimulating battle 1856 of 3020: haunter vs ponyta\nSimulating battle 1857 of 3020: haunter vs dewgong\nSimulating battle 1858 of 3020: haunter vs lapras\nSimulating battle 1859 of 3020: haunter vs ditto\nSimulating battle 1860 of 3020: haunter vs omanyte\nSimulating battle 1861 of 3020: gengar vs pinsir\nSimulating battle 1862 of 3020: gengar vs moltres\nSimulating battle 1863 of 3020: gengar vs ponyta\nSimulating battle 1864 of 3020: gengar vs scyther\nSimulating battle 1865 of 3020: gengar vs charmeleon\nSimulating battle 1866 of 3020: gengar vs drowzee\nSimulating battle 1867 of 3020: gengar vs dodrio\nSimulating battle 1868 of 3020: gengar vs nidoran-f\nSimulating battle 1869 of 3020: gengar vs lickitung\nSimulating battle 1870 of 3020: gengar vs mew\nSimulating battle 1871 of 3020: gengar vs slowpoke\nSimulating battle 1872 of 3020: gengar vs squirtle\nSimulating battle 1873 of 3020: gengar vs machamp\nSimulating battle 1874 of 3020: gengar vs mewtwo\nSimulating battle 1875 of 3020: gengar vs dragonite\nSimulating battle 1876 of 3020: gengar vs arcanine\nSimulating battle 1877 of 3020: gengar vs kakuna\nSimulating battle 1878 of 3020: gengar vs jynx\nSimulating battle 1879 of 3020: gengar vs gloom\nSimulating battle 1880 of 3020: gengar vs flareon\nSimulating battle 1881 of 3020: onix vs kingler\nSimulating battle 1882 of 3020: onix vs ninetales\nSimulating battle 1883 of 3020: onix vs abra\nSimulating battle 1884 of 3020: onix vs blastoise\nSimulating battle 1885 of 3020: onix vs dragonair\nSimulating battle 1886 of 3020: onix vs nidoran-f\nSimulating battle 1887 of 3020: onix vs venomoth\nSimulating battle 1888 of 3020: onix vs charmeleon\nSimulating battle 1889 of 3020: onix vs tangela\nSimulating battle 1890 of 3020: onix vs magnemite\nSimulating battle 1891 of 3020: onix vs koffing\nSimulating battle 1892 of 3020: onix vs jigglypuff\nSimulating battle 1893 of 3020: onix vs hitmonchan\nSimulating battle 1894 of 3020: onix vs persian\nSimulating battle 1895 of 3020: onix vs gyarados\nSimulating battle 1896 of 3020: onix vs mr-mime\nSimulating battle 1897 of 3020: onix vs pidgey\nSimulating battle 1898 of 3020: onix vs clefable\nSimulating battle 1899 of 3020: onix vs vaporeon\nSimulating battle 1900 of 3020: onix vs psyduck\nSimulating battle 1901 of 3020: drowzee vs zapdos\nSimulating battle 1902 of 3020: drowzee vs doduo\nSimulating battle 1903 of 3020: drowzee vs jynx\nSimulating battle 1904 of 3020: drowzee vs flareon\nSimulating battle 1905 of 3020: drowzee vs krabby\nSimulating battle 1906 of 3020: drowzee vs magnemite\nSimulating battle 1907 of 3020: drowzee vs vileplume\nSimulating battle 1908 of 3020: drowzee vs seaking\nSimulating battle 1909 of 3020: drowzee vs omanyte\nSimulating battle 1910 of 3020: drowzee vs grimer\nSimulating battle 1911 of 3020: drowzee vs kabutops\nSimulating battle 1912 of 3020: drowzee vs cloyster\nSimulating battle 1913 of 3020: drowzee vs machamp\nSimulating battle 1914 of 3020: drowzee vs electabuzz\nSimulating battle 1915 of 3020: drowzee vs diglett\nSimulating battle 1916 of 3020: drowzee vs kadabra\nSimulating battle 1917 of 3020: drowzee vs tentacool\nSimulating battle 1918 of 3020: drowzee vs articuno\nSimulating battle 1919 of 3020: drowzee vs ponyta\nSimulating battle 1920 of 3020: drowzee vs growlithe\nSimulating battle 1921 of 3020: hypno vs ponyta\nSimulating battle 1922 of 3020: hypno vs geodude\nSimulating battle 1923 of 3020: hypno vs psyduck\nSimulating battle 1924 of 3020: hypno vs pinsir\nSimulating battle 1925 of 3020: hypno vs magneton\nSimulating battle 1926 of 3020: hypno vs jynx\nSimulating battle 1927 of 3020: hypno vs shellder\nSimulating battle 1928 of 3020: hypno vs zapdos\nSimulating battle 1929 of 3020: hypno vs victreebel\nSimulating battle 1930 of 3020: hypno vs nidoran-m\nSimulating battle 1931 of 3020: hypno vs dragonair\nSimulating battle 1932 of 3020: hypno vs kabutops\nSimulating battle 1933 of 3020: hypno vs kingler\nSimulating battle 1934 of 3020: hypno vs exeggutor\nSimulating battle 1935 of 3020: hypno vs muk\nSimulating battle 1936 of 3020: hypno vs ninetales\nSimulating battle 1937 of 3020: hypno vs graveler\nSimulating battle 1938 of 3020: hypno vs metapod\nSimulating battle 1939 of 3020: hypno vs spearow\nSimulating battle 1940 of 3020: hypno vs kangaskhan\nSimulating battle 1941 of 3020: krabby vs machop\nSimulating battle 1942 of 3020: krabby vs jynx\nSimulating battle 1943 of 3020: krabby vs golduck\nSimulating battle 1944 of 3020: krabby vs meowth\nSimulating battle 1945 of 3020: krabby vs omastar\nSimulating battle 1946 of 3020: krabby vs weepinbell\nSimulating battle 1947 of 3020: krabby vs zapdos\nSimulating battle 1948 of 3020: krabby vs clefable\nSimulating battle 1949 of 3020: krabby vs sandslash\nSimulating battle 1950 of 3020: krabby vs poliwrath\nSimulating battle 1951 of 3020: krabby vs abra\nSimulating battle 1952 of 3020: krabby vs weedle\nSimulating battle 1953 of 3020: krabby vs porygon\nSimulating battle 1954 of 3020: krabby vs growlithe\nSimulating battle 1955 of 3020: krabby vs poliwag\nSimulating battle 1956 of 3020: krabby vs kakuna\nSimulating battle 1957 of 3020: krabby vs raichu\nSimulating battle 1958 of 3020: krabby vs hitmonchan\nSimulating battle 1959 of 3020: krabby vs dodrio\nSimulating battle 1960 of 3020: krabby vs mr-mime\nSimulating battle 1961 of 3020: kingler vs raichu\nSimulating battle 1962 of 3020: kingler vs clefable\nSimulating battle 1963 of 3020: kingler vs ivysaur\nSimulating battle 1964 of 3020: kingler vs aerodactyl\nSimulating battle 1965 of 3020: kingler vs zubat\nSimulating battle 1966 of 3020: kingler vs hitmonlee\nSimulating battle 1967 of 3020: kingler vs scyther\nSimulating battle 1968 of 3020: kingler vs jynx\nSimulating battle 1969 of 3020: kingler vs meowth\nSimulating battle 1970 of 3020: kingler vs geodude\nSimulating battle 1971 of 3020: kingler vs farfetchd\nSimulating battle 1972 of 3020: kingler vs pidgey\nSimulating battle 1973 of 3020: kingler vs ekans\nSimulating battle 1974 of 3020: kingler vs dragonair\nSimulating battle 1975 of 3020: kingler vs poliwag\nSimulating battle 1976 of 3020: kingler vs omanyte\nSimulating battle 1977 of 3020: kingler vs caterpie\nSimulating battle 1978 of 3020: kingler vs vileplume\nSimulating battle 1979 of 3020: kingler vs lickitung\nSimulating battle 1980 of 3020: kingler vs paras\nSimulating battle 1981 of 3020: voltorb vs caterpie\nSimulating battle 1982 of 3020: voltorb vs exeggutor\nSimulating battle 1983 of 3020: voltorb vs tauros\nSimulating battle 1984 of 3020: voltorb vs venonat\nSimulating battle 1985 of 3020: voltorb vs graveler\nSimulating battle 1986 of 3020: voltorb vs venusaur\nSimulating battle 1987 of 3020: voltorb vs ponyta\nSimulating battle 1988 of 3020: voltorb vs dratini\nSimulating battle 1989 of 3020: voltorb vs sandslash\nSimulating battle 1990 of 3020: voltorb vs seel\nSimulating battle 1991 of 3020: voltorb vs tentacruel\nSimulating battle 1992 of 3020: voltorb vs goldeen\nSimulating battle 1993 of 3020: voltorb vs kabutops\nSimulating battle 1994 of 3020: voltorb vs flareon\nSimulating battle 1995 of 3020: voltorb vs clefairy\nSimulating battle 1996 of 3020: voltorb vs lapras\nSimulating battle 1997 of 3020: voltorb vs starmie\nSimulating battle 1998 of 3020: voltorb vs weepinbell\nSimulating battle 1999 of 3020: voltorb vs diglett\nSimulating battle 2000 of 3020: voltorb vs nidoran-f\nSimulating battle 2001 of 3020: electrode vs dodrio\nAPI error occurred: Connection error.. Retrying in 30 seconds...\nSimulating battle 2002 of 3020: electrode vs golbat\nSimulating battle 2003 of 3020: electrode vs seaking\nSimulating battle 2004 of 3020: electrode vs machop\nSimulating battle 2005 of 3020: electrode vs venonat\nSimulating battle 2006 of 3020: electrode vs charmander\nSimulating battle 2007 of 3020: electrode vs dewgong\nSimulating battle 2008 of 3020: electrode vs golem\nSimulating battle 2009 of 3020: electrode vs dratini\nSimulating battle 2010 of 3020: electrode vs diglett\nSimulating battle 2011 of 3020: electrode vs vileplume\nSimulating battle 2012 of 3020: electrode vs marowak\nSimulating battle 2013 of 3020: electrode vs rhyhorn\nSimulating battle 2014 of 3020: electrode vs eevee\nSimulating battle 2015 of 3020: electrode vs doduo\nSimulating battle 2016 of 3020: electrode vs ekans\nSimulating battle 2017 of 3020: electrode vs cubone\nSimulating battle 2018 of 3020: electrode vs pikachu\nSimulating battle 2019 of 3020: electrode vs clefable\nSimulating battle 2020 of 3020: electrode vs jynx\nSimulating battle 2021 of 3020: exeggcute vs farfetchd\nSimulating battle 2022 of 3020: exeggcute vs kadabra\nSimulating battle 2023 of 3020: exeggcute vs ivysaur\nSimulating battle 2024 of 3020: exeggcute vs machoke\nSimulating battle 2025 of 3020: exeggcute vs kingler\nSimulating battle 2026 of 3020: exeggcute vs poliwhirl\nSimulating battle 2027 of 3020: exeggcute vs horsea\nSimulating battle 2028 of 3020: exeggcute vs bellsprout\nSimulating battle 2029 of 3020: exeggcute vs dodrio\nSimulating battle 2030 of 3020: exeggcute vs rapidash\nSimulating battle 2031 of 3020: exeggcute vs mew\nSimulating battle 2032 of 3020: exeggcute vs dragonair\nSimulating battle 2033 of 3020: exeggcute vs venusaur\nSimulating battle 2034 of 3020: exeggcute vs haunter\nSimulating battle 2035 of 3020: exeggcute vs pidgey\nSimulating battle 2036 of 3020: exeggcute vs nidoqueen\nSimulating battle 2037 of 3020: exeggcute vs starmie\nSimulating battle 2038 of 3020: exeggcute vs slowpoke\nSimulating battle 2039 of 3020: exeggcute vs zubat\nSimulating battle 2040 of 3020: exeggcute vs marowak\nSimulating battle 2041 of 3020: exeggutor vs gyarados\nSimulating battle 2042 of 3020: exeggutor vs slowbro\nSimulating battle 2043 of 3020: exeggutor vs nidoqueen\nSimulating battle 2044 of 3020: exeggutor vs golem\nSimulating battle 2045 of 3020: exeggutor vs onix\nSimulating battle 2046 of 3020: exeggutor vs primeape\nSimulating battle 2047 of 3020: exeggutor vs clefairy\nSimulating battle 2048 of 3020: exeggutor vs jynx\nSimulating battle 2049 of 3020: exeggutor vs wigglytuff\nSimulating battle 2050 of 3020: exeggutor vs goldeen\nSimulating battle 2051 of 3020: exeggutor vs drowzee\nSimulating battle 2052 of 3020: exeggutor vs lickitung\nSimulating battle 2053 of 3020: exeggutor vs aerodactyl\nSimulating battle 2054 of 3020: exeggutor vs mr-mime\nSimulating battle 2055 of 3020: exeggutor vs omastar\nSimulating battle 2056 of 3020: exeggutor vs mankey\nSimulating battle 2057 of 3020: exeggutor vs kadabra\nSimulating battle 2058 of 3020: exeggutor vs spearow\nSimulating battle 2059 of 3020: exeggutor vs flareon\nSimulating battle 2060 of 3020: exeggutor vs horsea\nSimulating battle 2061 of 3020: cubone vs porygon\nSimulating battle 2062 of 3020: cubone vs haunter\nSimulating battle 2063 of 3020: cubone vs raticate\nSimulating battle 2064 of 3020: cubone vs moltres\nSimulating battle 2065 of 3020: cubone vs nidoran-f\nSimulating battle 2066 of 3020: cubone vs pidgey\nSimulating battle 2067 of 3020: cubone vs aerodactyl\nSimulating battle 2068 of 3020: cubone vs lapras\nSimulating battle 2069 of 3020: cubone vs meowth\nSimulating battle 2070 of 3020: cubone vs dragonair\nSimulating battle 2071 of 3020: cubone vs omastar\nSimulating battle 2072 of 3020: cubone vs jigglypuff\nSimulating battle 2073 of 3020: cubone vs oddish\nSimulating battle 2074 of 3020: cubone vs doduo\nSimulating battle 2075 of 3020: cubone vs jolteon\nSimulating battle 2076 of 3020: cubone vs kangaskhan\nSimulating battle 2077 of 3020: cubone vs nidorina\nSimulating battle 2078 of 3020: cubone vs persian\nSimulating battle 2079 of 3020: cubone vs mew\nSimulating battle 2080 of 3020: cubone vs pinsir\nSimulating battle 2081 of 3020: marowak vs arbok\nSimulating battle 2082 of 3020: marowak vs ditto\nSimulating battle 2083 of 3020: marowak vs horsea\nSimulating battle 2084 of 3020: marowak vs beedrill\nSimulating battle 2085 of 3020: marowak vs goldeen\nSimulating battle 2086 of 3020: marowak vs nidoking\nSimulating battle 2087 of 3020: marowak vs eevee\nSimulating battle 2088 of 3020: marowak vs lickitung\nSimulating battle 2089 of 3020: marowak vs moltres\nSimulating battle 2090 of 3020: marowak vs zapdos\nSimulating battle 2091 of 3020: marowak vs staryu\nSimulating battle 2092 of 3020: marowak vs slowpoke\nSimulating battle 2093 of 3020: marowak vs charizard\nSimulating battle 2094 of 3020: marowak vs exeggcute\nSimulating battle 2095 of 3020: marowak vs machop\nSimulating battle 2096 of 3020: marowak vs bulbasaur\nSimulating battle 2097 of 3020: marowak vs mankey\nSimulating battle 2098 of 3020: marowak vs mewtwo\nSimulating battle 2099 of 3020: marowak vs rattata\nSimulating battle 2100 of 3020: marowak vs butterfree\nSimulating battle 2101 of 3020: hitmonlee vs weezing\nSimulating battle 2102 of 3020: hitmonlee vs muk\nSimulating battle 2103 of 3020: hitmonlee vs pidgeotto\nSimulating battle 2104 of 3020: hitmonlee vs kabuto\nSimulating battle 2105 of 3020: hitmonlee vs pidgey\nSimulating battle 2106 of 3020: hitmonlee vs pidgeot\nSimulating battle 2107 of 3020: hitmonlee vs mr-mime\nSimulating battle 2108 of 3020: hitmonlee vs blastoise\nSimulating battle 2109 of 3020: hitmonlee vs geodude\nSimulating battle 2110 of 3020: hitmonlee vs marowak\nSimulating battle 2111 of 3020: hitmonlee vs parasect\nSimulating battle 2112 of 3020: hitmonlee vs clefairy\nSimulating battle 2113 of 3020: hitmonlee vs koffing\nSimulating battle 2114 of 3020: hitmonlee vs drowzee\nSimulating battle 2115 of 3020: hitmonlee vs krabby\nSimulating battle 2116 of 3020: hitmonlee vs horsea\nSimulating battle 2117 of 3020: hitmonlee vs hypno\nSimulating battle 2118 of 3020: hitmonlee vs spearow\nSimulating battle 2119 of 3020: hitmonlee vs shellder\nSimulating battle 2120 of 3020: hitmonlee vs nidoqueen\nSimulating battle 2121 of 3020: hitmonchan vs paras\nSimulating battle 2122 of 3020: hitmonchan vs omastar\nSimulating battle 2123 of 3020: hitmonchan vs electrode\nSimulating battle 2124 of 3020: hitmonchan vs porygon\nSimulating battle 2125 of 3020: hitmonchan vs nidorino\nSimulating battle 2126 of 3020: hitmonchan vs growlithe\nSimulating battle 2127 of 3020: hitmonchan vs bulbasaur\nSimulating battle 2128 of 3020: hitmonchan vs charizard\nSimulating battle 2129 of 3020: hitmonchan vs ponyta\nSimulating battle 2130 of 3020: hitmonchan vs staryu\nSimulating battle 2131 of 3020: hitmonchan vs kabutops\nSimulating battle 2132 of 3020: hitmonchan vs weezing\nSimulating battle 2133 of 3020: hitmonchan vs omanyte\nSimulating battle 2134 of 3020: hitmonchan vs krabby\nSimulating battle 2135 of 3020: hitmonchan vs arcanine\nSimulating battle 2136 of 3020: hitmonchan vs kadabra\nSimulating battle 2137 of 3020: hitmonchan vs seaking\nSimulating battle 2138 of 3020: hitmonchan vs muk\nSimulating battle 2139 of 3020: hitmonchan vs wigglytuff\nSimulating battle 2140 of 3020: hitmonchan vs victreebel\nSimulating battle 2141 of 3020: lickitung vs venomoth\nSimulating battle 2142 of 3020: lickitung vs nidoran-f\nSimulating battle 2143 of 3020: lickitung vs blastoise\nSimulating battle 2144 of 3020: lickitung vs koffing\nSimulating battle 2145 of 3020: lickitung vs charmeleon\nSimulating battle 2146 of 3020: lickitung vs poliwrath\nSimulating battle 2147 of 3020: lickitung vs persian\nSimulating battle 2148 of 3020: lickitung vs pidgeot\nSimulating battle 2149 of 3020: lickitung vs raichu\nSimulating battle 2150 of 3020: lickitung vs horsea\nSimulating battle 2151 of 3020: lickitung vs weedle\nSimulating battle 2152 of 3020: lickitung vs abra\nSimulating battle 2153 of 3020: lickitung vs butterfree\nSimulating battle 2154 of 3020: lickitung vs exeggutor\nSimulating battle 2155 of 3020: lickitung vs tangela\nSimulating battle 2156 of 3020: lickitung vs poliwag\nSimulating battle 2157 of 3020: lickitung vs kabuto\nSimulating battle 2158 of 3020: lickitung vs mankey\nSimulating battle 2159 of 3020: lickitung vs beedrill\nSimulating battle 2160 of 3020: lickitung vs clefable\nSimulating battle 2161 of 3020: koffing vs gyarados\nSimulating battle 2162 of 3020: koffing vs graveler\nSimulating battle 2163 of 3020: koffing vs poliwag\nSimulating battle 2164 of 3020: koffing vs dragonite\nSimulating battle 2165 of 3020: koffing vs magneton\nSimulating battle 2166 of 3020: koffing vs farfetchd\nSimulating battle 2167 of 3020: koffing vs poliwhirl\nSimulating battle 2168 of 3020: koffing vs rapidash\nSimulating battle 2169 of 3020: koffing vs vulpix\nSimulating battle 2170 of 3020: koffing vs jolteon\nSimulating battle 2171 of 3020: koffing vs primeape\nSimulating battle 2172 of 3020: koffing vs hitmonlee\nSimulating battle 2173 of 3020: koffing vs ponyta\nSimulating battle 2174 of 3020: koffing vs victreebel\nSimulating battle 2175 of 3020: koffing vs pidgey\nSimulating battle 2176 of 3020: koffing vs articuno\nSimulating battle 2177 of 3020: koffing vs vileplume\nSimulating battle 2178 of 3020: koffing vs rhyhorn\nSimulating battle 2179 of 3020: koffing vs tauros\nSimulating battle 2180 of 3020: koffing vs weedle\nSimulating battle 2181 of 3020: weezing vs muk\nSimulating battle 2182 of 3020: weezing vs krabby\nSimulating battle 2183 of 3020: weezing vs flareon\nSimulating battle 2184 of 3020: weezing vs magneton\nSimulating battle 2185 of 3020: weezing vs hitmonchan\nSimulating battle 2186 of 3020: weezing vs marowak\nSimulating battle 2187 of 3020: weezing vs jigglypuff\nSimulating battle 2188 of 3020: weezing vs ponyta\nSimulating battle 2189 of 3020: weezing vs hypno\nSimulating battle 2190 of 3020: weezing vs venonat\nSimulating battle 2191 of 3020: weezing vs omastar\nSimulating battle 2192 of 3020: weezing vs scyther\nSimulating battle 2193 of 3020: weezing vs poliwrath\nSimulating battle 2194 of 3020: weezing vs growlithe\nSimulating battle 2195 of 3020: weezing vs vulpix\nSimulating battle 2196 of 3020: weezing vs staryu\nSimulating battle 2197 of 3020: weezing vs beedrill\nSimulating battle 2198 of 3020: weezing vs zapdos\nSimulating battle 2199 of 3020: weezing vs hitmonlee\nSimulating battle 2200 of 3020: weezing vs articuno\nSimulating battle 2201 of 3020: rhyhorn vs porygon\nSimulating battle 2202 of 3020: rhyhorn vs clefairy\nSimulating battle 2203 of 3020: rhyhorn vs voltorb\nSimulating battle 2204 of 3020: rhyhorn vs abra\nSimulating battle 2205 of 3020: rhyhorn vs machop\nSimulating battle 2206 of 3020: rhyhorn vs persian\nSimulating battle 2207 of 3020: rhyhorn vs dodrio\nSimulating battle 2208 of 3020: rhyhorn vs spearow\nSimulating battle 2209 of 3020: rhyhorn vs seadra\nSimulating battle 2210 of 3020: rhyhorn vs onix\nSimulating battle 2211 of 3020: rhyhorn vs arbok\nSimulating battle 2212 of 3020: rhyhorn vs omastar\nSimulating battle 2213 of 3020: rhyhorn vs venomoth\nSimulating battle 2214 of 3020: rhyhorn vs kakuna\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[19], line 88\n     82     winner = simulate_battle(row)\n     83     results.append({\n     84         \"challenger\": row['challenger'],\n     85         \"opponent\": row['opponent'],\n     86         \"winner\": winner\n     87     })\n---&gt; 88     time.sleep(1.5)  # Respect rate limits\n     90 # ---- Save Results ---- #\n     92 results_df = pd.DataFrame(results)\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html",
    "href": "posts/Lab-7/mlm-02.html",
    "title": "Lab-7",
    "section": "",
    "text": "When to use them:\n\nNested designs\nRepeated measures\nLongitudinal data\nComplex designs\n\nWhy use them:\n\nCaptures variance occurring between groups and within groups\n\nWhat they are:\n\nLinear model with extra residuals"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#multilevel-models",
    "href": "posts/Lab-7/mlm-02.html#multilevel-models",
    "title": "Lab-7",
    "section": "",
    "text": "When to use them:\n\nNested designs\nRepeated measures\nLongitudinal data\nComplex designs\n\nWhy use them:\n\nCaptures variance occurring between groups and within groups\n\nWhat they are:\n\nLinear model with extra residuals"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#today",
    "href": "posts/Lab-7/mlm-02.html#today",
    "title": "Lab-7",
    "section": "Today",
    "text": "Today\n\nEverything you need to know to run and report a MLM\n\nOrganizing data for MLM analysis\nEstimation\nFit and interpret multilevel models\nVisualization\nEffect size\nReporting\nPower"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#packages",
    "href": "posts/Lab-7/mlm-02.html#packages",
    "title": "Lab-7",
    "section": "Packages",
    "text": "Packages\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE,fig.path = \"Lab-6-figs/fig_\")\noptions(scipen=999)\n\nlibrary(pacman)\n\npacman::p_load(tidyverse, knitr, lme4, lmerTest, broom.mixed, afex, emmeans, ggeffects, easystats,ggeffects,ggrain,easystats,afex, install = T)\noptions(scipen=999) # get rid of sci notation\n\n\n### create plot aesthetics \npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\n\nFind the .qmd document here to follow along: https://github.com/suyoghc/PSY-504_Spring-2025/blob/main/Multilevel%20Modeling/mlm-02.qmd"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#todays-data",
    "href": "posts/Lab-7/mlm-02.html#todays-data",
    "title": "Lab-7",
    "section": "Today’s data",
    "text": "Today’s data\n\nWhat did you say?\n\nPs (N = 31) listened to both clear (NS) and 6 channel vocoded speech (V6)\n\n(https://www.mrc-cbu.cam.ac.uk/personal/matt.davis/vocode/a1_6.wav)\n\nFixed factor: ?\nRandom factor: ?"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#todays-data-1",
    "href": "posts/Lab-7/mlm-02.html#todays-data-1",
    "title": "Lab-7",
    "section": "Today’s data",
    "text": "Today’s data\n\neye  &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/vocoded_pupil.csv\") # data for class"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#data-organization",
    "href": "posts/Lab-7/mlm-02.html#data-organization",
    "title": "Lab-7",
    "section": "Data organization",
    "text": "Data organization\n\nData Structure\n\nMLM analysis (in R) requires data in long format"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#data-organization-1",
    "href": "posts/Lab-7/mlm-02.html#data-organization-1",
    "title": "Lab-7",
    "section": "Data organization",
    "text": "Data organization\n\nLevel 1: trial\nLevel 2: subject\n\n\n\n\n\n\nsubject\ntrial\nvocoded\nmean_pupil\n\n\n\n\nEYE15\n3\nV6\n0.0839555\n\n\nEYE15\n4\nV6\n0.0141083\n\n\nEYE15\n5\nV6\n0.0224967\n\n\nEYE15\n6\nV6\n0.0007424\n\n\nEYE15\n7\nV6\n0.0242540\n\n\nEYE15\n8\nV6\n0.0267617"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#centering",
    "href": "posts/Lab-7/mlm-02.html#centering",
    "title": "Lab-7",
    "section": "Centering",
    "text": "Centering\n\n\n\nIn a single-level regression, centering ensures that the zero value for each predictor is meaningful before running the model\nIn MLM, if you have specific questions about within, between, and contextual effects, you need to center!"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#group--vs.-grand-mean-centering",
    "href": "posts/Lab-7/mlm-02.html#group--vs.-grand-mean-centering",
    "title": "Lab-7",
    "section": "Group- vs. Grand-Mean Centering",
    "text": "Group- vs. Grand-Mean Centering\n\nGrand-mean centering: \\(x_{ij} - x\\)\n\nVariable represents each observation’s deviation from everyone’s norm, regardless of group\n\nGroup-mean centering: \\(x_{ij} - x_j\\)\n\nVariable represents each observation’s deviation from their group’s norm (removes group effect)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#group--vs.-grand-mean-centering-1",
    "href": "posts/Lab-7/mlm-02.html#group--vs.-grand-mean-centering-1",
    "title": "Lab-7",
    "section": "Group- vs. Grand-Mean Centering",
    "text": "Group- vs. Grand-Mean Centering\n\n\n\nLevel 1 predictors\n\nGrand-mean centering\n\nInclude means of level 2\n\nAllows us to directly test within-group effect\nCoefficient associated with the Level 2 group mean represents contextual effect\n\n\n\n\n\n\nGroup-mean centering\n\nLevel 1 coefficient will always be with within-group effect, regardless of whether the group means are included at Level 2 or not\nIf level 2 means included, coefficient represents the between-groups effect\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan apply to categorical predictors as well (see Yaremych, Preacher, & Hedeker, 2023)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#centering-in-r",
    "href": "posts/Lab-7/mlm-02.html#centering-in-r",
    "title": "Lab-7",
    "section": "Centering in R",
    "text": "Centering in R\n\n# how to group mean center \nd &lt;- d %&gt;% \n  # Grand mean centering (CMC)\n  mutate(iv.gmc = iv-mean(iv)) %&gt;%\n  # group  mean centering (more generally, centering within cluster)\n  group_by(id) %&gt;% \n  mutate(iv.cm = mean(iv),\n         iv.cwc = iv-iv.cm)\n\nlibrary(datawizard) #easystats \n\n#data wizard way\nx &lt;- demean(x, select=c(\"x\"), group=\"ID\") #gets within-group and included cluster means"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#maximum-likelihood",
    "href": "posts/Lab-7/mlm-02.html#maximum-likelihood",
    "title": "Lab-7",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\n\n\nIn MLM we try to maximize the likelihood of the data\n\nNo OLS!"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#probability-vs.-likelihood",
    "href": "posts/Lab-7/mlm-02.html#probability-vs.-likelihood",
    "title": "Lab-7",
    "section": "Probability vs. Likelihood",
    "text": "Probability vs. Likelihood\n\nProbability\n\n\nIf I assume a distribution with certain parameters (fixed), what is the probability I see a particular value in the data?\n\n\n\n\nPr⁡(𝑦&gt;0│𝜇=0,𝜎=1)=.50\nPr⁡(−1&lt;𝑦&lt;1│𝜇=0,𝜎=1)=.68\nPr⁡(0&lt;𝑦&lt;1│𝜇=0,𝜎=1)=.34\nPr⁡(𝑦&gt;2│𝜇=0,𝜎=1)=.02"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood",
    "href": "posts/Lab-7/mlm-02.html#likelihood",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood\n\n\n\n\\(L(𝜇,𝜎│𝑥)\\)\nHolding a sample of data constant, which parameter values are more likely?\n\nWhich values have higher likelihood?\n\nHere data is fixed and distribution can change"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood-1",
    "href": "posts/Lab-7/mlm-02.html#likelihood-1",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood-2",
    "href": "posts/Lab-7/mlm-02.html#likelihood-2",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood-3",
    "href": "posts/Lab-7/mlm-02.html#likelihood-3",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood-4",
    "href": "posts/Lab-7/mlm-02.html#likelihood-4",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood-5",
    "href": "posts/Lab-7/mlm-02.html#likelihood-5",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#likelihood-6",
    "href": "posts/Lab-7/mlm-02.html#likelihood-6",
    "title": "Lab-7",
    "section": "Likelihood",
    "text": "Likelihood\nInteractive: Understanding Maximum Likelihood Estimation: https://rpsychologist.com/likelihood/"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#log-likelihood",
    "href": "posts/Lab-7/mlm-02.html#log-likelihood",
    "title": "Lab-7",
    "section": "Log likelihood",
    "text": "Log likelihood\n\nWith large samples, likelihood values ℒ(𝜇,𝜎│𝑥) get very small very fast\n\nTo make them easier to work with, we usually work with the log-likelihood\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\n\n\nDeviance = \\(-2logL\\)\n\n\\(-2logL\\) follows a \\(\\chi^2\\) distribution with \\(n (\\text{sample size}) - p (\\text{paramters}) - 1\\) degrees of freedom"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#chi2-distribution",
    "href": "posts/Lab-7/mlm-02.html#chi2-distribution",
    "title": "Lab-7",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#comparing-nested-models",
    "href": "posts/Lab-7/mlm-02.html#comparing-nested-models",
    "title": "Lab-7",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses:\n\n\\(H_0\\): smaller model is better\n\\(H_1\\): Larger model is better\n\nTo do so, we will use the drop-in-deviance test (also known as the nested likelihood ratio test)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#drop-in-deviance-test",
    "href": "posts/Lab-7/mlm-02.html#drop-in-deviance-test",
    "title": "Lab-7",
    "section": "Drop-In-Deviance Test",
    "text": "Drop-In-Deviance Test\n\nHypotheses:\n\n\\(H_0\\): smaller model is better\n\\(H_1\\): Larger model is better\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\nP-value: \\(P(\\chi^2 &gt; G)\\):\n\nCalculated using a \\(\\chi^2\\) distribution\ndf = \\(df_1\\) - \\(df_2\\)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#testing-deviance",
    "href": "posts/Lab-7/mlm-02.html#testing-deviance",
    "title": "Lab-7",
    "section": "Testing deviance",
    "text": "Testing deviance\n\nWe can use the anova function to conduct this test\n\nAdd test = “Chisq” to conduct the drop-in-deviance test\n\nI like test_likelihoodratio from easystats\n\n\nanova(model1, model2, test=\"chisq\")\n\n# test using easystats function\n\ntest_likelihoodratio(model1, model2)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#model-fitting-ml-or-reml",
    "href": "posts/Lab-7/mlm-02.html#model-fitting-ml-or-reml",
    "title": "Lab-7",
    "section": "Model fitting: ML or REML?",
    "text": "Model fitting: ML or REML?\n\nTwo flavors of maximum likelihood\n\nMaximum Likelihood (ML or FIML)\n\nJointly estimate the fixed effects and variance components using all the sample data\nCan be used to draw conclusions about fixed and random effects\nIssue:\n\nResults are biased because fixed effects are estimated without error"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#model-fitting-ml-or-reml-1",
    "href": "posts/Lab-7/mlm-02.html#model-fitting-ml-or-reml-1",
    "title": "Lab-7",
    "section": "Model fitting: ML or REML",
    "text": "Model fitting: ML or REML\n\nRestricted Maximum Likelihood (REML)\n\nEstimates the variance components using the sample residuals not the sample data\nIt is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates\n\nThis results in unbiased estimates of variance components\nAssociated with error/penalty"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#model-fitting-ml-or-reml-2",
    "href": "posts/Lab-7/mlm-02.html#model-fitting-ml-or-reml-2",
    "title": "Lab-7",
    "section": "Model fitting: ML or REML?",
    "text": "Model fitting: ML or REML?\n\nResearch has not determined one method absolutely superior to the other\nREML (REML = TRUE; default in lmer) is preferable when:\n\nThe number of parameters is large\nPrimary objective is to obtain relaible estimates of the variance parameters\nFor REML, likelihood ratio tests can only be used to draw conclusions about variance components\n\nML (REML = FALSE) must be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#ml-or-reml",
    "href": "posts/Lab-7/mlm-02.html#ml-or-reml",
    "title": "Lab-7",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nWhat would we use if we wanted to compare the below models?\n\n\nx= lmer(DV ~ IV1 + IV2 + (1|ID))\n\ny= lmer(DV ~ IV1*IV2 + (1|ID))"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#ml-or-reml-1",
    "href": "posts/Lab-7/mlm-02.html#ml-or-reml-1",
    "title": "Lab-7",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nWhat would we use if we wanted to compare the below models?\n\n\nx = lmer(DV ~ IV1 + IV2 + (1+IV2|ID))\n\ny = lmer(DV ~ IV1+ IV2 + (1|ID))"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#modeling-approach",
    "href": "posts/Lab-7/mlm-02.html#modeling-approach",
    "title": "Lab-7",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nForward/backward approach\n\n\n\nKeep it maximal1\n\nWhatever can vary, should vary\n\nDecreases Type 1 error"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#modeling-approach-1",
    "href": "posts/Lab-7/mlm-02.html#modeling-approach-1",
    "title": "Lab-7",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nFull (maximal) model\n\nOnly when there is convergence issues should you remove terms\n\nif non-convergence (pay attention to warning messages in summary output!):\n\nTry different optimizer (afex::all_fit())\n\nSort out random effects\n\nRemove correlations between slopes and intercepts\nRandom slopes\nRandom Intercepts\n\nSort out fixed effects (e.g., interaction)\nOnce you arrive at the final model present it using REML estimation"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#modeling-approach-2",
    "href": "posts/Lab-7/mlm-02.html#modeling-approach-2",
    "title": "Lab-7",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nIf your model is singular (check output!!!!)\n\nVariance might be close to 0\nPerfect correlations (1 or -1)\n\nDrop the parameter!"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#modeling-approach-3",
    "href": "posts/Lab-7/mlm-02.html#modeling-approach-3",
    "title": "Lab-7",
    "section": "Modeling approach",
    "text": "Modeling approach\n\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/heck2011.csv\")\n\nsummary(lmer(math~ses + (1+ses|schcode), data=data))\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: math ~ ses + (1 + ses | schcode)\n   Data: data\n\nREML criterion at convergence: 48190.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8578 -0.5553  0.1290  0.6437  5.7098 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schcode  (Intercept)  3.2042  1.7900        \n          ses          0.7794  0.8828   -1.00\n Residual             62.5855  7.9111        \nNumber of obs: 6871, groups:  schcode, 419\n\nFixed effects:\n             Estimate Std. Error        df t value            Pr(&gt;|t|)    \n(Intercept)   57.6959     0.1315  378.6378  438.78 &lt;0.0000000000000002 ***\nses            3.9602     0.1408 1450.7730   28.12 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nses -0.284\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n. . .\n\nlmer(math~ses + (1+ses||schcode), data=data) # removes correlation() with double pipes. Does not work with categorical variables"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#null-model-unconditional-means",
    "href": "posts/Lab-7/mlm-02.html#null-model-unconditional-means",
    "title": "Lab-7",
    "section": "Null model (unconditional means)",
    "text": "Null model (unconditional means)\nGet ICC\n\nICC is a standardized way of expressing how much variance is due to clustering/group\n\nRanges from 0-1\n\nCan also be interpreted as correlation among observations within cluster/group!\nIf ICC is sufficiently low (i.e., \\(\\rho\\) &lt; .1), then you don’t have to use MLM! BUT YOU PROBABLY SHOULD 🙂"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#null-model-unconditional-means-1",
    "href": "posts/Lab-7/mlm-02.html#null-model-unconditional-means-1",
    "title": "Lab-7",
    "section": "Null model (unconditional means)",
    "text": "Null model (unconditional means)\n\nlibrary(lme4) # pop linear modeling package\n\nnull_model &lt;- lmer(mean_pupil ~ (1|subject), data = eye, REML=TRUE)\n\nsummary(null_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ (1 | subject)\n   Data: eye\n\nREML criterion at convergence: -19811.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1411 -0.5530 -0.0463  0.4822 10.8130 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n subject  (Intercept) 0.0001303 0.01142 \n Residual             0.0016840 0.04104 \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.005227   0.002124 29.457784   2.461   0.0199 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#calculating-icc",
    "href": "posts/Lab-7/mlm-02.html#calculating-icc",
    "title": "Lab-7",
    "section": "Calculating ICC",
    "text": "Calculating ICC\n\nRun baseline (null) model\nGet intercept variance and residual variance\n\n\\[\\mathrm{ICC}=\\frac{\\text { between-group variability }}{\\text { between-group variability+within-group variability}}\\]\n\\[\nICC=\\frac{\\operatorname{Var}\\left(u_{0 j}\\right)}{\\operatorname{Var}\\left(u_{0 j}\\right)+\\operatorname{Var}\\left(e_{i j}\\right)}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma^{2}}\n\\]\n\n# easystats \n#adjusted icc just random effects\n#unadjusted fixed effects taken into account\nperformance::icc(null_model)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.072\n  Unadjusted ICC: 0.072"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#maximal-model-fixed-effect-random-intercepts-subject-and-slopes-vocoded-model",
    "href": "posts/Lab-7/mlm-02.html#maximal-model-fixed-effect-random-intercepts-subject-and-slopes-vocoded-model",
    "title": "Lab-7",
    "section": "Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model",
    "text": "Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model\n\nmax_model &lt;- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)\n\nsummary(max_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#fixed-effects",
    "href": "posts/Lab-7/mlm-02.html#fixed-effects",
    "title": "Lab-7",
    "section": "Fixed effects",
    "text": "Fixed effects\n\nInterpretation same as lm\n\n\n#grab the fixed effects\nsummary(max_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#degrees-of-freedom-and-p-values",
    "href": "posts/Lab-7/mlm-02.html#degrees-of-freedom-and-p-values",
    "title": "Lab-7",
    "section": "Degrees of freedom and p-values",
    "text": "Degrees of freedom and p-values\n\nDegrees of freedom (denominator) and p-values can be assessed with several methods:\n\nSatterthwaite (default when install lmerTest and then run lmer)\nAsymptotic (Inf) (default behavior lme4)\nKenward-Rogers"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#random-effectsvariance-components",
    "href": "posts/Lab-7/mlm-02.html#random-effectsvariance-components",
    "title": "Lab-7",
    "section": "Random effects/variance components",
    "text": "Random effects/variance components\n\nTells us how much variability there is around the fixed intercept/slope\n\nHow much does the average pupil size change between participants\n\n\n\nsummary(max_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#random-effectsvariance-components-1",
    "href": "posts/Lab-7/mlm-02.html#random-effectsvariance-components-1",
    "title": "Lab-7",
    "section": "Random effects/variance components",
    "text": "Random effects/variance components\n\nCorrelation between random intercepts and slopes\n\nNegative correlation\n\nHigher intercept (for normal speech) less of effect (lower slope)\n\n\n\n\n\nParameter1 |  Parameter2 |     r |        95% CI | t(29) |     p\n----------------------------------------------------------------\nvocodedV6  | (Intercept) | -0.10 | [-0.44, 0.26] | -0.57 | 0.576\n\nObservations: 31"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#visualize-random-effects",
    "href": "posts/Lab-7/mlm-02.html#visualize-random-effects",
    "title": "Lab-7",
    "section": "Visualize Random Effects",
    "text": "Visualize Random Effects\n\n# use easystats to grab group variance\nrandom &lt;- estimate_grouplevel(max_model)\n\nplot(random) + plot_aes"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#model-comparisons",
    "href": "posts/Lab-7/mlm-02.html#model-comparisons",
    "title": "Lab-7",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nCan compare models using anova function or test_likelihoodratio from easystats\n\nWill be refit using ML if interested in fixed effects\n\n\n\n# you try\n\nanova(max_model,null_model)\n\nData: eye\nModels:\nnull_model: mean_pupil ~ (1 | subject)\nmax_model: mean_pupil ~ vocoded + (1 + vocoded | subject)\n           npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)   \nnull_model    3 -19816 -19796 9911.1   -19822                        \nmax_model     6 -19823 -19784 9917.7   -19835 13.269  3    0.00409 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#aic",
    "href": "posts/Lab-7/mlm-02.html#aic",
    "title": "Lab-7",
    "section": "AIC",
    "text": "AIC\n\nAIC:\n\n\\[\nD + 2p\n\\]\n\nwhere d = deviance and p = # of parameters in model\nCan compare AICs2:\n\\[\n\\Delta_i = AIC_{i} - AIC_{min}\n\\]\nLess than 2: More parsimonious model is preferred\nBetween 4 and 7: some evidence for lower AIC model\nGreater than 10,: strong evidence for lower AIC"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#bic",
    "href": "posts/Lab-7/mlm-02.html#bic",
    "title": "Lab-7",
    "section": "BIC",
    "text": "BIC\n\nBIC:\n\n\\[\nD + ln(n)*p\n\\]\n\nwhere d = deviance, p = # of parameters in model, n = sample size\nChange in BIC:\n\n\\(\\Delta{BIC}\\) &lt;= 2 (No difference)\n\\(\\Delta{BIC}\\) &gt; 3 (evidence for smaller BIC model)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#aicbic-1",
    "href": "posts/Lab-7/mlm-02.html#aicbic-1",
    "title": "Lab-7",
    "section": "AIC/BIC",
    "text": "AIC/BIC\n\nperformance::model_performance(max_model) %&gt;% # easystats\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2_conditional\nR2_marginal\nICC\nRMSE\nSigma\n\n\n\n\n-19801.66\n-19801.65\n-19761.87\n0.0773469\n0.0013442\n0.076105\n0.0407697\n0.0409264"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#hypothesis-testing",
    "href": "posts/Lab-7/mlm-02.html#hypothesis-testing",
    "title": "Lab-7",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nMultiple options\n\nt/F tests with approximate degrees of freedom (Kenward-Rogers or Satterwaithe)\nParametric bootstrap\nLikelihood ratio test (LRT)\n\n\nCan be interpreted as main effects and interactions\nUse afex package to do that"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#hypothesis-testing---afex",
    "href": "posts/Lab-7/mlm-02.html#hypothesis-testing---afex",
    "title": "Lab-7",
    "section": "Hypothesis testing - afex",
    "text": "Hypothesis testing - afex\n\nlibrary(afex) # load afex in \n\nm &lt;- mixed(mean_pupil ~ 1 + vocoded +  (1+vocoded|subject), data =eye, method = \"LRT\") # fit lmer using afex\n\nnice(m) %&gt;%\n  kable()\n\n\n\n\nEffect\ndf\nChisq\np.value\n\n\n\n\nvocoded\n1\n4.47 *\n.034"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#using-emmeans",
    "href": "posts/Lab-7/mlm-02.html#using-emmeans",
    "title": "Lab-7",
    "section": "Using emmeans",
    "text": "Using emmeans\n\nGet means and contrasts\n\n\nlibrary(emmeans) # get marginal means \n\nemmeans(max_model, specs = \"vocoded\") %&gt;% \n  kable() # grabs means/SEs for each level of vocode \n\n\n\n\nvocoded\nemmean\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\nNS\n0.0036427\n0.0022348\nInf\n-0.0007374\n0.0080229\n\n\nV6\n0.0067668\n0.0022618\nInf\n0.0023337\n0.0111999\n\n\n\n\npairs(emmeans(max_model, specs = \"vocoded\")) %&gt;%\n  confint() %&gt;%\n  kable()\n\n\n\n\ncontrast\nestimate\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\nNS - V6\n-0.0031241\n0.0014532\nInf\n-0.0059723\n-0.0002759\n\n\n\n\n# use this to get pariwise compairsons between levels of factors"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#check-assumptions",
    "href": "posts/Lab-7/mlm-02.html#check-assumptions",
    "title": "Lab-7",
    "section": "Check assumptions",
    "text": "Check assumptions\n\n\n\nLinearity\nNormality\n\nLevel 1 residuals are normally distributed around zero\nLevel 2 residuals are multivariate-normal with a mean of zero\n\nHomoskedacticity\n\nLevel 1/Level 2 predictors and residuals are homoskedastic\n\n\n\n\nCollinearity\nOutliers"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#assumptions-1",
    "href": "posts/Lab-7/mlm-02.html#assumptions-1",
    "title": "Lab-7",
    "section": "Assumptions",
    "text": "Assumptions\n\nlibrary(easystats) # performance package\n\ncheck_model(max_model)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#visualization",
    "href": "posts/Lab-7/mlm-02.html#visualization",
    "title": "Lab-7",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#ggeffects",
    "href": "posts/Lab-7/mlm-02.html#ggeffects",
    "title": "Lab-7",
    "section": "ggeffects",
    "text": "ggeffects\n\nggemmeans(max_model, terms=c(\"vocoded\")) %&gt;% plot()"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#effect-size",
    "href": "posts/Lab-7/mlm-02.html#effect-size",
    "title": "Lab-7",
    "section": "Effect size",
    "text": "Effect size\n\nReport pseudo-\\(R^2\\) for marginal (fixed) and conditional model (full model) (Nakagawa et al. 2017)\n\n\\[\nR^2_{LMM(c)} = \\frac{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n\\]\n\\[\nR^2_{\\text{LMM}(m)} = \\frac{\\sigma_f^2\\text{fixed}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n\\]\n\nReport semi-partial \\(R^2\\) for each predictor variable\n\n\\(R^2_\\beta\\)\n\npartR2 package in R does this for you"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#effect-size-1",
    "href": "posts/Lab-7/mlm-02.html#effect-size-1",
    "title": "Lab-7",
    "section": "Effect size",
    "text": "Effect size\n\n#get r2 for model with performance from easystats\n\nperformance::r2(max_model) \n\n# R2 for Mixed Models\n\n  Conditional R2: 0.077\n     Marginal R2: 0.001\n\n\n\n# get semi-part\nlibrary(partR2)\n# does not work with random slopes for some reason :/\nR2_3 &lt;- partR2(max_model,data=eye, \n  partvars = c(\"vocoded\"),\n  R2_type = \"marginal\", nboot = 10, CI = 0.95\n)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#effect-size-2",
    "href": "posts/Lab-7/mlm-02.html#effect-size-2",
    "title": "Lab-7",
    "section": "Effect size",
    "text": "Effect size\n\nCohen’s \\(d\\) for treatment effects/categorical predictions3\n\n\\[\nd = \\frac{\\text{Effect}}{\\sqrt{\\sigma^2_\\text{Intercept} + \\sigma^2_\\text{slope} + \\sigma^2_\\text{residual}}}\n\\]\n\nemmeans(max_model,~ vocoded) %&gt;% \n eff_size(.,sigma=.04, edf=30) # need to calcuate sigma and add dfs\n\n contrast effect.size     SE  df asymp.LCL asymp.UCL\n NS - V6      -0.0781 0.0377 Inf    -0.152  -0.00421\n\nsigma used for effect sizes: 0.04 \nDegrees-of-freedom method: inherited from asymptotic when re-gridding \nConfidence level used: 0.95"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#describing-a-mlm-analysis---structure",
    "href": "posts/Lab-7/mlm-02.html#describing-a-mlm-analysis---structure",
    "title": "Lab-7",
    "section": "Describing a MLM analysis - Structure",
    "text": "Describing a MLM analysis - Structure\n\nWhat was the nested data structure (e.g., how many levels; what were the units at each level?)\n• How many units were in each level, on average?\n• What was the range of the number of lower-level units in each group/cluster?"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#describing-a-mlm-analysis---model",
    "href": "posts/Lab-7/mlm-02.html#describing-a-mlm-analysis---model",
    "title": "Lab-7",
    "section": "Describing a MLM analysis - Model",
    "text": "Describing a MLM analysis - Model\n\n\n\nWhat equation can best represent your model?\nWhat estimation method was used (e.g., ML, REML)?\nIf there were convergence issues, how was this addressed?\nWhat software (and version) was used (when using R, what packages as well)?\n\n\n\nIf degrees of freedom were used, what kind?\nWhat type of models were estimated (i.e., unconditional, random intercept, random slope, max)?\nWhat variables were centered and what kind of centering was used?\nWhat model assumptions were checked and what were the results?"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#describing-a-mlm-analysis---results",
    "href": "posts/Lab-7/mlm-02.html#describing-a-mlm-analysis---results",
    "title": "Lab-7",
    "section": "Describing a MLM analysis - Results",
    "text": "Describing a MLM analysis - Results\n\n\n\nWhat was the ICC of the outcome variable?\nAre fixed effects and variance components reported?\nWhat inferential statistics were used (e.g., t-statistics, LRTs)?\nHow precise were the results (report the standard errors and/or confidence intervals)?\n\n\n\nWere model comparisons performed (e.g., AIC, BIC, if using an LRT,report the χ2, degrees of freedom, and p value)?\nWere effect sizes reported for overall model and individual predictors (e.g., Cohen’s d, \\(R^2\\) )?"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#write-up",
    "href": "posts/Lab-7/mlm-02.html#write-up",
    "title": "Lab-7",
    "section": "Write-up",
    "text": "Write-up\n\nreport::report(max_model) # easystats report function\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict mean_pupil with vocoded (formula: mean_pupil ~ vocoded). The model included vocoded as random effects (formula: ~1 + vocoded | subject). The model’s total explanatory power is weak (conditional R2 = 0.08) and the part related to the fixed effects alone (marginal R2) is of 1.34e-03. The model’s intercept, corresponding to vocoded = NS, is at 3.64e-03 (95% CI [-7.38e-04, 8.02e-03], t(5603) = 1.63, p = 0.103). Within this model:\n\nThe effect of vocoded [V6] is statistically significant and positive (beta = 3.12e-03, 95% CI [2.75e-04, 5.97e-03], t(5603) = 2.15, p = 0.032; Std. beta = 0.07, 95% CI [6.49e-03, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation."
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#table",
    "href": "posts/Lab-7/mlm-02.html#table",
    "title": "Lab-7",
    "section": "Table",
    "text": "Table\n\nmodelsummary::modelsummary(list(\"max model\" = max_model), output=\"html\") # modelsummary package\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                max model\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.004\n                \n                \n                  \n                  (0.002)\n                \n                \n                  vocodedV6\n                  0.003\n                \n                \n                  \n                  (0.001)\n                \n                \n                  SD (Intercept subject)\n                  0.012\n                \n                \n                  SD (vocodedV6 subject)\n                  0.005\n                \n                \n                  Cor (Intercept~vocodedV6 subject)\n                  −0.195\n                \n                \n                  SD (Observations)\n                  0.041\n                \n                \n                  Num.Obs.\n                  5609\n                \n                \n                  R2 Marg.\n                  0.001\n                \n                \n                  R2 Cond.\n                  0.077\n                \n                \n                  AIC\n                  −19801.7\n                \n                \n                  BIC\n                  −19761.9\n                \n                \n                  ICC\n                  0.1\n                \n                \n                  RMSE\n                  0.04"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#power",
    "href": "posts/Lab-7/mlm-02.html#power",
    "title": "Lab-7",
    "section": "Power",
    "text": "Power\n\nSimulation-based power analyses\n\nSimulate new data\n\nfaux (https://debruine.github.io/faux/articles/sim_mixed.html)\n\nUse pilot data (what I would do)\n\nmixedpower(https://link.springer.com/article/10.3758/s13428-021-01546-0)\nsimr (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504)"
  },
  {
    "objectID": "posts/Lab-7/mlm-02.html#footnotes",
    "href": "posts/Lab-7/mlm-02.html#footnotes",
    "title": "Lab-7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001↩︎\nBURNHAM, ANDERSON, & HUYVAERT (2011)↩︎\nBrysbaert, M., & Debeer, D. (2023, September 12). How to run linear mixed effects analysis for pairwise comparisons? A tutorial and a proposal for the calculation of standardized effect sizes. https://doi.org/10.31234/osf.io/esnku↩︎"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#overview",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#overview",
    "title": "Missing Data",
    "section": "Overview",
    "text": "Overview\n\nImportance of addressing missing data in research\n\nTraditional vs. modern methods\n\nFocus on:\n\nMaximum Likelihood (ML)\n\nMultiple Imputation (MI)\n\n\nBenefits of modern methods\n\nDemo using a toy dataset\n\n\nHi! Today im going to be talking to you today about missing data.\nMissing data is a common problem in research, and it can have a significant impact on the validity of your results.\nIn this presentation, I will discuss the importance of addressing missing data, traditional methods for handling it, and modern techniques like Maximum Likelihood and Multiple Imputation.\nI will also attempt to provide a demo using a toy dataset to illustrate these concepts.\n\nLet’s get started"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#the-problem-of-missing-data",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#the-problem-of-missing-data",
    "title": "Missing Data",
    "section": "The Problem of Missing Data",
    "text": "The Problem of Missing Data\n\nMissing data is common in quantitative research\nTraditional methods (e.g., deletion, mean imputation) are often inadequate\n\nBiased estimates\nReduced statistical power\n\n\n\n\n\n\nMissing data are ubiquitous in quantitative research studies.\nBecause of its pervasive nature, some methodologists have described missing data as “one of the most important statistical and design problems in research” (methodologist William Shadish, quoted in Azar, 2002, p. 70)."
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#different-types-of-missing-data",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#different-types-of-missing-data",
    "title": "Missing Data",
    "section": "Different Types of Missing Data",
    "text": "Different Types of Missing Data\n\nMissing Completely at Random (MCAR): Missingness is unrelated to observed or unobserved data\nMissing at Random (MAR): Missingness is related to observed data but not unobserved data\nMissing Not at Random (MNAR): Missingness is related to unobserved data\n\n\n\nMCAR (Missing Completely at Random): - Missingness unrelated to any data. - E.g., a student moves districts for unrelated reasons. - Analyses remain unbiased—but true MCAR is rare.\nMAR (Missing at Random): - Missingness depends only on other observed variables. - E.g., students who use substances more often skip school—and thus a self‑esteem survey. - Can be handled well with multiple imputation or maximum likelihood.\nMNAR (Missing Not at Random): - Missingness depends on the unobserved value itself. - E.g., poor readers skip hard test items because they can’t answer them. - Statistically challenging, since the missingness conveys hidden information."
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#building-and-deploying-example",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#building-and-deploying-example",
    "title": "Missing Data",
    "section": "Building and deploying example",
    "text": "Building and deploying example\n\nCreate Toy DatasetVisualize Missing Data\n\n\nFirst, let’s create our toy dataset\n\nset.seed(42)\nn &lt;- 1000  # Number of observations\ntoy &lt;- data.frame(\n  x = rnorm(n, mean = 0, sd = 5),  \n  y = rnorm(n, mean = 0, sd = 7),  \n  z = sample(0:3, n, replace = TRUE)\n)\n\n# Introduce missingness in `y`\ntoy$y[sample(1:n, 300)] &lt;- NA  # 300 missing values randomly assigned to `y`\n\n\n\n\nvisualize with gg_miss_var from the naniar package\nas you can see we have missing data in the y variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we’ll create our synthetic dataset using hte code provided.\nThe dataset contains three variables: x, y, and z where y has ~300 missing values.\n\nWhat are some ways in which we can deal with them?"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#traditional-missing-data-techniques",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#traditional-missing-data-techniques",
    "title": "Missing Data",
    "section": "Traditional Missing Data Techniques",
    "text": "Traditional Missing Data Techniques\n\nListwise Deletion: Drops any row with NA (e.g., na.rm = T)\n\nMean Imputation: Replaces missing values with the mean value\n\nThese are easy to use, but often leads to biased results, let’s see why\n\n\nTraditionally, people use things like listwise deletion or mean imputation to deal with missing data.\nBut this can create problems because it’ll reduce power (if you remove observations from your dataset) and can also introduce bias (if you replace missing values with the mean). Let’s see how we’d implement this in R"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#how-to-implement-mean-imputation",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#how-to-implement-mean-imputation",
    "title": "Missing Data",
    "section": "How to implement Mean Imputation",
    "text": "How to implement Mean Imputation\n\nHere, we are asking R to impute the mean of y and replace any missing values with that mean\n\n\n\n# Mean imputation\ntoy$y_mean &lt;- ifelse(is.na(toy$y), mean(toy$y, na.rm = TRUE), toy$y)"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#visualizing-mean-imputation",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#visualizing-mean-imputation",
    "title": "Missing Data",
    "section": "Visualizing Mean Imputation",
    "text": "Visualizing Mean Imputation\n\n\n\nVisualize the original data and the imputed data to compare the distributions\nNotice the thick line at the mean"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#how-does-this-influence-downstream-processes",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#how-does-this-influence-downstream-processes",
    "title": "Missing Data",
    "section": "How does this influence downstream processes?",
    "text": "How does this influence downstream processes?\n\n\n\nNow, let’s see how this affects our regression model.\nThe difference seems neligible (at least with working with big data) but the mean imputation is biased\nIn smaller datasets mean imputation can have larger downstream consequences"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#modern-method-multiple-imputation-mi",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#modern-method-multiple-imputation-mi",
    "title": "Missing Data",
    "section": "Modern Method: Multiple Imputation (MI)",
    "text": "Modern Method: Multiple Imputation (MI)\n\nImputes multiple plausible values\n\nModels missingness using relationships among variables\n\nPools results for accurate estimates and standard errors\n\nMore robust than traditional methods"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#multiple-imputation-with-mice-package",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#multiple-imputation-with-mice-package",
    "title": "Missing Data",
    "section": "Multiple Imputation with MICE Package",
    "text": "Multiple Imputation with MICE Package\n\nMice stands for Multiple Imputation with Chained Equations\n\n\nCodeExplanation\n\n\n\n# Multiple imputation\nimp &lt;- mice(toy[, c(\"x\", \"y\", \"z\")], m = 5, method = \"pmm\", seed = 42,printFlag = F)\nfit_mi &lt;- with(imp, lm(y ~ x + z))\npooled_summary = summary(pool(fit_mi))\n\npooled_summary |&gt; \n  as.data.frame() |&gt;\n  mutate_if(is.numeric, round, 3)  |&gt; \n  DT::datatable(options = list(pageLength = 10, autoWidth = TRUE), \n                rownames = FALSE)\n\n\n\n\n\n\n\nWhat is this code doing?\n\nStep 1: Use the mice() function to create 5 imputed datasets for the variables x, y, and z\nStep 2: Fit a linear model (y ~ x + z) on each imputed dataset using xwith().\nStep 3: Combine the results across all models using pool() to account for variability between imputations"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#plotting-imputed-datasets-produced-from-mice",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#plotting-imputed-datasets-produced-from-mice",
    "title": "Missing Data",
    "section": "Plotting Imputed Datasets produced from MICE",
    "text": "Plotting Imputed Datasets produced from MICE"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#modern-method-maximum-likelihood-ml",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#modern-method-maximum-likelihood-ml",
    "title": "Missing Data",
    "section": "Modern Method: Maximum Likelihood (ML)",
    "text": "Modern Method: Maximum Likelihood (ML)\n\nMLE doesn’t fill in missing values.\nInstead, it finds the parameter values (e.g., mean, regression coefficients) that make the observed data most probable.\nBased on the log likelihood function – it chooses parameters that minimize the distance between the model and the data.\n\nRequires:\n\nAssumes MAR\nAssumes multivariate normality\nThe under the hood math\n\n\\[\n\\begin{align}\n\\log L &= \\sum_{i=1}^{N} \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{1}{2} \\left( \\frac{y_i - \\mu}{\\sigma} \\right)^2 \\right) \\right)\n\\end{align}\n\\]\n\nAssumes multivariate normality and MAR like multiple imputation, but instead of imputing missing values, MLE identifies the parameter values that maximize the log‑likelihood across all available data—complete and incomplete. Conceptually akin to OLS’s minimization of residuals, it uses the log‑likelihood function to select the parameters most likely to have generated the observed sample.\nMLE asks, “Which parameter values would make the data we actually observed most probable?”—using whatever data we have, without guessing missing entries."
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#using-auxiliary-variables",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#using-auxiliary-variables",
    "title": "Missing Data",
    "section": "Using Auxiliary Variables",
    "text": "Using Auxiliary Variables\n\nAuxiliary variables can help improve imputation\n\nThese are variables that are not of primary interest but can help explain the missingness\n\nImproves imputation quality and reduces bias"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#implementation-example",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#implementation-example",
    "title": "Missing Data",
    "section": "Implementation Example",
    "text": "Implementation Example\n\n# Add auxiliary variable\ntoy$aux &lt;- toy$x + rnorm(n)\ntoy$y[sample(1:n, 300)] &lt;- NA  # More missingness\n\nimp_aux &lt;- mice(toy[, c(\"x\", \"y\", \"z\", \"aux\")], m = 5, method = \"pmm\", seed = 42,printFlag = F)\nfit_aux &lt;- with(imp_aux, lm(y ~ x + z + aux))\npooled_summary = summary(pool(fit_aux))\npooled_summary |&gt; \n  as.data.frame() |&gt;\n  mutate_if(is.numeric, round, 3)  |&gt; \n  DT::datatable(options = list(pageLength = 10, autoWidth = TRUE), \n                rownames = FALSE)"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#plotting-imputed-datasets-produced-from-mice-with-auxiliary-variable",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#plotting-imputed-datasets-produced-from-mice-with-auxiliary-variable",
    "title": "Missing Data",
    "section": "Plotting Imputed Datasets produced from MICE with auxiliary variable",
    "text": "Plotting Imputed Datasets produced from MICE with auxiliary variable\n\n\nHere we are testing whether adding the auxiliary variable aux improves your model’s estimation of y, especially given the extra missingness. Including helpful predictors like aux can improve the quality of imputations and regression estimates. To do that we compare stuff like fit stats between the two models to see which produces the better fit."
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#summary-of-key-takeaways",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#summary-of-key-takeaways",
    "title": "Missing Data",
    "section": "Summary of Key Takeaways",
    "text": "Summary of Key Takeaways\n\n\n\nMethod\nBias\nVariability\nEase of Use\n\n\n\n\nListwise\n❌ High\n❌ Reduced\n✅ Easy\n\n\nMean Imputation\n❌ High\n❌ Too Low\n✅ Easy\n\n\nML\n✅ Low\n✅ Accurate\n⚠ Intermediate\n\n\nMI\n✅ Low\n✅ Accurate\n⚠️ Intermediate"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#conclusion",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#conclusion",
    "title": "Missing Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nTraditional methods can lead to biased results\n\nModern techniques (ML, MI) use all available data\n\nBetter estimates, standard errors, and power\n\nUse tools like mice and naniar for effective handling of missing data"
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#package-citations",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#package-citations",
    "title": "Missing Data",
    "section": "Package Citations",
    "text": "Package Citations\n\n\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rinker TW, Kurkiewicz D (2018). _pacman: Package Management for R_. version 0.5.0, &lt;http://github.com/trinker/pacman&gt;.\n  - Tiedemann F (2022). _gghalves: Compose Half-Half Plots Using Your Favourite Geoms_. R package version 0.1.4, &lt;https://CRAN.R-project.org/package=gghalves&gt;.\n  - Tierney N, Cook D (2023). \"Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\" _Journal of Statistical Software_, *105*(7), 1-31. doi:10.18637/jss.v105.i07 &lt;https://doi.org/10.18637/jss.v105.i07&gt;.\n  - van Buuren S, Groothuis-Oudshoorn K (2011). \"mice: Multivariate Imputation by Chained Equations in R.\" _Journal of Statistical Software_, *45*(3), 1-67. doi:10.18637/jss.v045.i03 &lt;https://doi.org/10.18637/jss.v045.i03&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, Bryan J, Barrett M, Teucher A (2024). _usethis: Automate Package and Project Setup_. R package version 3.0.0, &lt;https://CRAN.R-project.org/package=usethis&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Hester J, Chang W, Bryan J (2022). _devtools: Tools to Make Developing R Packages Easier_. R package version 2.4.5, &lt;https://CRAN.R-project.org/package=devtools&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;."
  },
  {
    "objectID": "posts/Missing-data-presentation/Missing-data-pres.html#thanks-for-listening",
    "href": "posts/Missing-data-presentation/Missing-data-pres.html#thanks-for-listening",
    "title": "Missing Data",
    "section": "Thanks for listening!",
    "text": "Thanks for listening!"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html",
    "href": "posts/Lab-8/Bayes_Lab_1.html",
    "title": "Lab-8: Bayes",
    "section": "",
    "text": "Here is a worksheet and assignment that combines Bayes (brms) with tidyverse tools. The focus is on the essentials when it comes to simple linear regression with brms.\nPlease read and run through this worksheet and answer the conceptual questions that are interleaved within them. At the end of each part, is a coding exercise based on the material you’ve read until then."
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#packages-and-data",
    "href": "posts/Lab-8/Bayes_Lab_1.html#packages-and-data",
    "title": "Lab-8: Bayes",
    "section": "Packages and data",
    "text": "Packages and data\nLoad the primary packages.\n\nlibrary(pacman)\np_load(tidyverse, ggside, brms, broom, broom.mixed,install = T)\n\noptions(scipen=999) # get rid of sci notation\n\nsetwd(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts\")\n### create plot aesthetics \npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nWe’ll use the penguins data set from the palmerpenguins package.\n\ndata(penguins, package = \"palmerpenguins\")\n\n# Any type of looking at data is a part of EDA \nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nYou might divide the data set by the three levels of species.\n\npenguins %&gt;% \n  count(species) |&gt; \n  DT::datatable()\n\n\n\n\n\nTo start, we’ll make a subset of the data called chinstrap.\n\nchinstrap &lt;- penguins %&gt;% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n\nRows: 68\nColumns: 8\n$ species           &lt;fct&gt; Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            &lt;fct&gt; Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    &lt;dbl&gt; 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     &lt;dbl&gt; 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm &lt;int&gt; 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       &lt;int&gt; 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               &lt;fct&gt; female, male, male, female, male, female, female, ma…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nWe’ve done from a full data set with \\(N = 344\\) rows, to a subset with \\(n = 68\\) rows. (“$” signs hold LaTex snippets)"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#more-exploratory-data-analysis-eda",
    "href": "posts/Lab-8/Bayes_Lab_1.html#more-exploratory-data-analysis-eda",
    "title": "Lab-8: Bayes",
    "section": "More Exploratory data analysis (EDA)",
    "text": "More Exploratory data analysis (EDA)\nOur focal variables will be body_mass_g and bill_length_mm. Here they are in a scatter plot.\n\nchinstrap %&gt;% \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) + plot_aes\n\n\n\n\n\n\n\n\nWe can augment the plot with some nice functions from the ggside package.\n\nchinstrap %&gt;% \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) +\n  # from ggside\n  geom_xsidehistogram(bins = 30) +\n  geom_ysidehistogram(bins = 30) +\n  scale_xsidey_continuous(breaks = NULL) +\n  scale_ysidex_continuous(breaks = NULL) +\n  theme(ggside.panel.scale = 0.25) + plot_aes\n\n\n\n\n\n\n\n\nIt’s a good idea to get a sense of the sample statistics. Here are the means and SD’s for the two variables.\n\nchinstrap %&gt;% \n  summarise(body_mass_g_mean = mean(body_mass_g),\n            body_mass_g_sd = sd(body_mass_g),\n            bill_length_mm_mean = mean(bill_length_mm),\n            bill_length_mm_sd = sd(bill_length_mm)) |&gt;\n  pivot_longer(cols = everything()) |&gt;\n  mutate(name = str_replace(name, \"_\", \" \"),\n         value = round(value, digits = 3)) |&gt;\n  DT::datatable()\n\n\n\n\n\nAnd you know that more efficient way to compute sample statistics for multiple variables is to first convert the data into the long format with pivot_longer(). Then you use a group_by() line before the main event in summarise().\n\nchinstrap %&gt;% \n  pivot_longer(cols = c(body_mass_g, bill_length_mm)) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            sd = sd(value),\n            # count the missing data (if any)\n            n_missing = sum(is.na(value))) |&gt; \n  mutate(across(c(mean, sd), ~ round(., digits = 3)) ) |&gt;\n  DT::datatable()\n\n\n\n\n\n\nQuestion 1.1: What do the marginal histograms added by ggside tell you about the distribution of body_mass_g and bill_length_mm individually?\n\nThey tell us that hte body_mass_g is right skewed and bill_length_mm is left skewed. But its important to note that these inferences are descriptive since we don’t have skewness and kurotsis stats."
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#ols",
    "href": "posts/Lab-8/Bayes_Lab_1.html#ols",
    "title": "Lab-8: Bayes",
    "section": "OLS",
    "text": "OLS\nWe’ll fit the model\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon)\n\\end{align}\n\\]\nwhere bill_length_mm is the dependent variable or a response variable. The sole predictor is body_mass_g. Both variables have \\(i\\) subscripts, which indicate they vary across the \\(i\\) rows in the data set. For now, you might think if \\(i\\) as standing for “index.” The last term in the first line, \\(\\epsilon\\), is often called the error, or noise term. In the second line, we see we’re making the conventional assumption the “errors” are normally distributed around the regression line.\nAn alternative and equivalent way to write that equation is\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n\\]\nwhich is meant to convey we are modeling bill_length_mm as normally distributed, with a conditional mean. You don’t tend to see equations written this way in the OLS paradigm. However, this style of notation will serve us better when we start modeling our data with other distributions.\nThis notation grows on you\nFitting the model with the base R lm() function, which uses the OLS algorithm.\n\n# fit\nfit1.ols &lt;- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# summarize the results\nsummary(fit1.ols)\n\n\nCall:\nlm(formula = bill_length_mm ~ 1 + body_mass_g, data = chinstrap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8399 -2.2370  0.3247  1.8385  9.3138 \n\nCoefficients:\n              Estimate Std. Error t value          Pr(&gt;|t|)    \n(Intercept) 32.1741929  3.4433623   9.344 0.000000000000107 ***\nbody_mass_g  0.0044627  0.0009176   4.863 0.000007480491992 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.887 on 66 degrees of freedom\nMultiple R-squared:  0.2638,    Adjusted R-squared:  0.2527 \nF-statistic: 23.65 on 1 and 66 DF,  p-value: 0.00000748\n\n\nThe point estimates are in scientific notation. We can pull them with the coef() function.\n\ncoef(fit1.ols) \n\n (Intercept)  body_mass_g \n32.174192865  0.004462694 \n\n\nWe can compute fitted values, or predictions, with the predict() function. Here’s the default behavior. Modifying to just get the top 6 to keep this concise.\n\npredict(fit1.ols) |&gt; \n  head()\n\n       1        2        3        4        5        6 \n47.79362 49.57870 48.46303 47.90519 48.79773 49.80183 \n\n\nWe get one prediction, one fitted value, for each case in the data set. We can express the uncertainty around those predictions with confidence intervals.\n\npredict(fit1.ols,\n        interval = \"confidence\") %&gt;% \n  head() |&gt; \n  DT::datatable()\n\n\n\n\n\nWe might also ask for a standard error for each prediction.\n\npredict(fit1.ols,\n        se.fit = TRUE) %&gt;% \n  data.frame()\n\n        fit    se.fit df residual.scale\n1  47.79362 0.4102359 66       2.886728\n2  49.57870 0.3821060 66       2.886728\n3  48.46303 0.3582736 66       2.886728\n4  47.90519 0.3987564 66       2.886728\n5  48.79773 0.3501459 66       2.886728\n6  49.80183 0.4026961 66       2.886728\n7  46.67795 0.5648454 66       2.886728\n8  48.90930 0.3504110 66       2.886728\n9  50.69437 0.5185569 66       2.886728\n10 48.68616 0.3513814 66       2.886728\n11 49.13243 0.3554108 66       2.886728\n12 49.02086 0.3521734 66       2.886728\n13 48.68616 0.3513814 66       2.886728\n14 50.24810 0.4550963 66       2.886728\n15 48.12832 0.3789333 66       2.886728\n16 50.24810 0.4550963 66       2.886728\n17 46.90108 0.5296025 66       2.886728\n18 48.68616 0.3513814 66       2.886728\n19 47.57049 0.4359183 66       2.886728\n20 51.81005 0.7050167 66       2.886728\n21 48.23989 0.3707575 66       2.886728\n22 47.34735 0.4647215 66       2.886728\n23 45.11601 0.8407923 66       2.886728\n24 49.13243 0.3554108 66       2.886728\n25 46.90108 0.5296025 66       2.886728\n26 50.69437 0.5185569 66       2.886728\n27 47.34735 0.4647215 66       2.886728\n28 49.13243 0.3554108 66       2.886728\n29 48.68616 0.3513814 66       2.886728\n30 52.47945 0.8273195 66       2.886728\n31 46.45481 0.6015246 66       2.886728\n32 51.36378 0.6270243 66       2.886728\n33 47.12422 0.4961023 66       2.886728\n34 50.47124 0.4856973 66       2.886728\n35 48.23989 0.3707575 66       2.886728\n36 49.57870 0.3821060 66       2.886728\n37 49.35556 0.3661365 66       2.886728\n38 53.59512 1.0397147 66       2.886728\n39 44.22347 1.0105441 66       2.886728\n40 52.25632 0.7859885 66       2.886728\n41 49.80183 0.4026961 66       2.886728\n42 48.46303 0.3582736 66       2.886728\n43 48.01676 0.3882941 66       2.886728\n44 47.79362 0.4102359 66       2.886728\n45 48.57459 0.3541019 66       2.886728\n46 52.03318 0.7451900 66       2.886728\n47 47.34735 0.4647215 66       2.886728\n48 51.36378 0.6270243 66       2.886728\n49 46.67795 0.5648454 66       2.886728\n50 48.57459 0.3541019 66       2.886728\n51 47.01265 0.5126128 66       2.886728\n52 49.80183 0.4026961 66       2.886728\n53 48.23989 0.3707575 66       2.886728\n54 50.24810 0.4550963 66       2.886728\n55 47.12422 0.4961023 66       2.886728\n56 47.57049 0.4359183 66       2.886728\n57 46.67795 0.5648454 66       2.886728\n58 50.24810 0.4550963 66       2.886728\n59 49.13243 0.3554108 66       2.886728\n60 47.90519 0.3987564 66       2.886728\n61 49.80183 0.4026961 66       2.886728\n62 48.46303 0.3582736 66       2.886728\n63 48.46303 0.3582736 66       2.886728\n64 50.02497 0.4272392 66       2.886728\n65 47.34735 0.4647215 66       2.886728\n66 49.02086 0.3521734 66       2.886728\n67 50.47124 0.4856973 66       2.886728\n68 49.02086 0.3521734 66       2.886728\n\n\nInstead of relying on predictions from the values in the data, we might instead define a sequence of values from the predictor variable. We’ll call those nd.\n\nnd &lt;- tibble(body_mass_g = seq(from = min(chinstrap$body_mass_g),\n                               to = max(chinstrap$body_mass_g),\n                               length.out = 50))\n\nglimpse(nd)\n\nRows: 50\nColumns: 1\n$ body_mass_g &lt;dbl&gt; 2700.000, 2742.857, 2785.714, 2828.571, 2871.429, 2914.286…\n\n\nWe can insert our nd data into the newdata argument. Here we are using the model to predict new data\n\npredict(fit1.ols,\n        interval = \"confidence\",\n        newdata = nd) %&gt;% \n  # just the top 6\n  head()\n\n       fit      lwr      upr\n1 44.22347 42.20585 46.24108\n2 44.41473 42.47057 46.35888\n3 44.60598 42.73489 46.47708\n4 44.79724 42.99874 46.59574\n5 44.98850 43.26207 46.71493\n6 45.17976 43.52482 46.83469\n\n\nNow we wrangle those predictions a bit and pump the results right into ggplot().\n\npredict(fit1.ols,\n        interval = \"confidence\",\n        newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% confidence interval ribbon\n  geom_ribbon(aes(ymin = lwr, ymax = upr),\n              alpha = 0.2) +\n  # point estimate line\n  geom_line(aes(y = fit)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nIf we wanted to, we could look at the residuals with help from the residuals() function.\n\nresiduals(fit1.ols) |&gt; \n  head()\n\n         1          2          3          4          5          6 \n-1.2936220  0.4213003  2.8369738 -2.5051894  3.9022718 -4.6018344 \n\n\nHere we might put them in a tibble and display them in a plot.\n\n# put them in a tibble\ntibble(r = residuals(fit1.ols)) %&gt;% \n  ggplot(aes(x = r)) +\n  geom_histogram(binwidth = 1) + plot_aes\n\n\n\n\n\n\n\n\n\nQuestion 1.2: Can you predict what the mean value, and standard deviations will be? Why? Calculate it. Compare this against outputs in summary(fit1.ols) and explain. Map the values you find to the latex equations before.\n\nWe can predict that the mean value will be 0 and the standard deviation will be 1. This is because the residuals are the difference between the observed and predicted values."
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#bayes-with-default-settings",
    "href": "posts/Lab-8/Bayes_Lab_1.html#bayes-with-default-settings",
    "title": "Lab-8: Bayes",
    "section": "Bayes with default settings",
    "text": "Bayes with default settings\nWe’ll be fitting our Bayesian models with the brms package. The primary function is brm().\nbrm() can work a lot like the OLS-based lm() function. For example, here’s how to fit a Bayesian version of our OLS model fit1.ols.\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-8/models/fit1.rds\")\n\nif (!file.exists(model_path)) {\n fit1.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit1.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit1.b &lt;- readRDS(model_path)\n}\n\nNotice what’s happening in the console, below. We’ll get into the details of what just happened later. For now, appreciate we just fit our first Bayesian model, and it wasn’t all that hard.\nSummarize the model.\n\nsummary(fit1.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.17      3.46    25.20    38.77 1.00     5027     2952\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5128     2545\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.92      0.27     2.45     3.51 1.00     1860     1673\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nQuestion 1.3: Contrast the language of in the brm() output from the in the lm() output. Ignore ‘Rhat,’ ‘Bulk_ESS,’ and ‘Tail_ESS’ for now.\nWe can get a quick and dirty plot of the fitted line with the conditional_effects() function.\n\nconditional_effects(fit1.b) \n\n\n\n\n\n\n\n# %&gt;% \n#   plot(points = TRUE)"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#coefficients-and-coefficient-plots",
    "href": "posts/Lab-8/Bayes_Lab_1.html#coefficients-and-coefficient-plots",
    "title": "Lab-8: Bayes",
    "section": "Coefficients and coefficient plots",
    "text": "Coefficients and coefficient plots\nWe might want to compare the coefficient summaries from the OLS model to those from the Bayesian model. Here’s the frequentist summary:\n\ncbind(coef(fit1.ols),              # point estimates\n      sqrt(diag(vcov(fit1.ols))),  # standard errors\n      confint(fit1.ols))           # 95% CIs\n\n                                             2.5 %       97.5 %\n(Intercept) 32.174192865 3.4433622902 25.299298235 39.049087495\nbody_mass_g  0.004462694 0.0009176106  0.002630625  0.006294763\n\n\nWe can compute a focused summary of the Bayesian model with the fixef() function.\n\nfixef(fit1.b)\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.171601696 3.4599647320 25.197783898 38.767885598\nbody_mass_g  0.004464182 0.0009227771  0.002668377  0.006304322\n\n\nIn this case, the results are very similar.\nWe can also pull this information from our OLS model with the broom::tidy() function.\n\ntidy(fit1.ols, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 32.2      3.44          9.34 1.07e-13 25.3      39.0    \n2 body_mass_g  0.00446  0.000918      4.86 7.48e- 6  0.00263   0.00629\n\n\nIf you would like to use the tidy() function with your brms models, it will have to be the version of tidy() from the broom.mixed package.\n\ntidy(fit1.b)\n\n# A tibble: 3 × 8\n  effect   component group    term         estimate std.error conf.low conf.high\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    cond      &lt;NA&gt;     (Intercept)  32.2      3.46     25.2      38.8    \n2 fixed    cond      &lt;NA&gt;     body_mass_g   0.00446  0.000923  0.00267   0.00630\n3 ran_pars cond      Residual sd__Observa…  2.92     0.266     2.45      3.51   \n\n\nHere’s how to wrangle and combine these two results into a single data frame. Then we’ll make a coefficient plot.\n\nbind_rows(\n  tidy(fit1.ols, conf.int = TRUE) %&gt;% select(term, estimate, contains(\"conf\")),\n  tidy(fit1.b) %&gt;% select(term, estimate, contains(\"conf\")) %&gt;% filter(term != \"sd__Observation\")\n) %&gt;% \n  mutate(method = rep(c(\"lm()\", \"brm()\"), each = 2)) %&gt;% \n  \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = method)) +\n  geom_pointrange() +\n  scale_x_continuous(\"parameter space\", expand = expansion(mult = 0.2)) +\n  scale_y_discrete(expand = expansion(mult = 5)) +\n  facet_wrap(~ term, scales = \"free_x\") + plot_aes\n\n\n\n\n\n\n\n\nAt a superficial level for simple conventional regression type models, the results from a Bayesian brm() model will be very similar to those from an OLS lm() model. This will not always be case, and even in this example there are many differences once we look below the surface."
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#more-questionsexercise",
    "href": "posts/Lab-8/Bayes_Lab_1.html#more-questionsexercise",
    "title": "Lab-8: Bayes",
    "section": "More Questions/Exercise",
    "text": "More Questions/Exercise\nGo back to the full penguins data set. This time, make a subset of the data called gentoo, which is only the cases for which species == \"Gentoo\".\n\ngentoo &lt;- penguins %&gt;% \n  filter(species == \"Gentoo\")\n\ngentoo |&gt; \n  head() |&gt; \n  DT::datatable()\n\n\n\n\n\nCan you fit the same OLS model to these data?\n\ngentoo_ols = lm( bill_length_mm ~ 1 + body_mass_g, data = gentoo)\nsummary(gentoo_ols)\n\n\nCall:\nlm(formula = bill_length_mm ~ 1 + body_mass_g, data = gentoo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8802 -1.5075 -0.0575  1.3118  8.1107 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 26.739549   2.106594  12.693 &lt;0.0000000000000002 ***\nbody_mass_g  0.004091   0.000413   9.905 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 121 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4478,    Adjusted R-squared:  0.4432 \nF-statistic: 98.12 on 1 and 121 DF,  p-value: &lt; 0.00000000000000022\n\n\nHow about plotting the results with predict()?\n\npredict(gentoo_ols,\n        interval = \"confidence\") %&gt;% \n  head() |&gt; \n  DT::datatable()\n\n\n\n\n### create new data \n\nnd_2 &lt;- tibble(body_mass_g = seq(from = min(gentoo$body_mass_g,na.rm = TRUE),\n                               to = max(gentoo$body_mass_g,na.rm = TRUE),\n                               length.out = 1e6))\n\n\n\npredict(gentoo_ols,\n        interval = \"confidence\",\n        newdata = nd_2) %&gt;% \n  bind_cols(nd_2) %&gt;% \n  data.frame() |&gt; \n  ggplot(aes(x = body_mass_g)) +  # Ensure that body_mass_g is available here\n  geom_ribbon(aes(ymin = lwr, ymax = upr),\n              alpha = 0.2) +  \n  # point estimate line\n  geom_line(aes(y = fit)) +\n  # Add points for the original data\n  geom_point(data = gentoo, aes(x = body_mass_g, y = bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nCan you fit the same default Bayesian brm() model to these data?\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-8/models/fit2.rds\")\n\nif (!file.exists(model_path)) {\n fit2 &lt;- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit1.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit2 &lt;- readRDS(model_path)\n}\n\nsummary(fit2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: gentoo (Number of observations: 123) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      26.77      2.14    22.65    31.06 1.00     4282     2507\nbody_mass_g     0.00      0.00     0.00     0.00 1.00     4325     2620\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.32      0.15     2.06     2.64 1.01     1766     1628\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHow about plotting the results with conditional_effects()?\n\nconditional_effects(fit2) \n\n\n\n\n\n\n\n\nCan you make a coefficient plot comparing the new OLS and Bayesian beta coefficients?\n\nbind_rows(\n  tidy(gentoo_ols, conf.int = TRUE) %&gt;% select(term, estimate, contains(\"conf\")),\n  tidy(fit2) %&gt;% select(term, estimate, contains(\"conf\")) %&gt;% filter(term != \"sd__Observation\")\n) %&gt;% \n  mutate(method = rep(c(\"lm()\", \"brm()\"), each = 2)) %&gt;% \n  \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = method)) +\n  geom_pointrange() +\n  scale_x_continuous(\"parameter space\", expand = expansion(mult = 0.2)) +\n  scale_fill_manual(values = palette) +\n  scale_y_discrete(expand = expansion(mult = 5)) +\n  facet_wrap(~ term, scales = \"free_x\") + plot_aes"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#exploring-model-results",
    "href": "posts/Lab-8/Bayes_Lab_1.html#exploring-model-results",
    "title": "Lab-8: Bayes",
    "section": "Exploring model results",
    "text": "Exploring model results\nWe can extract the posterior draws from our Bayesian models with the as_draws_df() function.\n\nas_draws_df(fit1.b)\n\n# A draws_df: 1000 iterations, 4 chains, and 6 variables\n   b_Intercept b_body_mass_g sigma Intercept lprior lp__\n1           38        0.0030   2.8        49   -4.2 -172\n2           39        0.0025   2.6        49   -4.2 -175\n3           34        0.0039   2.6        49   -4.2 -172\n4           31        0.0046   2.7        49   -4.3 -172\n5           28        0.0054   2.7        49   -4.3 -172\n6           34        0.0040   3.0        49   -4.3 -171\n7           35        0.0038   3.3        49   -4.4 -172\n8           29        0.0052   2.7        48   -4.3 -172\n9           35        0.0038   2.9        49   -4.3 -171\n10          37        0.0032   2.8        49   -4.3 -172\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nNote the meta data. We can get a sense of the full posterior distributions of the \\(\\beta\\) parameters with plots.\n\n# wrangle\nas_draws_df(fit1.b) %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = value)) + \n  # geom_density(fill = \"grey20\") +\n  geom_histogram(bins = 40) +\n  facet_wrap(~ name, scales = \"free\") + plot_aes\n\n\n\n\n\n\n\n\nWe might summarize those posterior distributions with basic descriptive statistics, like their means, SD’s, and inner 95-percentile range.\n\nas_draws_df(fit1.b) %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            sd = sd(value),\n            ll = quantile(value, probs = 0.025),\n            ul = quantile(value, probs = 0.975)) |&gt; \n  DT::datatable()\n\n\n\n\n\nNotice how these values match up exactly with those from fixef().\n\nfixef(fit1.b)\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.171601696 3.4599647320 25.197783898 38.767885598\nbody_mass_g  0.004464182 0.0009227771  0.002668377  0.006304322\n\n\nThus,\n\nThe Bayesian posterior mean is analogous to the frequentist point estimate.\nThe Bayesian posterior SD is analogous to the frequentist standard error.\nThe Bayesian posterior percentile-based 95% (credible) interval is analogous to the frequentist 95% confidence interval.\n\nThese are not exactly the same, mind you. But they serve similar functions.\nWe can also get a sense of these distributions with the plot() function.\n\nplot(fit1.b)\n\n\n\n\n\n\n\n\nIgnore the trace plots on the right for a moment. And let’s consider the pairs() plot.\n\npairs(fit1.b)\n\n\n\n\n\n\n\n# we can adjust some of the settings with the off_diag_args argument\npairs(fit1.b, off_diag_args = list(size = 1/4, alpha = 1/4))\n\n\n\n\n\n\n\n\n\nQuestion 2.1 : In the parlance of Probability, do you know what is the term by which the distributions in the diagonal of the above plot are known as? And the distributions in the off-diagonal?\n\nThe distributions in the diagonal are known as the marginal distributions and the distributions in the off-diagonal are known as the joint distributions.\n\nNotice how the two \\(\\beta\\) parameters seem to have a strong negative correlation. We can quantify that correlation with the vcov() function.\n\nvcov(fit1.b)                      # variance/covariance metric\n\n               Intercept      body_mass_g\nIntercept   11.971355947 -0.0031762934154\nbody_mass_g -0.003176293  0.0000008515175\n\nvcov(fit1.b, correlation = TRUE)  # correlation metric\n\n             Intercept body_mass_g\nIntercept    1.0000000  -0.9948375\nbody_mass_g -0.9948375   1.0000000\n\n\nThis correlation/covariance among the parameters is not unique to Bayesian models. Here’s the vcov() output for the OLS model.\n\nvcov(fit1.ols)  # variance/covariance metric\n\n             (Intercept)      body_mass_g\n(Intercept) 11.856743861 -0.0031432947972\nbody_mass_g -0.003143295  0.0000008420092\n\n\nI’m not aware of an easy way to get that output in a correlation metric for our OLS model. Here’s how to compute the correlation by hand.\n\ncov_xy &lt;- vcov(fit1.ols)[2, 1]  # covariance between the intercept and slope\nvar_x  &lt;- vcov(fit1.ols)[1, 1]  # variance for the intercept\nvar_y  &lt;- vcov(fit1.ols)[2, 2]  # variance for the slope\n\n# convert the covariance into a correlation\ncov_xy / (sqrt(var_x) * sqrt(var_y))\n\n[1] -0.9948188\n\n\nThat code follows the definition of a covariance, which can be expressed as\n\\[\n\\text{Cov}(x, y) = \\rho \\sigma_x \\sigma_y,\n\\]\nwhere \\(\\sigma_x\\) is the standard deviation for x, \\(\\sigma_y\\) is the standard deviation for y, and \\(\\rho\\) is their correlation. And thus, you can convert a covariance into a correlation with the formula\n\\[\n\\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y},\n\\]\nwhere \\(\\sigma_{xy}\\) is the covariance of x and y."
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#draws",
    "href": "posts/Lab-8/Bayes_Lab_1.html#draws",
    "title": "Lab-8: Bayes",
    "section": "Draws",
    "text": "Draws\nLet’s save the as_draws_df() output for our model as an object called draws.\n\ndraws &lt;- as_draws_df(fit1.b)\nglimpse(draws)\n\nRows: 4,000\nColumns: 9\n$ b_Intercept   &lt;dbl&gt; 37.92621, 39.29808, 34.15103, 31.39905, 28.47337, 34.143…\n$ b_body_mass_g &lt;dbl&gt; 0.002966134, 0.002510550, 0.003885012, 0.004581258, 0.00…\n$ sigma         &lt;dbl&gt; 2.761649, 2.573020, 2.613326, 2.694444, 2.727092, 3.0373…\n$ Intercept     &lt;dbl&gt; 48.99905, 48.67019, 48.65413, 48.50129, 48.67069, 48.982…\n$ lprior        &lt;dbl&gt; -4.241599, -4.220093, -4.230620, -4.263561, -4.255627, -…\n$ lp__          &lt;dbl&gt; -172.4217, -174.5516, -171.7488, -171.5667, -171.6560, -…\n$ .chain        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ .iteration    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ .draw         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n\n\nFor each parameter in the model, we have 4,000 draws from the posterior.\n\nQuestion 2.2: How does this concept relate to representing uncertainty? Can you anticipate how predictions are made based upon these 4000 draws and the linear regression formula?\n\nThe 4000 draws represent the uncertainty in the model. The predictions are made by using the 4000 draws to calculate the predicted values for each observation in the data set. The linear regression formula is used to calculate the predicted values for each observation in the data set.\n\n\\[\\widehat{\\text{bill_length_mm}}_i = \\beta_0 + \\beta_1 \\text{body_mass_g}_i.\\]\nLet’s break the 4000 draws down with our draws object.\n\n# adjust the parameter names \ndraws &lt;- draws %&gt;% \n  mutate(beta0 = b_Intercept,\n         beta1 = b_body_mass_g)\n\n# Note: go through this one line at a time\ndraws %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  geom_density()+ \n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51)) + plot_aes\n\n\n\n\n\n\n\n\nHere’s what that is for the OLS model.\n\npredict(fit1.ols,\n        newdata = tibble(body_mass_g = mean(chinstrap$body_mass_g)),\n        interval = \"confidence\") %&gt;% \n  data.frame() %&gt;% \n  \n  ggplot(aes(x = fit, xmin = lwr, xmax = upr, y = 0)) +\n  geom_pointrange() +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Frequentists have point estmates and 95% CI's\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51)) + plot_aes\n\n\n\n\n\n\n\n\nAnother handy way to present a Bayesian posterior is as a density with a point-interval summary below.\n\nlibrary(ggdist) #for stat_half_eye and mean_qi\ndraws %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  stat_halfeye(point_interval = mean_qi, .width = .95) +\n  # scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51)) + plot_aes\n\n\n\n\n\n\n\n\nThe dot at the base of the plot is the posterior mean, and the horizontal line marks the 95% percentile-based interval. If you’d like to mark the median instead, set point_interval = median_qi. If you’re like a different kind of horizontal interval, adjust the .width argument.\n\ndraws %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  # note the changes to this line\n  stat_halfeye(point_interval = median_qi, .width = c(.5, .99)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Bayesians have posterior distributions\",\n       subtitle = \"The dot marks the median.\\nThe thicker line marks the 50% interval, and\\nthe thinner line marks the 99% interval.\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51)) + plot_aes"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#about-those-means-sds-and-intervals.",
    "href": "posts/Lab-8/Bayes_Lab_1.html#about-those-means-sds-and-intervals.",
    "title": "Lab-8: Bayes",
    "section": "About those means, SD’s, and intervals.",
    "text": "About those means, SD’s, and intervals.\nYou can describe a Bayesian posterior in a lot of different ways. Earlier we said the posterior mean is the Bayesian point estimate. This isn’t strictly true. Means are very popular, but you can summarize a posterior by its mean, median, or mode.\nLet’s see what this looks like in practice. First, we compute and save our statistics for each of our model parameters.\n\npoints &lt;- draws %&gt;% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  group_by(parameter) %&gt;% \n  summarise(mean = mean(value),\n            median = median(value),\n            mode = Mode(value)) %&gt;% \n  pivot_longer(starts_with(\"m\"), names_to = \"statistic\")\n\n# what?\npoints\n\n# A tibble: 9 × 3\n  parameter statistic    value\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n1 beta[0]   mean      32.2    \n2 beta[0]   median    32.2    \n3 beta[0]   mode      32.4    \n4 beta[1]   mean       0.00446\n5 beta[1]   median     0.00446\n6 beta[1]   mode       0.00455\n7 sigma     mean       2.92   \n8 sigma     median     2.90   \n9 sigma     mode       2.85   \n\n\nNow plot.\n\ndraws %&gt;% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density() +\n  geom_vline(data = points,\n             aes(xintercept = value, color = statistic),\n             size = 3/4) +\n  scale_color_viridis_d(option = \"A\", end = .8) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, labeller = label_parsed, scales = \"free\", ncol = 1) +\n  theme(strip.text = element_text(size = 14)) + plot_aes\n\n\n\n\n\n\n\n\n\nQuestion 2.3: Discuss the skew in \\(\\sigma\\).Why it might arise, etc.?\n\nThe mean is the brms default summary, and McElreath (2015, 2020) defaulted to the mean in his texts.\nThe median is also available for many brms functions, and it’s what Gelman et al (2020) recommend.\nThe mode can be attractive for very skewed distributions, and it’s what Kruschke (2015) used in his text.\n\nWith many brms functions, you can request the median by setting robust = TRUE. For example:\n\nfixef(fit1.b)                 # means\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.171601696 3.4599647320 25.197783898 38.767885598\nbody_mass_g  0.004464182 0.0009227771  0.002668377  0.006304322\n\nfixef(fit1.b, robust = TRUE)  # medians\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.207289570 3.4510956069 25.197783898 38.767885598\nbody_mass_g  0.004459851 0.0009074036  0.002668377  0.006304322\n\n\n\n\nQuestion 2.4: Given the skew in sigma and what you know about summary statistics, what might be the implication of using just the mean, median, or mode of posteriors to make a prediction?\n\nSD’s and MAD SD’s.\nEarlier we said the posterior SD is the Bayesian standard error. This isn’t strictly true. You can also use the median absolute deviation (MAD SD). If we let \\(M\\) stand for the median of some variable \\(y\\), which varies across \\(i\\) cases, we can define the MAD SD as\n\\[\\textit{MAD SD} = 1.4826 \\times \\operatorname{median}_{i = 1}^n |y_i - M|,\\]\nwhere \\(1.4826\\) is a constant that scales the MAD SD into a standard-deviation metric. Here’s what this looks like in practice.\n\n# go through this line by line\ndraws %&gt;% \n  select(beta0) %&gt;% \n  mutate(mdn = median(beta0)) %&gt;% \n  mutate(`|yi - mdn|` = abs(beta0 - mdn)) %&gt;% \n  summarise(MAD_SD = 1.4826 * median(`|yi - mdn|`))\n\n# A tibble: 1 × 1\n  MAD_SD\n   &lt;dbl&gt;\n1   3.45\n\n\nBase R also has a mad() function.\n\n?mad\n\nHelp on topic 'mad' was found in the following packages:\n\n  Package               Library\n  posterior             /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n  stats                 /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n\n\nUsing the first match ...\n\ndraws %&gt;% \n  summarise(MAD_SD = mad(beta0))\n\n# A tibble: 1 × 1\n  MAD_SD\n   &lt;dbl&gt;\n1   3.45\n\n\nYou can request the MAD SD from many brms functions by setting robust = TRUE.\n\nfixef(fit1.b)                 # SD\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.171601696 3.4599647320 25.197783898 38.767885598\nbody_mass_g  0.004464182 0.0009227771  0.002668377  0.006304322\n\nfixef(fit1.b, robust = TRUE)  # MAD SD\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.207289570 3.4510956069 25.197783898 38.767885598\nbody_mass_g  0.004459851 0.0009074036  0.002668377  0.006304322\n\n\n\nTo my eye, many authors (e.g., Kruschke, McElreath) just use the SD.\nGelman et al (see Section 5.3) recommend the MAD SD.\n\n\n\nBayesian intervals.\nBayesians describe the widths of their posteriors with intervals. I’ve seen these variously described as confidence intervals, credible intervals, probability intervals, and even uncertainty intervals. My recommendation is just pick a term, and clearly tell your audience what you mean (e.g., at the end of a Method section in a journal article).\nTo my eye, the most popular interval is a 95% percentile-based interval. 95% is conventional, perhaps due to the popularity of the 95% frequentist confidence interval, which is related to the 0.05 alpha level used for the conventional \\(p\\)-value cutoff. However, you can use other percentiles. Some common alternatives are 99%, 89%, 80%, and 50%.\nAlso, Bayesian intervals aren’t always percentile based. An alternative is the highest posterior density interval (HPDI), which has mathematical properties some find desirable.\nbrms only supports percentile-based intervals, but it does allow for a variety of different ranges via the prob argument. For example, here’s how to request 80% intervals in summary().\n\nsummary(fit1.b, prob = .80)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.17      3.46    27.71    36.76 1.00     5027     2952\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5128     2545\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.92      0.27     2.59     3.26 1.00     1860     1673\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRegarding interval widths:\n\n95% Intervals are widely used.\nMcElreat likes 89% intervals, and uses them as a default in his rethinking package.\nSome of the bayesplot, ggdist, and tidybayes functions return 80% intervals.\nSome of the ggdist, and tidybayes functions return 66% or 50% intervals.\nI’ve heard Gelman report his fondness for 50% intervals on his blog (https://statmodeling.stat.columbia.edu/2016/11/05/why-i-prefer-50-to-95-intervals/).\n\nRegarding interval types:\n\nPercentile-based intervals are widely used in the Stan ecosystem, and are supported in texts like Gelman et al.\nKruschke has consistently advocates for HPDI’s in his articles, and in his text.\n\n\n\n\nPosterior summaries with tidybayes.\nMatthew Kay’s tidybayes package (https://mjskay.github.io/tidybayes/) offers an array of convenience functions for summarizing posterior distributions with points and intervals. See the Point summaries and intervals section of Kay’s Extracting and visualizing tidy draws from brms models vignette (https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals) for a detailed breakdown. In short, the family of functions use the naming scheme [median|mean|mode]_[qi|hdi]. Here are a few examples.\n\ndraws %&gt;% mean_qi(beta0)                        # mean and 95% percentile interval\n\n# A tibble: 1 × 6\n  beta0 .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1  32.2   25.2   38.8   0.95 mean   qi       \n\ndraws %&gt;% median_qi(beta0, .width = .80)        # median and 80% percentile interval\n\n# A tibble: 1 × 6\n  beta0 .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1  32.2   27.7   36.8    0.8 median qi       \n\ndraws %&gt;% mode_hdi(beta0, .width = c(.5, .95))  # mode, with 95 and 50% HPDI's\n\n# A tibble: 2 × 6\n  beta0 .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1  32.4   30.5   35.1   0.5  mode   hdi      \n2  32.4   25.1   38.6   0.95 mode   hdi      \n\n\nAs an aside, the Mode() function we used a while back was also from tidybayes.\n\n\nSpaghetti plots.\nRemember how we said the draw was something like 4,000 separate equations for our Bayesian model? Let’s see that again.\n\ndraws %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  # here's the equation\n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  # subset the top 6\n  head()\n\n# A tibble: 6 × 5\n  .draw beta0   beta1 body_mass_g y_hat\n  &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     1  37.9 0.00297       3733.  49.0\n2     2  39.3 0.00251       3733.  48.7\n3     3  34.2 0.00389       3733.  48.7\n4     4  31.4 0.00458       3733.  48.5\n5     5  28.5 0.00541       3733.  48.7\n6     6  34.1 0.00398       3733.  49.0\n\n\nOne way we might emphasize the 4,000 equations is with a spaghetti plot. When we display the fitted line for bill_length_mm over the range of body_mass_g values, we can display a single line for each posterior draw. Here’s what that can look like.\n\nrange(chinstrap$body_mass_g)\n\n[1] 2700 4800\n\n# Note: go through this one line at a time\ndraws %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw)) +\n  geom_line(linewidth = 1/10, alpha = 1/10) + plot_aes\n\n\n\n\n\n\n\n\nIt might be easier to see what’s going on with a random subset of, say, 10 of the posterior draws.\n\nset.seed(10)\n\ndraws %&gt;% \n  # take a random sample of 10 rows\n  slice_sample(n = 10) %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw)) +\n  geom_line(linewidth = 1/2, alpha = 1/2) + plot_aes\n\n\n\n\n\n\n\n\nWhile we’re at it, let’s take 20 draws and do a little color coding.\n\nset.seed(20)\n\ndraws %&gt;% \n  # take a random sample of 20 rows\n  slice_sample(n = 20) %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw, color = beta0)) +\n  geom_line() +\n  scale_color_viridis_c(expression(beta[0]~(the~intercept)), end = .9) + plot_aes\n\n\n\n\n\n\n\n\nDo you remember how we said \\(\\beta_0\\) and \\(\\beta_1\\) had a strong negative correlation? Notice how the lines computed by lower \\(\\beta_0\\) values also tend to have higher slopes. This will happen all the time with conventional regression models.\n\n\nQuestion 2.5: We have done all this without yet specifying a prior. What do you think is going on?\n\nThe default prior is being used. The default prior is a weakly informative prior that is centered around 0."
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#questionexercise",
    "href": "posts/Lab-8/Bayes_Lab_1.html#questionexercise",
    "title": "Lab-8: Bayes",
    "section": "Question/Exercise:",
    "text": "Question/Exercise:\nIn the last part, we made a subset of the penguins data called gentoo, which was only the cases for which species == \"Gentoo\". Do that again and refit the Bayesian model to those data. Can you then remake some of the figures in this file with the new version of the model?\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-8/models/fit2.rds\")\n\nif (!file.exists(model_path)) {\n fit2 &lt;- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit1.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit2 &lt;- readRDS(model_path)\n}\n\n\ndraws &lt;- as_draws_df(fit2)\ndraws &lt;- draws %&gt;% \n  mutate(beta0 = b_Intercept,\n         beta1 = b_body_mass_g)\n\ndraws %&gt;% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density() +\n  geom_vline(data = points,\n             aes(xintercept = value, color = statistic),\n             size = 3/4) +\n  scale_color_viridis_d(option = \"A\", end = .8) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, labeller = label_parsed, scales = \"free\", ncol = 1) +\n  theme(strip.text = element_text(size = 14)) + plot_aes"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#references",
    "href": "posts/Lab-8/Bayes_Lab_1.html#references",
    "title": "Lab-8: Bayes",
    "section": "References",
    "text": "References\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/"
  },
  {
    "objectID": "posts/Lab-8/Bayes_Lab_1.html#session-information",
    "href": "posts/Lab-8/Bayes_Lab_1.html#session-information",
    "title": "Lab-8: Bayes",
    "section": "Session information",
    "text": "Session information\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.3.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggdist_3.3.2        broom.mixed_0.2.9.5 broom_1.0.7        \n [4] brms_2.21.0         Rcpp_1.0.13         ggside_0.3.1       \n [7] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[10] dplyr_1.1.4         purrr_1.0.4         readr_2.1.5        \n[13] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[16] tidyverse_2.0.0     pacman_0.5.1       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     viridisLite_0.4.2    farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] digest_0.6.37        timechange_0.3.0     estimability_1.5.1  \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  magrittr_2.0.3      \n[13] posterior_1.6.0      compiler_4.4.1       sass_0.4.9          \n[16] rlang_1.1.5          tools_4.4.1          utf8_1.2.4          \n[19] yaml_2.3.10          knitr_1.48           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    pkgbuild_1.4.4      \n[25] curl_5.2.2           plyr_1.8.9           abind_1.4-5         \n[28] withr_3.0.1          grid_4.4.1           stats4_4.4.1        \n[31] fansi_1.0.6          xtable_1.8-4         colorspace_2.1-1    \n[34] future_1.34.0        inline_0.3.19        emmeans_1.10.7      \n[37] globals_0.16.3       scales_1.3.0         cli_3.6.4           \n[40] mvtnorm_1.3-1        rmarkdown_2.28       generics_0.1.3      \n[43] RcppParallel_5.1.9   rstudioapi_0.17.1    reshape2_1.4.4      \n[46] tzdb_0.4.0           cachem_1.1.0         rstan_2.32.6        \n[49] splines_4.4.1        bayesplot_1.11.1     parallel_4.4.1      \n[52] matrixStats_1.4.1    vctrs_0.6.5          V8_6.0.1            \n[55] Matrix_1.7-0         jsonlite_1.8.9       hms_1.1.3           \n[58] listenv_0.9.1        crosstalk_1.2.1      jquerylib_0.1.4     \n[61] glue_1.8.0           parallelly_1.38.0    codetools_0.2-20    \n[64] DT_0.33              distributional_0.5.0 stringi_1.8.4       \n[67] gtable_0.3.5         QuickJSR_1.3.1       palmerpenguins_0.1.1\n[70] munsell_0.5.1        pillar_1.9.0         furrr_0.3.1         \n[73] htmltools_0.5.8.1    Brobdingnag_1.2-9    R6_2.5.1            \n[76] evaluate_1.0.0       lattice_0.22-6       backports_1.5.0     \n[79] bslib_0.8.0          rstantools_2.4.0     coda_0.19-4.1       \n[82] gridExtra_2.3        nlme_3.1-164         checkmate_2.3.2     \n[85] mgcv_1.9-1           xfun_0.49            pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/Lab-6/MLM_intro_questions.html",
    "href": "posts/Lab-6/MLM_intro_questions.html",
    "title": "Intro to MLM Exercise/Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "posts/Lab-6/MLM_intro_questions.html#footnotes",
    "href": "posts/Lab-6/MLM_intro_questions.html#footnotes",
    "title": "Intro to MLM Exercise/Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html",
    "href": "posts/Lab-9/Bayes_Lab_2.html",
    "title": "Lab 9",
    "section": "",
    "text": "For Lab 1, you had explored the data and looked at models built via lm() and via brms(using default priors). You had also drawn posterior samples after fitting the model.\nFor Lab 2, we continue with the Palmer Penguins. And we will look more at distributions and priors.\nAgain, there will be conceptual questions to answer as you work through this example, and exercises."
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#setup-packages-and-data",
    "href": "posts/Lab-9/Bayes_Lab_2.html#setup-packages-and-data",
    "title": "Lab 9",
    "section": "Setup: Packages and data",
    "text": "Setup: Packages and data\nWe load the primary packages.\n\nlibrary(pacman)\npacman::p_load(tidyverse,brms,tidybayes,ggdist,install = T)\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nWe want the same data set up as in the last lab.\n\n# load the penguins data\ndata(penguins, package = \"palmerpenguins\")\n\n# subset the data\nchinstrap &lt;- penguins %&gt;% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap) |&gt; \n  DT::datatable()\n\nRows: 68\nColumns: 8\n$ species           &lt;fct&gt; Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            &lt;fct&gt; Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    &lt;dbl&gt; 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     &lt;dbl&gt; 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm &lt;int&gt; 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       &lt;int&gt; 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               &lt;fct&gt; female, male, male, female, male, female, female, ma…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#models",
    "href": "posts/Lab-9/Bayes_Lab_2.html#models",
    "title": "Lab 9",
    "section": "Models",
    "text": "Models\nOnce again, we’ll fit the model\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) ,\n\\end{align}\n\\]\nwith both lm() and brm().\n\n# OLS\nfit1.ols &lt;- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n\n#define model path \n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/fit1b.rds\")\n\nif (!file.exists(model_path)) {\nfit1.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit1.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit1.b &lt;- readRDS(model_path)\n}"
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#bayesians-have-many-kinds-of-distributions",
    "href": "posts/Lab-9/Bayes_Lab_2.html#bayesians-have-many-kinds-of-distributions",
    "title": "Lab 9",
    "section": "Bayesians have many kinds of distributions",
    "text": "Bayesians have many kinds of distributions\nIn Bayesian statistics, we have at least 6 distributions to keep track of. Those are:\n\nthe likelihood distributions\nthe prior parameter distribution (aka priors)\nthe prior predictive distributions\nthe posterior parameter distributions (aka posteriors)\nthe posterior-predictive distribution\n\nIn many respect, it’s distributions ‘all the way down,’ with Bayesians. This can be indeed be difficult to keep track of at first. But since this is true for any class of Bayesian models (not just regression), you’ll hopefully get used to it.\n\n\nQUESTION 1: How would you represent these 6 distributions mathematically, using \\(P_0\\)’\\(P\\), \\(D\\), \\(|\\), and \\(\\theta\\) ?\n\n\n\n\n\n\nTip\n\n\n\nHint 1: Many of these terms were in the Bayes Rule.\n\n\n\n\nAnswer: …."
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#mathematical-representations",
    "href": "posts/Lab-9/Bayes_Lab_2.html#mathematical-representations",
    "title": "Lab 9",
    "section": "Mathematical Representations",
    "text": "Mathematical Representations\n\nLikelihood Distributions: The likelihood represents the probability of the observed data, given the model parameters: \\(\\[ P(D \\| \\theta) \\]\\)\nPrior Parameter Distribution (Priors): The prior distribution reflects our belief about the parameters before observing the data: \\(\\[ P(\\theta) \\]\\)\nPrior Predictive Distribution: This distribution represents the probability of the data before seeing any observations, based on the prior belief about the parameters: \\(\\[ P(D \\| P(\\theta)) \\]\\)\nPosterior Parameter Distribution (Posteriors): After observing the data, the posterior distribution represents our updated belief about the parameters. Using Bayes’ theorem, it is given by: \\(\\[ P(\\theta \\| D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} \\]\\)\nPosterior Predictive Distribution: This distribution gives the probability of new data points, based on the posterior distribution of the parameters: \\(\\[ P(D' \\| D) = \\int P(D' \\| \\theta) P(\\theta \\| D) d\\theta \\]\\) We also have some other distributions that follow from these. For example, - the distributions of the model expectations (i.e., the predicted means)\n\n\nLikelihood distributions.\nWe are approaching Bayesian statistics from a likelihood-based perspective. That is, we situate regression models within the greater context of a likelihood function. (There are ways to do non-parametric Bayesian statistics, which don’t focus on likelihoods. We won’t get into that right now.)\nSo far, we have been using the conventional Gaussian likelihood. If we have some variable \\(y\\), we can express it as normally distributed by\n\\[\n\\operatorname{Normal}(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{1}{2} \\left( \\frac{y - \\mu}{\\sigma}\\right)^2\\right),\n\\]\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. With this likelihood,\n\n\\(\\mu \\in \\mathbb R\\)\n\nthe mean can be any real number, ranging from \\(-\\infty\\) to \\(\\infty\\)\n\n\\(\\sigma \\in \\mathbb R_{&gt; 0}\\)\n\nthe standard deviation can take on any real number greater than zero.\n\n\nIt’s also the assumption\n\n\\(y \\in \\mathbb R\\)\n\nthe focal variable \\(y\\) can be any real number, ranging from \\(-\\infty\\) to \\(\\infty\\).\n\n\nOne of the ways we wrote our model formula back in the first file was\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n\\]\nand further in the discussion, we updated that equation with the posterior means for our three parameters to\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, 2.92) \\\\\n\\mu_i & = 32.2 + 0.004 \\text{body_mass_g}_i.\n\\end{align}\n\\]\nBefore we get into this, though, let’s back up and consider an intercept-only model of the form\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 ,\n\\end{align}\n\\]\nwhere there is no predictor variable. Here’s how to fit the model with brm().\n\n# Bayes\n\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/fit0b.rds\")\n\nif (!file.exists(model_path)) {\nfit0.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit0.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit0.b &lt;- readRDS(model_path)\n}\n\nLet’s look at the model summary.\n\nsummary(fit0.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.21      3.49    25.42    38.97 1.00     4595     2924\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4626     2720\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.25     2.47     3.45 1.00     2010     2020\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe intercept parameter \\(\\beta_0\\) is a stand-in for \\(\\mu\\). The \\(\\sigma\\) parameter is just \\(\\sigma\\). Here they are in a plot.\n\ndraws &lt;- as_draws_df(fit0.b) \n\ndraws %&gt;% \n  rename(`beta[0]==mu` = b_Intercept) %&gt;% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes + \n  scale_fill_manual(values = palette_condition) \n\n\n\n\n\n\n\n\nHere are the posterior means for those two parameters.\n\nmu &lt;- mean(draws$b_Intercept)\nsigma &lt;- mean(draws$sigma)\n\nmu; sigma\n\n[1] 32.21014\n\n\n[1] 2.925302\n\n\nWe can use dnorm() to compute the shape of \\(\\operatorname{Normal}(48.8, 3.4)\\).\n\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %&gt;% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\") + plot_aes\n\n\n\n\n\n\n\n\nWe can compare this to the sample distribution of the bill_length_mm data:\n\nchinstrap %&gt;% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\") + plot_aes\n\n\n\n\n\n\n\n\nIt’s not a great fit, but not horrible either.\nNow let’s see what this means for our univariable model fit1.b. First, let’s learn about the posterior_summary() function, which we’ll use to save a few posterior means.\n\nposterior_summary(fit1.b)\n\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.211118e+01 3.4771611651  2.552389e+01  3.902608e+01\nb_body_mass_g  4.479243e-03 0.0009256637  2.642918e-03  6.245207e-03\nsigma          2.932229e+00 0.2496554440  2.499569e+00  3.471021e+00\nIntercept      4.883259e+01 0.3406152809  4.813486e+01  4.948863e+01\nlprior        -4.300245e+00 0.0678611888 -4.450849e+00 -4.187769e+00\nlp__          -1.722437e+02 1.1939186872 -1.753094e+02 -1.708890e+02\n\nb0    &lt;- posterior_summary(fit1.b)[1, 1]\nb1    &lt;- posterior_summary(fit1.b)[2, 1]\nsigma &lt;- posterior_summary(fit1.b)[3, 1]\n\nNow we plot.\n\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %&gt;% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nOur univariable model fit1.b can be viewed as something like a 3-dimensional Gaussian hill.\n\n\nPrior distributions & Prior predictive distributions.\nLet’s hold off on this for a bit.\n\n\nParameter distributions.\nUp above, we plotted the posterior distributions for our intercept-only fit0.b model. Here they are again.\n\ndraws %&gt;% \n  rename(`beta[0]==mu` = b_Intercept) %&gt;% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes\n\n\n\n\n\n\n\n\nWe might practice making a similar plot for our univariable model fit1.b.\n\nas_draws_df(fit1.b) %&gt;% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes\n\n\n\n\n\n\n\n\nSome authors, like John Kruschke, have a strong preference for plotting their posteriors with histograms, rather than density plots."
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#distributions-of-the-model-expectations.",
    "href": "posts/Lab-9/Bayes_Lab_2.html#distributions-of-the-model-expectations.",
    "title": "Lab 9",
    "section": "Distributions of the model expectations.",
    "text": "Distributions of the model expectations.\nTake another look at the conditional_effects() plot from earlier.\n\nconditional_effects(fit1.b) %&gt;% \n  plot(points = TRUE)\n\n\n\n\n\n\n\n\nThe blue line is the posterior mean, for the \\(\\mu_i\\), the model-based mean for bill_length_mm, given the value for the predictor body_mass_g. The semitransparent gray ribbon marks the percentile-based interval for the conditional mean.\nWe can make a similar plot with the fitted() function. First we’ll need a predictor grid, we’ll call nd.\n\nnd &lt;- tibble(body_mass_g = seq(\n  from = min(chinstrap$body_mass_g),\n  to = max(chinstrap$body_mass_g),\n  length.out = 100))\n\nglimpse(nd)\n\nRows: 100\nColumns: 1\n$ body_mass_g &lt;dbl&gt; 2700.000, 2721.212, 2742.424, 2763.636, 2784.848, 2806.061…\n\n\nNow pump nd into the fitted() function.\n\nfitted(fit1.b, newdata = nd) %&gt;% \n  # subset the first 6 rows\n  head()\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.20513 1.0197075 42.24705 46.17349\n[2,] 44.30015 1.0012213 42.37850 46.23949\n[3,] 44.39516 0.9827796 42.51240 46.29758\n[4,] 44.49018 0.9643851 42.64621 46.35822\n[5,] 44.58519 0.9460405 42.77985 46.42049\n[6,] 44.68020 0.9277487 42.91383 46.49053\n\n\nNow plot.\n\nfitted(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nLook what happens if we augment the probs argument in fitted().\n\nfitted(fit1.b, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nNow look what happens if we set summary = FALSE.\n\nfitted(fit1.b, \n       newdata = nd,\n       summary = FALSE) %&gt;% \n  str()\n\n num [1:4000, 1:100] 44.4 43.9 44.1 44.5 46.7 ...\n\n\nWe get full 4,000 draw posterior distributions for each of the 100 levels of the predictor body_mass_g. Now look at what happens if we wrangle that output a little, and plot with aid from stat_lineribbon() from the ggdist package.\n\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, body_mass_g)) %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(body_mass_g = as.double(name)) %&gt;%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  stat_lineribbon() +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  plot_aes\n\n\n\n\n\n\n\n\nLook what happens when we request more intervals in the .width argument.\n\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, body_mass_g)) %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(body_mass_g = as.double(name)) %&gt;%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  plot_aes\n\n\n\n\n\n\n\n\nThe conditional mean, \\(\\mu_i\\), has its own distribution. We can take this visualization approach even further to make a color gradient.\n\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, body_mass_g)) %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(body_mass_g = as.double(name)) %&gt;%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  plot_aes\n\n\n\n\n\n\n\n\nFor technical details on this visualization approach, go here: https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-gradients.\nThe ggdist package even has an experimental visualization approach that’s based on density gradients, rather than interval-width gradients. Since this is experimental, I’m not going to go into the details. But if you’re curious and adventurous, you can learn more here: https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-density-gradients.\n\nPosterior-predictive distributions.\nThe last section showed the posterior distributions for the model expectations (i.e., the conditional means). In the context of the Gaussian distribution, that’s \\(\\mu\\), or \\(\\mu_i\\) in the case of the univariable model fit1.b. But the whole Gaussian distribution includes \\(\\mu\\) and \\(\\sigma\\).\nThis is where the predict() function comes in. First, we compare the fitted() output to predict().\n\nfitted(fit1.b, newdata = nd) %&gt;% \n  # subset the first 6 rows\n  head()\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.20513 1.0197075 42.24705 46.17349\n[2,] 44.30015 1.0012213 42.37850 46.23949\n[3,] 44.39516 0.9827796 42.51240 46.29758\n[4,] 44.49018 0.9643851 42.64621 46.35822\n[5,] 44.58519 0.9460405 42.77985 46.42049\n[6,] 44.68020 0.9277487 42.91383 46.49053\n\npredict(fit1.b, newdata = nd) %&gt;% \n  # subset the first 6 rows\n  head()\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.17269  3.119825 38.04113 50.21742\n[2,] 44.37479  3.155426 37.96836 50.48929\n[3,] 44.40880  3.071269 38.66573 50.38529\n[4,] 44.46118  3.080088 38.24711 50.43833\n[5,] 44.61212  3.105005 38.47867 50.87375\n[6,] 44.71355  3.103898 38.64486 50.96648\n\n\nThe posterior means (Estimate) are about the same, but the SD’s (Est.Error) are much larger in the predict() output, and the widths of the 95% intervals are too. Let’s make a plot.\n\npredict(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nThe gray band is the 95% interval for the entire posterior predictive distribution, not just the mean. In a good model, about 95% of the data points should be within those bands.\nDiscuss how the jagged lines have to do with the uncertainty in \\(\\sigma\\).\nIf we wanted to, we could integrate the fitted()-based conditional posterior mean, with the predict()-based posterior-predictive distribution.\n\n# save the fitted() results\nf &lt;- fitted(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) \n\npredict(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) + plot_aes\n\n\n\n\n\n\n\n\nIt’s the posterior predictive distribution that we use to predict new data points. For example, here’s what happens if we use predict() without the newdata argument.\n\npredict(fit1.b) %&gt;% \n  head()\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 47.76025  2.949676 42.06117 53.56949\n[2,] 49.59141  2.932069 43.90961 55.37971\n[3,] 48.38616  2.915855 42.66543 54.07587\n[4,] 47.87208  2.971567 41.98423 53.79133\n[5,] 48.81092  2.958257 42.87191 54.60564\n[6,] 49.76666  2.921512 44.09992 55.48069\n\n\nWe get posterior predictive summaries for each of the original data points. Here’s what happens if we set summary = FALSE.\n\npredict(fit1.b, summary = FALSE) %&gt;% \n  str()\n\n num [1:4000, 1:68] 44.8 46.3 52 47.8 45.2 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nThis time, we got 4,000 posterior draws for each. We can reduce that output with the ndraws argument.\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %&gt;% \n  str()\n\n num [1:6, 1:68] 46.9 49.2 52 43.6 51.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nNow wrangle and plot.\n\nset.seed(1)\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %&gt;% \n  data.frame() %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(row = str_remove(name, \"X\") %&gt;% as.double()) %&gt;% \n  left_join(chinstrap %&gt;% \n              mutate(row = 1:n()),\n            by = join_by(row)) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both) + plot_aes\n\n\n\n\n\n\n\n\nWith predict(), we can use the entire posterior-predictive distribution to simulate new data based on the values of our predictor variable(s). To give you a better sense of what’s happening under the hood, here’s an as_draws_df() based alternative.\n\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit1.b) %&gt;% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %&gt;% \n  select(.draw, beta0, beta1, sigma) %&gt;% \n  slice_sample(n = 6) %&gt;% \n  expand_grid(chinstrap %&gt;% select(body_mass_g)) %&gt;% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both) + plot_aes\n\n\n\n\n\n\n\n\nNow take a look at what happens when we plot the densities of several simulated draws.\n\nset.seed(1)\n\nas_draws_df(fit1.b) %&gt;% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %&gt;% \n  select(.draw, beta0, beta1, sigma) %&gt;% \n  slice_sample(n = 50) %&gt;%  # increase the number of random draws\n  expand_grid(chinstrap %&gt;% select(body_mass_g)) %&gt;% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(chinstrap$bill_length_mm) + c(-2, 2)) + plot_aes\n\n\n\n\n\n\n\n\nThe similarities and differences among the individual density lines give you a sense of the (un)certainty of the posterior-predictive distribution.\nThis may be a good time for you to work on Exercise 1 (see end of the document)\n#Part 4: Beginning to look at priors"
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#bayes-rule",
    "href": "posts/Lab-9/Bayes_Lab_2.html#bayes-rule",
    "title": "Lab 9",
    "section": "Bayes’ rule",
    "text": "Bayes’ rule\nBayes’ theorem will allow us to determine the plausibility of various values of our parameter(s) of interest, \\(\\theta\\), given the data \\(d\\), which we can express formally as \\(\\Pr(\\theta \\mid d)\\). Bayes’ rule takes on the form\n\\[\n\\Pr(\\theta \\mid d) = \\frac{\\Pr(d \\mid \\theta) \\Pr(\\theta)}{\\Pr(d)}.\n\\]\nwhere\n\n\\(\\Pr(d \\mid \\theta)\\) is the likelihood,\n\\(\\Pr(\\theta)\\) is the prior,\n\\(\\Pr(d)\\) is the average probability of the data, and\n\\(\\Pr(\\theta \\mid d)\\) is the posterior.\n\nWe can express this in words as\n\\[\n\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\n\\]\nThe denominator \\(\\Pr(d)\\) is a normalizing constant, and dividing by this constant is what converts the posterior \\(\\Pr(\\theta \\mid d)\\) into a probability metric."
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#default-priors",
    "href": "posts/Lab-9/Bayes_Lab_2.html#default-priors",
    "title": "Lab 9",
    "section": "Default priors",
    "text": "Default priors\nTo set your priors with brms, the brm() function has a prior argument. If you don’t explicitly use the prior argument, brm() will use default priors. This is what happened with our fit1.b model from above. We used default priors. If you’d like to see what those priors are, execute fit1.b$prior.\n\n# maybe show str(fit1.b)\nfit1.b$prior\n\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThus, a fuller expression of our model is\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i \\\\\n\\beta_0 & \\sim \\operatorname{Student-t}(3, 49.5, 3.6) \\\\\n\\beta_1 & \\sim \\operatorname{Uniform}(-\\infty, \\infty) \\\\\n\\sigma & \\sim \\operatorname{Student-t}^+(3, 0, 3.6).\n\\end{align}\n\\]\nIf we had wanted to see the brm() defaults before fitting the model, we could have used the get_prior() function.\n\nget_prior(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nIf you recall, the normal distribution is a member of the Student-t family, where the \\(\\nu\\) (aka degrees of freedom or normality parameter) is set to \\(\\infty\\). To give you a sense, here are the densities of three members of the Student-t family, with varying \\(\\nu\\) values.\n\ncrossing(theta = seq(from = -4.5, to = 4.5, length.out = 200),\n         nu = c(3, 10, Inf)) %&gt;% \n  mutate(density = dt(x = theta, df = nu)) %&gt;% \n  \n  ggplot(aes(x = theta, y = density, color = factor(nu))) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(expression(nu), option = \"A\", end = .7) +\n  labs(title = \"3 members of the Student-t family\",\n       x = expression(theta)) +\n  coord_cartesian(xlim = c(-4, 4)) + plot_aes\n\n\n\n\n\n\n\n\nThus, Student-t distributions have thicker tails when they have smaller \\(\\nu\\) parameters. In the case where \\(\\nu = 3\\), the tails are pretty thick, which means they are more tolerant of more extreme values. And thus priors with small-\\(\\nu\\) parameters will be weaker (i.e., more permissive) than their Gaussian counterparts.\nWe can visualize functions from ggdist to visualize the default brm() priors. We’ll start with the student_t(3, 49.5, 3.6) \\(\\beta_0\\) prior, and also take the opportunity to compare that with a slightly stronger normal(49.5, 3.6) alternative.\n\nc(prior(student_t(3, 49.5, 3.6)),\n  prior(normal(49.5, 3.6))) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75)) + plot_aes\n\n\n\n\n\n\n\n\nSee how that \\(n = 3\\) parameter in the default prior let do much thicker tails than it’s Gaussian counterpart. We can make the same kind of plot for our default \\(\\sigma\\) prior and its half-Gaussian counterpart.\n\nc(prior(student_t(3, 0, 3.6), lb = 0),  # note our use of the lb = 0 argument\n  prior(normal(0, 3.6), lb = 0)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30)) + plot_aes\n\n\n\n\n\n\n\n\nHere’s how we could have explicitly set our priors by hand.\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/fit2b.rds\")\n\nif (!file.exists(model_path)) {\nfit2.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g,\n  prior = prior(student_t(3, 49.5, 3.6), class = Intercept) +\n    prior(student_t(3, 0, 3.6), class = sigma, lb = 0)\n)\n  saveRDS(fit2.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit2.b &lt;- readRDS(model_path)\n}\n\nCompare the results.\n\nsummary(fit1.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.11      3.48    25.52    39.03 1.00     5495     2845\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5628     2873\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.25     2.50     3.47 1.00     1796     1802\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit2.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.22      3.48    25.26    38.86 1.00     4587     2988\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4608     2999\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.94      0.27     2.47     3.52 1.00     1894     1547\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#question-2-are-the-priors-the-same-what-do-you-think-is-going-on",
    "href": "posts/Lab-9/Bayes_Lab_2.html#question-2-are-the-priors-the-same-what-do-you-think-is-going-on",
    "title": "Lab 9",
    "section": "QUESTION 2 Are the priors the same? What do you think is going on?",
    "text": "QUESTION 2 Are the priors the same? What do you think is going on?\n\nAnswer: ….\n\n\n\n\n\n\nThe priors are not the same. The default priors for the intercept and sigma parameters in the fit1.b model are Student-t distributions with 3 degrees of freedom, while the fit2.b model has a normal distribution for the intercept and a half-normal distribution for sigma. The choice of priors can significantly affect the posterior distributions, especially when the sample size is small or when there are outliers in the data.\n\n\n\nIf you want to learn more about the default prior settings for brms, read through the set_prior section of the brms reference manual (https://CRAN.R-project.org/package=brms/brms.pdf)."
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#references",
    "href": "posts/Lab-9/Bayes_Lab_2.html#references",
    "title": "Lab 9",
    "section": "References",
    "text": "References\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/"
  },
  {
    "objectID": "posts/Lab-9/Bayes_Lab_2.html#session-information",
    "href": "posts/Lab-9/Bayes_Lab_2.html#session-information",
    "title": "Lab 9",
    "section": "Session information",
    "text": "Session information"
  },
  {
    "objectID": "posts/final-project/Final-project.html",
    "href": "posts/final-project/Final-project.html",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "",
    "text": "This tutorial will walk you through the process of simulating Pokémon battles using the OpenAI API and then analyzing the results using a multi-level binary logistic regression model using a Specification Curve Analysis framework. The main analytic goal is to see which pokemon stats are most predictive of winning a battle.\nWe are using a multi-level binary logistic regression model because…"
  },
  {
    "objectID": "posts/final-project/Final-project.html#load-libraries-api-key-and-set-model-information",
    "href": "posts/final-project/Final-project.html#load-libraries-api-key-and-set-model-information",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Load Libraries, API Key, and set model information",
    "text": "Load Libraries, API Key, and set model information\nStart by loading in the necessary libraries (and installing them if necessary) and setting up the OpenAI API key. I’d recommend running this in a separate notebook, as it will take a while to run the ~3000 individual battles. We also set a temperature of 0 to make the model more deterministic.\n\n\n\n\n\n\n💡 It is important to note that you should not share your OpenAI API key with anyone. It is a sensitive piece of information that should be kept private. If you are using a public notebook or sharing your code, make sure to remove or mask your API key before sharing. You can use environment variables or a .env file to store your API key securely.\n\n\n\n\nfrom openai import OpenAI, RateLimitError, APIError, APITimeoutError\nimport pandas as pd \nfrom tqdm.notebook import tqdm\nfrom dotenv import load_dotenv\nimport re\nimport numpy as np\nimport json\nimport argparse\nimport random\nimport time\nimport os\nimport ast\n\nload_dotenv(\"/Users/sm9518/Desktop/Article-Summarizer/.env\") # where i keep my API key... \napi_key = os.getenv(\"OPENAI_API_KEY\")\nif api_key:\n    print(\"API Key loaded successfully!\\n:)\")\nelse:\n    raise ValueError(\"API Key not found.\\nMake sure it is set in the .env file.\")\nmodel=\"gpt-3.5-turbo\" # set model. we dont need anything fancy for this task.\ntemperature=0 # set temp to be rather determinisitic \nSEED = random.seed(42) # set seed for reproducibility"
  },
  {
    "objectID": "posts/final-project/Final-project.html#load-data-and-sample",
    "href": "posts/final-project/Final-project.html#load-data-and-sample",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Load Data and Sample",
    "text": "Load Data and Sample\nBefore we run the simulations, we have to create our dataset. This can be done by downloading the original kaggle dataset here.\nOnce you’ve done so, we will extract the original 151 Pokémon and create a dataset of 20 matchups for each Pokémon.\n\ndf = pd.read_csv('/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project/data/pokedex.csv', index_col=0)\ndf.head()\n\nOG_pokedex = df.iloc[:151].copy() # take the OG 151 pokemon\n\n# build the 20‐matchups per Pokémon\nmatchups = []\nfor challenger in OG_pokedex['name']:\n    pool = [p for p in OG_pokedex['name'] if p != challenger]\n    opponents = random.sample(pool, 20) # give them 20 challengers\n    for opponent in opponents:\n        matchups.append({'challenger': challenger, 'opponent': opponent})\nmatchups_df = pd.DataFrame(matchups)\n\n\n\n# merge challenger metadata\nmatchups_with_meta = (\n    matchups_df\n    .merge(\n        OG_pokedex.add_suffix('_challenger'),\n        left_on='challenger',\n        right_on='name_challenger',\n        how='left'\n    )\n    # drop the redundant name_challenger column if you like\n    .drop(columns=['name_challenger'])\n    # merge opponent metadata\n    .merge(\n        OG_pokedex.add_suffix('_opponent'),\n        left_on='opponent',\n        right_on='name_opponent',\n        how='left'\n    )\n    .drop(columns=['name_opponent'])\n)\n\n# now every row has both challenger_* and opponent_* columns\nmatchups_with_meta.head()"
  },
  {
    "objectID": "posts/final-project/Final-project.html#hit-the-api-to-simulate-match-ups",
    "href": "posts/final-project/Final-project.html#hit-the-api-to-simulate-match-ups",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Hit the API to Simulate Match-ups",
    "text": "Hit the API to Simulate Match-ups\nOnce we’ve created the dataset, we can use the OpenAI API to simulate the match-ups. In short, for each battle, we will feed the stats of both Pokémon and ask GPT to determine the winner. We can then extract and save that data for downstream analyses.\n\n\n\n\n\n\n💡 The API call is rate limited, so we need to be careful about how many requests we send. We will use the tqdm library to show a progress bar and add a sleep time between requests to avoid hitting the rate limit.\nOur prompt is as follows:\n    \"Based on the stats, which Pokémon would win a one-on-one battle?\\n\\n\"\n    f\"{p1_stats}\\nVS\\n\\n{p2_stats}\\n\\n\"\n    \"Only respond with the name of the winning Pokémon.\"\n\n\n\n\n# Initialize OpenAI client\nclient = OpenAI()\n# ---- Utility Functions ---- #\n\ndef safe_parse_types(val):\n    if isinstance(val, list):\n        return val\n    try:\n        return ast.literal_eval(val)\n    except Exception:\n        return [str(val)]\n\ndef format_pokemon_stats(name, row, suffix):\n    types = safe_parse_types(row[f'type{suffix}'])\n    return (\n        f\"{name.title()}:\\n\"\n        f\"- Type: {', '.join(types)}\\n\"\n        f\"- HP: {row[f'hp{suffix}']}\\n\"\n        f\"- Attack: {row[f'attack{suffix}']}\\n\"\n        f\"- Defense: {row[f'defense{suffix}']}\\n\"\n        f\"- Special Attack: {row[f's_attack{suffix}']}\\n\"\n        f\"- Special Defense: {row[f's_defense{suffix}']}\\n\"\n        f\"- Speed: {row[f'speed{suffix}']}\\n\"\n    )\n\n# ---- API Interaction ---- #\n\ndef get_completion(prompt):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature\n    )\n    return response.choices[0].message.content.strip()\n\ndef get_response(prompt):\n    try:\n        return get_completion(prompt)\n    except RateLimitError as e:\n        retry_time = getattr(e, 'retry_after', 30)\n        print(f\"Rate limit exceeded. Retrying in {retry_time} seconds...\")\n        time.sleep(retry_time)\n        return get_response(prompt)\n    except APIError as e:\n        print(f\"API error occurred: {e}. Retrying in 30 seconds...\")\n        time.sleep(30)\n        return get_response(prompt)\n    except APITimeoutError as e:\n        print(f\"Request timed out: {e}. Retrying in 10 seconds...\")\n        time.sleep(10)\n        return get_response(prompt)\n    except Exception as e:\n        print(f\"Unexpected error: {e}. Retrying in 10 seconds...\")\n        time.sleep(10)\n        return get_response(prompt)\n\n# ---- Simulate One Battle ---- #\n\ndef simulate_battle(row):\n    p1_stats = format_pokemon_stats(row['challenger'], row, '_challenger')\n    p2_stats = format_pokemon_stats(row['opponent'], row, '_opponent')\n\n    prompt = (\n        \"Based on the stats, which Pokémon would win a one-on-one battle?\\n\\n\"\n        f\"{p1_stats}\\nVS\\n\\n{p2_stats}\\n\\n\"\n        \"Only respond with the name of the winning Pokémon.\"\n    )\n\n    response = get_response(prompt)\n    return response.lower()\n\n# ---- Run All Simulations ---- #\n\n# This should be your DataFrame containing all matchups\n# matchups_with_meta = pd.read_csv(...)  # Load your data here\n\nresults = []\n\nfor idx, row in tqdm(matchups_with_meta.iterrows(), total=len(matchups_with_meta), desc=\"Simulating battles\"):\n    print(f\"Simulating battle {idx + 1} of {len(matchups_with_meta)}: {row['challenger']} vs {row['opponent']}\")\n    winner = simulate_battle(row)\n    results.append({\n        \"challenger\": row['challenger'],\n        \"opponent\": row['opponent'],\n        \"winner\": winner\n    })\n    time.sleep(1.5)  # Respect rate limits\n\n# ---- Save Results ---- #\n\nresults_df = pd.DataFrame(results)\nmatchups_with_results = matchups_with_meta.merge(\n    results_df,\n    on=[\"challenger\", \"opponent\"],\n    how=\"left\"\n)\nmatchups_with_results.head()\nmatchups_with_results.to_csv(f\"/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project/data/pokemon_battle_results_{model}_{SEED}_{temperature}.csv\", index=False)\nprint(f\"\\nDone! Winners saved to pokemon_battle_results_{model}_{SEED}_{temperature}.csv.\")"
  },
  {
    "objectID": "posts/final-project/Final-project.html#take-a-look-at-the-data",
    "href": "posts/final-project/Final-project.html#take-a-look-at-the-data",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Take a look at the data",
    "text": "Take a look at the data\nHere, we are taking a look at a random sample of the data to see if it looks like we expect. We can also use the DT package to create an interactive table that allows us to sort and filter the data.\n\ndf_small |&gt; \n  sample_n(10) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "posts/final-project/Final-project.html#visualize-the-distribution-of-the-various-pokémon-stats",
    "href": "posts/final-project/Final-project.html#visualize-the-distribution-of-the-various-pokémon-stats",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Visualize the distribution of the various Pokémon stats",
    "text": "Visualize the distribution of the various Pokémon stats\n\nDistribution of Pokémon stats\nFrom looking at the density plots we have some interesting insights. For example, we can see that the distribution of the Pokémon stats is not normal, and that there are some outliers in the data. We’ll leave them in since we are interested in the relationship between the stats and the outcome of the battle and know this is how the Pokemon appear in the game.\n\ndf_small |&gt; \n  dplyr::select(3:12,-type) |&gt; \n  pivot_longer(cols = everything(), names_to = \"stat\", values_to = \"value\") |&gt; \n  ggplot(aes(x = value, fill = stat)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ stat, scales = \"free\",ncol = 3) +\n  labs(title = \"Distribution of Pokémon Stats\", x = \"Value\", y = \"Density\") +\n  scale_fill_manual(values = palette) +\n  plot_aes\n\n\n\n\n\n\n\n\n\n\nComparing Distributions of Pokémon stats by outcome\nNow, let’s look at the distribution of the Pokémon stats by winner. This will allow us to see if there are any differences in the different distributions between the winners and losers without running any analyses\n\ndf_small |&gt; \n  dplyr::select(3:12, -type, winner) |&gt; \n  rename(outcome = winner) |&gt; \n  pivot_longer(cols = -outcome, names_to = \"stat\", values_to = \"value\") |&gt; \n  ggplot(aes(x = value, fill = outcome)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ stat, scales = \"free\", ncol = 3) +\n  labs(title = \"Distribution of Pokémon Stats by Battle Outcome\", \n       x = \"Value\", y = \"Density\") +\n  scale_fill_manual(values = c(\"Win\" = \"#4daf4a\", \"Loss\" = \"#e41a1c\")) +\n  plot_aes\n\n\n\n\n\n\n\n\n\n\nComparing Relations between variables and outcomes\nThis plot shows how different Pokémon stats relate to each other and to winning or losing a battle. The diagonal panels show how each stat is distributed for winners (green) and losers (red). The lower panels show relationships between pairs of stats, with trendlines and points colored by outcome. The upper panels give the strength of the relationship between each pair of stats. Look for where the green and red separate—those are the stats or stat combinations most associated with winning or losing.\n\n# Define your color palette\nmy_colors &lt;- c(\"Win\" = \"#4daf4a\", \"Loss\" = \"#e41a1c\")\n\ndf_small %&gt;%\n  dplyr::select(3:12, -type, winner) %&gt;%\n  rename(outcome = winner) |&gt; \n  ggpairs(\n    columns = 1:9,  # Excludes winner column from variables\n    mapping = aes(color = outcome, alpha = 0.2),\n    lower = list(\n      continuous = wrap(\"smooth\", method = \"lm\", se = FALSE)\n    ),\n    upper = list(\n      continuous = wrap(\"cor\", size = 3, color = \"black\")\n    ),\n    diag = list(\n      continuous = function(data, mapping, ...) {\n        ggally_densityDiag(data = data, mapping = mapping, ...) +\n          scale_fill_manual(values = my_colors)\n      }\n    )\n  ) +\n  scale_color_manual(values = my_colors) +\n  theme(\n    axis.text = element_text(size = 6),\n    strip.text = element_text(size = 8),\n    legend.position = \"top\"\n  ) + plot_aes"
  },
  {
    "objectID": "posts/final-project/Final-project.html#write-functions-and-prep-for-sca-analysis",
    "href": "posts/final-project/Final-project.html#write-functions-and-prep-for-sca-analysis",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Write functions and prep for SCA Analysis",
    "text": "Write functions and prep for SCA Analysis\nBefore we get into the nittygritty of running analyses, we need to define some helper functions for SCA. The first one is a function to run the binomial logistic regression model. The second one is a function to extract the r-squared values from the model.\n\n### write binomial logistic regression function to pass to specr\nglmer_binomial &lt;- possibly(\n  function(formula, data) {\n    require(lme4)\n    require(broom.mixed)\n    glmer(formula,\n          data,\n          family = binomial(link = \"logit\"),\n          control = glmerControl(optimizer = \"bobyqa\"))\n  },\n  otherwise = NULL\n)\n\ntidy_new &lt;- function(x) {\n  fit &lt;- broom::tidy(x, conf.int = TRUE)\n  r2_vals &lt;- tryCatch(\n    performance::r2(x),\n    error = function(e) NULL\n  )\n  r2_marginal &lt;- NA\n  r2_conditional &lt;- NA\n  \n  if (!is.null(r2_vals)) {\n    if (\"R2_marginal\" %in% names(r2_vals)) {\n      # Mixed models: store Marginal and Conditional R2\n      r2_marginal &lt;- r2_vals$R2_marginal\n      r2_conditional &lt;- r2_vals$R2_conditional\n    } else if (\"R2\" %in% names(r2_vals)) {\n      # Simple models: store R2 in marginal, NA in conditional\n      r2_marginal &lt;- r2_vals$R2\n    }\n  }\n  fit$res &lt;- list(x)\n  fit$r2_marginal &lt;- r2_marginal\n  fit$r2_conditional &lt;- r2_conditional\n  return(fit)\n}"
  },
  {
    "objectID": "posts/final-project/Final-project.html#set-up-the-specifications",
    "href": "posts/final-project/Final-project.html#set-up-the-specifications",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Set up the specifications",
    "text": "Set up the specifications\nThe Specr package allows us to set up the specifications for the models we want to run. We will set up the syntax for the models we want to run, including the variables we want to include in the model and the random effects. We will also set up a function to extract the results from the models.\nThe model we are trying to specify is winner ~ Predictors + (1 | challenger).\n\n\n\n\n\n\n💡 In this case, we are including a random slope for the challenger variable. This means that we are allowing the effect of the challenger variable to vary across different levels of the data. This is important because it allows us to account for the fact that different challengers may have different effects on the outcome of the battle.\n\n\n\nHere is a brief breakdown of the different arguments\n\n\n\n\n\n\n\ndata: The data frame containing the data to be analyzed.\nx: The independent variables to be included in the model.\ny: The dependent variable to be predicted.\nmodel: The type of model to be used. In this case, we are using a binomial logistic regression model, which we specified earlier\ncontrols: The control variables to be included in the model. These are the variables that we want to control for in the analysis.\nadd_to_formula: The random effects to be included in the model. In this case, we are including a random slope for the challenger variable.\nfun1: The function to be used to extract the results from the model. In this case, we are using the broom.mixed::tidy() function to extract the results.\nfun2: The function to be used to extract the r-squared values from the model. In this case, we are using the tidy_new() function we defined earlier.\n\n\n\n\n\n### generate the different models\nspecs = specr::setup(\n  data = df_small_scaled,\n  x = c(\"height\", \"weight\",\"attack\", \"defense\", \n        \"s_attack\", \"s_defense\", \"speed\"),\n  y = c('winner'),\n  model = c('glmer_binomial'),\n  controls = c(\"height\", \"weight\",\"attack\", \"defense\", \n        \"s_attack\", \"s_defense\", \"speed\",\"hp\"),\nadd_to_formula = \"(1 | challenger) \",  # Random slope\nfun1 = function(x) broom.mixed::tidy(x, conf.int = TRUE),\nfun2 = tidy_new\n \n)"
  },
  {
    "objectID": "posts/final-project/Final-project.html#define-the-formulas",
    "href": "posts/final-project/Final-project.html#define-the-formulas",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Define the formulas",
    "text": "Define the formulas\nNow that we have set up the specifications, we can define the formulas for the models we want to run. The specr package allows us to define the formulas for the models we want to run and extract the results in a tidy format. Use the table below to inspect the various models we aim to run.\n\nspecs$specs &lt;- specs$specs %&gt;%\n  mutate(\n    controls_sorted = sapply(strsplit(as.character(controls), \",\"), function(x) paste(sort(trimws(x)), collapse = \",\"))\n  ) %&gt;%\n  distinct(x, y, model, controls_sorted, .keep_all = TRUE) %&gt;%  # REMOVE add_to_formula\n  dplyr::select(-controls_sorted)\n\n\nspecs$specs |&gt; \n  dplyr::select(x, y, controls,formula) |&gt;\n  DT::datatable(\n    options = list(\n      pageLength = 10,\n      autoWidth = TRUE,\n      columnDefs = list(list(width = '200px', targets = \"_all\"))\n    ),\n    rownames = FALSE\n  )"
  },
  {
    "objectID": "posts/final-project/Final-project.html#execute-the-analyses-in-parallel-using-furrr",
    "href": "posts/final-project/Final-project.html#execute-the-analyses-in-parallel-using-furrr",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Execute the Analyses in parallel using furrr",
    "text": "Execute the Analyses in parallel using furrr\nNow that we have set up the specifications and defined the formulas, we can run the models.\nThe specr package allows us to run the models in parallel and extract the results in a tidy format, we’ll utilize furrr to run our jobs in parallel to speed up the process. We’ll also cache our output as a .RDS file, so each time we run the code, it won’t have to re-run the models.\n\nmodel_path &lt;- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/final-project/models/sca_mode.rds\") # load in the model\n\nif (!file.exists(model_path)) { # if the file doesn't exist, then execute the code\n  specs &lt;- readRDS(model_path)\n  plan() # check what plan we have\n  opts &lt;- furrr_options(\n    globals = list(glmer_binomial = glmer_binomial) # tell the code we wanna use glmer \n  )\n  plan(strategy = multisession, workers = 6) # switch to multisession plan to make this run faster\n  results &lt;- specr(\n    specs,\n    .options = opts,   # Pass opts to specr\n    .progress = TRUE\n  )\n  plan(sequential) # switch back to sequential once done running\n  saveRDS(results, model_path)\n} else { # if the file exists, then load it in\n  results &lt;- readRDS(model_path)\n}"
  },
  {
    "objectID": "posts/final-project/Final-project.html#view-the-plots",
    "href": "posts/final-project/Final-project.html#view-the-plots",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "View the Plots",
    "text": "View the Plots\nWe can view our results using the plot function from specr. This simplest way to visualize most of the information contained in the results dataframe produced by our analyses. Briefly, the first plot shows the odds ratios for each model, while the second plot shows the specifications used in each model. The odds ratios are plotted on a log scale, and the confidence intervals are shown as error bars. The second plot shows the specifications used in each model, with the x-axis showing the different specifications and the y-axis showing the number of models that used that specification.\nGiven that we have several hundred unique models this graph gets kinda crazy to look at. You can zoom in on the plot to see the details. We’ll walk through two other ways to extract information from our results below.\n\np1 &lt;- plot(results, \n           type = \"curve\",\n           ci = T, \n           ribbon = F) +\n  geom_hline(yintercept = 0, \n             linetype = \"dashed\", \n             color = \"black\") +\n  labs(x = \"\", y = \"Odds Ratio\") + plot_aes\n\np2 &lt;- plot(results, \n           type = \"choices\",\n           choices = c(\"x\", \"y\", \"controls\")) +\n  labs(x = \"specifications (ranked)\") +\n  plot_aes + \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.text.y = element_text(size = 5),\n    axis.ticks.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.line.x = element_line(\"black\", size = .5),\n    axis.line.y = element_line(\"black\", size = .5)\n  )\n\nplot_grid(\n  p1, p2,\n  ncol = 1,           \n  align = \"v\",          \n  axis = \"rbl\",          \n  rel_heights = c(.60, 2.25)  \n)"
  },
  {
    "objectID": "posts/final-project/Final-project.html#individually-inspect-the-top-n-models",
    "href": "posts/final-project/Final-project.html#individually-inspect-the-top-n-models",
    "title": "Final Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression",
    "section": "Individually Inspect the top N-Models",
    "text": "Individually Inspect the top N-Models\nNow that we used specr(), we can summarize individual specifications by using broom::tidy() and broom::glance(). For most cases, this provides a sufficient and appropriate summary of the relationship of interest and model characteristics. Sometimes, however, it might be useful to investigate specific models in more detail or to investigate a specific parameter that is not provided by the two functions (e.g., r-square or variance accounted for by the model).\n\nInspect the significant models\nFirst, we’ll look at just the significant models (i.e., p &lt; 0.05). This is done by filtering the results dataframe to only include significant models. We can then use the DT package to create an interactive table that allows us to sort and filter the data.\n\nmodels &lt;- results %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(formula, x, y, estimate, std.error, p.value, conf.low, conf.high) %&gt;% \n  filter(p.value &lt; 0.051) %&gt;%  # keep only significant models\n  mutate(\n    estimate = round(estimate, 3),\n    std.error = round(std.error, 3),\n    p.value = round(p.value, 3),\n    conf.low = round(conf.low, 3),\n    conf.high = round(conf.high, 3)\n  ) %&gt;%\n  arrange(desc(abs(estimate)))\n\nmodels |&gt;\n  DT::datatable(\n    options = list(\n      pageLength = 10,\n      autoWidth = TRUE,\n      columnDefs = list(list(targets = \"_all\"))\n    ),\n    rownames = FALSE\n  )\n\n\n\n\n\n\n\nHow does R-squared change with different models?\nWe can also evaluate the best model by looking at the conditional r-square value. We start by ranking the models by their conditional r-square value and then plotting the results. This will allow us to see which models are the best predictors of the outcome.\nFrom the results below, we can see that we can account for over 50% of the variance in the outcome using just the challenger pokemon’s stats. This is a pretty good result, and it suggests that we can use these stats to predict the outcome of a battle. However, we still don’t quite know what the recipe for the best model is yet.\n\nbest_model &lt;- results %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(formula, x, y, estimate, std.error, p.value, conf.low, conf.high,fit_r2_conditional) %&gt;% \n  filter(p.value &lt; 0.051) %&gt;%  # keep only significant models\n  mutate(\n    estimate = round(estimate, 3),\n    std.error = round(std.error, 3),\n    p.value = round(p.value, 3),\n    conf.low = round(conf.low, 3),\n    conf.high = round(conf.high, 3)\n  ) \n\nbest_model %&gt;%\n  arrange(fit_r2_conditional) %&gt;%\n  mutate(rank = 1:n()) %&gt;%\n  ggplot(aes(x = rank, y = fit_r2_conditional)) +\n  geom_line(color = \"#ADA7C9\", size = 0.85) +  # smooth teal-ish line\n  geom_point(size = 1.5, alpha = 0.7, color = \"#4D4861\") +  # darker small points\n  theme_minimal(base_family = \"Futura Medium\") +  # match your font\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", size = 0.5),\n    axis.ticks = element_line(color = \"black\"),\n    axis.text = element_text(color = \"black\"),\n    strip.text = element_blank()\n  ) +\n  labs(\n    x = \"Model Rank\",\n    y = \"Conditional R²\"\n  ) +\n  plot_aes\n\n\n\n\n\n\n\n\n\n\nWhat seems to be the best model?\nNow that we have a sense of the best model, we can plot the results using ggplot to create a bargraph that shows us how much variance each of the top 50 models accounts for.\n\nbest_model %&gt;%\n  arrange(desc(fit_r2_conditional), desc(estimate)) %&gt;%\n  head(50) %&gt;%\n  mutate(rank = 1:n()) %&gt;%\n  ggplot(aes(x = factor(rank), \n             y = fit_r2_conditional, \n             fill = fit_r2_conditional)) +  # Use fit_r2_conditional for color fill\n  geom_col() +\n  geom_text(aes(label = formula), \n            vjust = -0.5, \n            size = 3, \n            angle = 90) +\n  scale_fill_gradient(low = \"#ee9b00\", high = \"#c44536\") +  # Gradient from low to high values\n  plot_aes + \n  theme(\n    strip.text = element_blank(),\n    axis.line = element_line(color = \"black\", size = .5),\n    axis.text = element_text(color = \"black\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels by 45 degrees\n    legend.position = \"none\"\n  ) +\n  labs(x = \"Model Rank\", y = \"Conditional R²\") +\n  ylim(0, 0.55)\n\n\n\n\n\n\n\n\nBased on what our graph tell’s us, the best model is (1 | challenger) + s_defense. This model accounts for 55% of the variance in the outcome, which is a pretty good result. However, it’s important to note that this is a pretty simple model and there are likely other factors that could be included to improve the model. For example, we could include the opponent’s stats or other variables that might be relevant to the outcome of the battle. If anything, this goes ot show how balanced of a meta Pokemon has.\nThanks for following along!\n\n\n\nIf you’d like to learn more about SCA, I’d recommend checking out ✨Dr. Dani Cosme’s ✨ website. She’s an amazing person, teacher, and has a ton of great resources on SCA and other statistical methods in R. I especially recommend this reproducibililty workshop."
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html",
    "href": "posts/Lab-5/poisson_lab_questions-1.html",
    "title": "Poisson Lab Answers",
    "section": "",
    "text": "To complete this lab:\nlibrary(pacman)\npacman::p_load(MASS,tidyverse,emmeans,ggeffects,easystats,performance,knitr, naniar,skimr,install = T)\n\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\n\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\ndata &lt;- read_delim(\"https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/2010.csv\")\n\ndata |&gt; \n  head() |&gt; \n  DT::datatable()\n### clean the data \n\ndata_pos &lt;- data %&gt;%\n  dplyr::select(wwwhr, wordsum, age, sex, reliten, polviews, wrkhome) %&gt;%\nreplace_with_na(.,\n             replace = list(wwwhr = c(-1, 998, 999),\n                          wordsum = c(-1, 99),\n                          reliten = c(0, 8, 9), \n             polviews = c(0, 8, 9), \n             wrkhome = c(0,8,9), \n             age=c(0, 98, 99)))\n\ndata_pos |&gt; \n  head() |&gt; \n  DT::datatable()\nQ: Can you explain what might be going on in the above code?\nA: The replace_with_na function is replacing zeros with NAs\nQ: The next step in data cleaning would be to ensure that the data in your code are aligned with the description/ usage context of the variables\ndata_pos &lt;- data_pos |&gt; \n  mutate(sex = factor(ifelse(sex == -1, \"Male\", \n                             ifelse(sex == 1, \"Female\", NA)), \n                      levels = c(\"Male\", \"Female\")),\n         reliten_recode = factor(reliten, levels = 1:5))"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#missingness",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#missingness",
    "title": "Poisson Lab Answers",
    "section": "Missingness",
    "text": "Missingness\n\ndata_pos %&gt;%\n  dplyr::select(reliten, reliten_recode)\n\n# A tibble: 2,044 × 2\n   reliten reliten_recode\n     &lt;dbl&gt; &lt;fct&gt;         \n 1       1 1             \n 2       4 4             \n 3       1 1             \n 4       1 1             \n 5       1 1             \n 6       4 4             \n 7       3 3             \n 8       1 1             \n 9       1 1             \n10       1 1             \n# ℹ 2,034 more rows\n\nskimr::skim(data_pos)\n\n\nData summary\n\n\nName\ndata_pos\n\n\nNumber of rows\n2044\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\nFALSE\n2\nMal: 1153, Fem: 891\n\n\nreliten_recode\n99\n0.95\nFALSE\n4\n2: 747, 1: 707, 4: 363, 3: 128\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwwhr\n996\n0.51\n9.79\n13.41\n0\n2\n5\n14\n168\n▇▁▁▁▁\n\n\nwordsum\n657\n0.68\n6.03\n2.07\n0\n5\n6\n7\n10\n▁▃▇▅▂\n\n\nage\n3\n1.00\n47.97\n17.68\n18\n33\n47\n61\n89\n▇▇▇▅▃\n\n\nreliten\n99\n0.95\n2.08\n1.08\n1\n1\n2\n3\n4\n▇▇▁▂▃\n\n\npolviews\n71\n0.97\n4.08\n1.46\n1\n3\n4\n5\n7\n▃▂▇▃▅\n\n\nwrkhome\n882\n0.57\n2.26\n1.72\n1\n1\n1\n4\n6\n▇▁▁▂▁"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#fit-a-poisson-model-to-the-data.",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#fit-a-poisson-model-to-the-data.",
    "title": "Poisson Lab Answers",
    "section": "Fit a Poisson model to the data.",
    "text": "Fit a Poisson model to the data.\n\npoisson_model &lt;- glm(wwwhr ~ wordsum + age + sex + reliten+ polviews + wrkhome, \n                     data = data_pos, \n                     family = poisson(link = \"log\"))\n\nmodel_summary &lt;- summary(poisson_model)$coefficients\nci &lt;- confint(poisson_model)  # Compute confidence intervals\n\n# Create a tidy dataframe\neffects_table &lt;- as.data.frame(model_summary) |&gt; \n  tibble::rownames_to_column(var = \"Predictor\") |&gt; \n  dplyr::mutate(\n    `Estimate` = round(Estimate, 3),\n    `Std. Error` = round(`Std. Error`, 3),\n    `z value` = round(`z value`, 3),\n    `Pr(&gt;|z|)` = round(`Pr(&gt;|z|)`, 3),\n    `CI Lower` = round(ci[,1], 3),\n    `CI Upper` = round(ci[,2], 3)\n  )\n\n# Display the table using kable\nkable(effects_table, format = \"markdown\", caption = \"Poisson Model Coefficients\")\n\n\nPoisson Model Coefficients\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\nCI Lower\nCI Upper\n\n\n\n\n(Intercept)\n1.649\n0.082\n20.225\n0\n1.489\n1.809\n\n\nwordsum\n0.100\n0.008\n12.836\n0\n0.084\n0.115\n\n\nage\n-0.016\n0.001\n-15.132\n0\n-0.019\n-0.014\n\n\nsexFemale\n0.259\n0.026\n9.836\n0\n0.208\n0.311\n\n\nreliten\n0.199\n0.012\n16.726\n0\n0.176\n0.222\n\n\npolviews\n-0.036\n0.010\n-3.666\n0\n-0.054\n-0.017\n\n\nwrkhome\n0.078\n0.008\n10.243\n0\n0.063\n0.093"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#carry-out-model-checking",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#carry-out-model-checking",
    "title": "Poisson Lab Answers",
    "section": "Carry out model checking",
    "text": "Carry out model checking\nHint: performance package has the function you’re looking for\n\ncheck_model(poisson_model,plot = T)"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#find-any-outliers",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#find-any-outliers",
    "title": "Poisson Lab Answers",
    "section": "Find any outliers",
    "text": "Find any outliers\n\n# Filter the data to remove outliers\ndata_pos_filtered &lt;- data_pos %&gt;%\n  mutate(mean_wwwhr = mean(wwwhr, na.rm = TRUE),\n         sd_wwwhr = sd(wwwhr, na.rm = TRUE)) %&gt;%\n  filter(wwwhr &gt;= (mean_wwwhr - 3 * sd_wwwhr) & wwwhr &lt;= (mean_wwwhr + 3 * sd_wwwhr)) %&gt;%\n  select(-mean_wwwhr, -sd_wwwhr)  # Remove the temporary columns\n\n\n\n\ncheck_outliers(poisson_model)\n\n3 outliers detected: cases 72, 156, 363.\n- Based on the following method and threshold: cook (0.9).\n- For variable: (Whole model)."
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#refit-the-model-after-excluding-outliers",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#refit-the-model-after-excluding-outliers",
    "title": "Poisson Lab Answers",
    "section": "Refit the model after excluding outliers",
    "text": "Refit the model after excluding outliers\n\npoisson_model_filtered &lt;- glm(wwwhr ~ wordsum + age + sex + reliten+ polviews + wrkhome, \n                     data = data_pos_filtered, \n                     family = poisson(link = \"log\"))\n\nmodel_summary &lt;- summary(poisson_model_filtered)$coefficients\nci &lt;- confint(poisson_model_filtered)  # Compute confidence intervals\n\n# Create a tidy dataframe\neffects_table &lt;- as.data.frame(model_summary) |&gt; \n  tibble::rownames_to_column(var = \"Predictor\") |&gt; \n  dplyr::mutate(\n    `Estimate` = round(Estimate, 3),\n    `Std. Error` = round(`Std. Error`, 3),\n    `z value` = round(`z value`, 3),\n    `Pr(&gt;|z|)` = round(`Pr(&gt;|z|)`, 3),\n    `CI Lower` = round(ci[,1], 3),\n    `CI Upper` = round(ci[,2], 3)\n  )\n\n# Display the table using kable\nkable(effects_table, format = \"markdown\", caption = \"Poisson Model Coefficients (Excluding Outliers)\") \n\n\nPoisson Model Coefficients (Excluding Outliers)\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\nCI Lower\nCI Upper\n\n\n\n\n(Intercept)\n1.631\n0.086\n18.885\n0.000\n1.461\n1.800\n\n\nwordsum\n0.091\n0.008\n11.002\n0.000\n0.075\n0.107\n\n\nage\n-0.011\n0.001\n-9.705\n0.000\n-0.013\n-0.009\n\n\nsexFemale\n0.142\n0.028\n5.071\n0.000\n0.087\n0.197\n\n\nreliten\n0.143\n0.013\n11.254\n0.000\n0.118\n0.168\n\n\npolviews\n-0.022\n0.010\n-2.118\n0.034\n-0.042\n-0.002\n\n\nwrkhome\n0.040\n0.008\n4.758\n0.000\n0.023\n0.056\n\n\n\n\n\n\nCheck for Overdispersion\nHint: performance package has the function you’re looking for\n\ncheck_overdispersion(poisson_model_filtered)\n\n# Overdispersion test\n\n       dispersion ratio =   10.684\n  Pearson's Chi-Squared = 6282.259\n                p-value =  &lt; 0.001\n\n\nWhat do you notice?\n\nThat we are expriencing overdispersion\n\nAnd what’s a good next step forward?\n\nWe can deal with this using a Negative Binomial regression model instead of a Poisson model.\n\nCan there be another model class that can fit the data? If so, fit this model to the data.\n\nA Negative Binomial regression model\n\n\nnb_model &lt;- glm.nb(wwwhr ~ wordsum + age + sex + reliten_recode + polviews + wrkhome, \n                   data = data_pos_filtered)\n\nmodel_summary &lt;- summary(nb_model)$coefficients\nci &lt;- confint(nb_model)  # Compute confidence intervals\n\n# Create a tidy dataframe\neffects_table &lt;- as.data.frame(model_summary) |&gt; \n  tibble::rownames_to_column(var = \"Predictor\") |&gt; \n  dplyr::mutate(\n    `Estimate` = round(Estimate, 3),\n    `Std. Error` = round(`Std. Error`, 3),\n    `z value` = round(`z value`, 3),\n    `Pr(&gt;|z|)` = round(`Pr(&gt;|z|)`, 3),\n    `CI Lower` = round(ci[,1], 3),\n    `CI Upper` = round(ci[,2], 3)\n  )\n\n# Display the table using kable\nkable(effects_table, format = \"markdown\", caption = \"Negative Binomial (Without Outliers)\") \n\n\nNegative Binomial (Without Outliers)\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\nCI Lower\nCI Upper\n\n\n\n\n(Intercept)\n1.720\n0.252\n6.829\n0.000\n1.224\n2.221\n\n\nwordsum\n0.101\n0.025\n3.972\n0.000\n0.049\n0.153\n\n\nage\n-0.012\n0.003\n-3.608\n0.000\n-0.019\n-0.005\n\n\nsexFemale\n0.105\n0.088\n1.201\n0.230\n-0.067\n0.278\n\n\nreliten_recode2\n0.293\n0.106\n2.766\n0.006\n0.085\n0.500\n\n\nreliten_recode3\n0.397\n0.200\n1.984\n0.047\n0.016\n0.806\n\n\nreliten_recode4\n0.478\n0.125\n3.815\n0.000\n0.236\n0.721\n\n\npolviews\n-0.024\n0.032\n-0.740\n0.460\n-0.086\n0.038\n\n\nwrkhome\n0.035\n0.026\n1.330\n0.183\n-0.017\n0.089"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#which-one-is-better--your-earlier-model-or-later-model",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#which-one-is-better--your-earlier-model-or-later-model",
    "title": "Poisson Lab Answers",
    "section": "Which one is better- your earlier model, or later model?",
    "text": "Which one is better- your earlier model, or later model?\n\nAIC(poisson_model_filtered, nb_model)\n\n                       df      AIC\npoisson_model_filtered  7 7136.602\nnb_model               10 3808.681\n\n# Alternatively, you can compare the residual deviance and degrees of freedom:\ndeviance_poisson &lt;- deviance(poisson_model_filtered)\ndf_poisson &lt;- df.residual(poisson_model_filtered)\ndeviance_ratio_poisson &lt;- deviance_poisson / df_poisson\n\n# Calculate deviance and degrees of freedom for Negative Binomial model\ndeviance_nb &lt;- deviance(nb_model)\ndf_nb &lt;- df.residual(nb_model)\ndeviance_ratio_nb &lt;- deviance_nb / df_nb\n\n# Check for overdispersion and compare the models\nbetter_model &lt;- ifelse(deviance_ratio_poisson &gt; 1, \n                       \"Negative Binomial Model is better due to overdispersion\", \n                       ifelse(deviance_ratio_poisson &gt; deviance_ratio_nb, \n                              \"Poisson Model is better\", \n                              \"Negative Binomial Model is better\"))\n\n# Print out the results\ncat(\"Deviance-to-DF Ratio for Poisson Model: \", deviance_ratio_poisson, \"\\n\")\n\nDeviance-to-DF Ratio for Poisson Model:  8.721789 \n\ncat(\"Deviance-to-DF Ratio for Negative Binomial Model: \", deviance_ratio_nb, \"\\n\")\n\nDeviance-to-DF Ratio for Negative Binomial Model:  1.129097 \n\ncat(\"Model Comparison: \", better_model)\n\nModel Comparison:  Negative Binomial Model is better due to overdispersion\n\n\n\nThe Negative Binomial model has a much lower AIC (3808.681) compared to the Poisson model (7136.602), suggesting that the Negative Binomial model provides a better fit to the data.\n\n\nBased on deviance, the negative binomial model is better."
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "title": "Poisson Lab Answers",
    "section": "What is zero inflation? Is there zero-inflation in your chosen model?",
    "text": "What is zero inflation? Is there zero-inflation in your chosen model?\n\nZero-inflation occurs when the data contains an excess number of zero outcomes that cannot be explained by the underlying model, such as a Poisson or Negative Binomial model. In other words, there are more zero values in the data than expected given the distribution (Poisson or Negative Binomial), which can indicate that a separate process is generating these excess zeros.\n\n\nperformance::check_zeroinflation(nb_model)\n\n# Check for zero-inflation\n\n   Observed zeros: 40\n  Predicted zeros: 63\n            Ratio: 1.57"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#log-lambda",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#log-lambda",
    "title": "Poisson Lab Answers",
    "section": "Log Lambda",
    "text": "Log Lambda\n\nlambda_poisson &lt;- predict(poisson_model_filtered, type = \"response\")\nlambda_nb &lt;- predict(nb_model, type = \"response\")\n\n# Log transform the lambda values\nlog_lambda_poisson &lt;- log(lambda_poisson)\nlog_lambda_nb &lt;- log(lambda_nb)\n\n# Output the log lambda values for a subset of the data\nhead(data.frame(log_lambda_poisson, log_lambda_nb))\n\n   log_lambda_poisson log_lambda_nb\n1            2.171034      2.079081\n2            2.881145      2.913070\n4            2.115022      2.220856\n8            2.191091      2.252398\n9            1.992306      2.030755\n12           2.578640      2.592963"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#mean-count",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#mean-count",
    "title": "Poisson Lab Answers",
    "section": "Mean Count",
    "text": "Mean Count\n\npredicted_counts_poisson &lt;- predict(poisson_model_filtered, type = \"response\")\npredicted_counts_nb &lt;- predict(nb_model, type = \"response\")\n\n# Calculate the mean of predicted counts for each model\nmean_count_poisson &lt;- mean(predicted_counts_poisson, na.rm = TRUE)\nmean_count_nb &lt;- mean(predicted_counts_nb, na.rm = TRUE)\n\n# Output the mean counts\ncat(\"Mean Count (Poisson Model): \", mean_count_poisson, \"\\n\")\n\nMean Count (Poisson Model):  8.821849 \n\ncat(\"Mean Count (Negative Binomial Model): \", mean_count_nb, \"\\n\")\n\nMean Count (Negative Binomial Model):  8.836638"
  },
  {
    "objectID": "posts/Lab-5/poisson_lab_questions-1.html#report-your-conclusions",
    "href": "posts/Lab-5/poisson_lab_questions-1.html#report-your-conclusions",
    "title": "Poisson Lab Answers",
    "section": "Report your conclusions",
    "text": "Report your conclusions\n\nThese results show that, on average, both models predict almost the same number of hours, with the Negative Binomial model giving a slightly higher estimate. This small difference could reflect model nuances, but both models suggest a similar central tendency in internet usage."
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html",
    "href": "posts/Lab-3/ord_lab_q.html",
    "title": "Ordinal Regression Lab Answers",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#instructions",
    "href": "posts/Lab-3/ord_lab_q.html#instructions",
    "title": "Ordinal Regression Lab Answers",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#load-packages",
    "href": "posts/Lab-3/ord_lab_q.html#load-packages",
    "title": "Ordinal Regression Lab Answers",
    "section": "Load packages:",
    "text": "Load packages:\n\nlibrary(pacman)\npacman::p_load(tidyverse, DT, broom, performance,\n               ordinal,car,ggeffects,gofact,brms,\n               emmeans,knirt,MASS,brant,\n               install = TRUE)\n\n\n#### define plot objects and stuff\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\n\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#load-data",
    "href": "posts/Lab-3/ord_lab_q.html#load-data",
    "title": "Ordinal Regression Lab Answers",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\n# gb &lt;- ....\n\n### only use the the first three ranks \ndata = gbbo |&gt; \n  rename(Technical_Rank = `Technical Rank`) |&gt; \n  filter(Technical_Rank &lt; 4) |&gt; \n  mutate(Technical_Rank = factor(Technical_Rank, levels = c(1, 2, 3), ordered = TRUE),\n         Gender = factor(Gender))"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#explore",
    "href": "posts/Lab-3/ord_lab_q.html#explore",
    "title": "Ordinal Regression Lab Answers",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\ngb &lt;- data %&gt;% \n  mutate(AgeGroup = cut(Age, \n                        breaks = seq(floor(min(Age, na.rm = TRUE)), ceiling(max(Age, na.rm = TRUE)), by = 10),\n                        include.lowest = TRUE, right = FALSE))\n\n# Compute percentages by Age Group\nage_rank &lt;- gb %&gt;%\n  group_by(AgeGroup, `Technical_Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\n# Compute percentages by Gender\ngender_rank &lt;- gb %&gt;%\n  group_by(Gender, `Technical_Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\n# Plot for Age Group\nggplot(age_rank, aes(x = AgeGroup, y = perc, fill = factor(`Technical_Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_manual(values = palette_condition) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age Group\",\n       x = \"Age Group\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  plot_aes +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Plot for Gender\nggplot(gender_rank, aes(x = Gender, y = perc, fill = factor(`Technical_Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_manual(values = palette_condition) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Gender\",\n       x = \"Gender\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  plot_aes"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#ordinal-analysis",
    "href": "posts/Lab-3/ord_lab_q.html#ordinal-analysis",
    "title": "Ordinal Regression Lab Answers",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\ndata = gbbo |&gt; \n  rename(Technical_Rank = `Technical Rank`) |&gt; \n  filter(Technical_Rank &lt; 4) |&gt; \n  mutate(Technical_Rank = factor(Technical_Rank, levels = c(1, 2, 3), ordered = TRUE),\n  Gender = factor(Gender)) \n\n\nThe order represents their placement in a technical bake-off.\n\n\nConvert input variables to categorical factors as appropriate.\n\n# Factorizing gender\nGender = factor(Gender)\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\n# Fit the ordinal logistic regression model\nmodel &lt;- clm(`Technical_Rank` ~ Gender * Age, data = gb)\n\n# Extract results with 95% confidence intervals\nresults &lt;- tidy(model, conf.int = TRUE) %&gt;%\n  rename(Estimate = estimate, `Lower CI` = conf.low, `Upper CI` = conf.high) %&gt;%\n  mutate(\n    Estimate = round(Estimate, 3),\n    `Lower CI` = round(`Lower CI`, 3),\n    `Upper CI` = round(`Upper CI`, 3),\n    p.value = round(2 * (1 - pnorm(abs(statistic))), 3)  # Compute p-values manually and round\n  )\n\n# Display results in an interactive DT table\ndatatable(results, \n          options = list(pageLength = 5, scrollX = TRUE),\n          caption = \"Ordinal Logistic Regression Results with 95% Confidence Intervals\")\n\n\n\n\nsummary(model)\n\nformula: Technical_Rank ~ Gender * Age\ndata:    gb\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  309  -336.64 683.28 3(0)  4.04e-08 1.1e+05\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \nGenderM     -1.14947    0.67290  -1.708   0.0876 .\nAge         -0.02311    0.01246  -1.855   0.0636 .\nGenderM:Age  0.03879    0.01853   2.093   0.0363 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -1.416692   0.459602  -3.082\n2|3  0.004944   0.452043   0.011\n\n\n\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n::: {.cell}\n\n```{.r .cell-code}\nmodel_interaction &lt;- clm(`Technical_Rank` ~ Gender * Age, data = gb)\n\n# Fit the model without the interaction term\nmodel_main &lt;- clm(`Technical_Rank` ~ Gender + Age, data = gb)\n\n# Compare the two models using ANOVA\nanova_results &lt;- anova(model_main, model_interaction)\nanova_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of cumulative link models:\n \n                  formula:                      link: threshold:\nmodel_main        Technical_Rank ~ Gender + Age logit flexible  \nmodel_interaction Technical_Rank ~ Gender * Age logit flexible  \n\n                  no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel_main             4 685.72 -338.86                        \nmodel_interaction      5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nwe should use the interaction term since th emodel has significantly better fit\n\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\npreds &lt;- ggemmeans(model_interaction, terms = c(\"Age\", \"Gender\"), type = \"fixed\")\n\nggplot(preds, aes(x = x, y = predicted, color = group, fill = group)) +\n  geom_line(size = 1) +  \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, linetype = \"dashed\") +  \n  facet_wrap(~response.level, scales = \"free_y\") +  \n  labs(title = \"Predicted Probability of Technical Rank by Age and Gender\",\n       x = \"Age\",\n       y = \"Predicted Probability\",\n       color = \"Gender\",\n       fill = \"Gender\") +  \n  scale_color_manual(values = palette) +  \n  scale_fill_manual(values = palette) +  \n  plot_aes\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\nols_clm = MASS::polr(Technical_Rank~Gender*Age, data=gb)\n\nggeffect(ols_clm, c(\"Age[all]\", \"Gender\"), latent=TRUE) %&gt;% plot() +  scale_color_manual(values = palette) +  \n  scale_fill_manual(values = palette) +  plot_aes \n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nbrant(ols_clm)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     1.29    3   0.73\nGenderM     0.58    1   0.44\nAge     0.06    1   0.8\nGenderM:Age 0.92    1   0.34\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\nWe fail to rejecet it proportional odds assumption holds"
  },
  {
    "objectID": "posts/Lab-3/ord_lab_q.html#brms",
    "href": "posts/Lab-3/ord_lab_q.html#brms",
    "title": "Ordinal Regression Lab Answers",
    "section": "brms",
    "text": "brms\n\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\nmodel_path &lt;- file.path(\"/Users/sm9518/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/PSY-504/stevens-blog/posts/Lab-3/models/brms_model.rds\")\n\nif (!file.exists(model_path)) {\n  # If the RDS file does not exist, create the model\n  ols2_brm &lt;- brm(Technical_Rank ~ Gender * Age, data = gb, \n                  family = cumulative, cores = 4, chains = 4)\n  \n  # Save the model output to an RDS file\n  saveRDS(ols2_brm, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  ols2_brm &lt;- readRDS(model_path)\n}\n\n\nThe results are the same since we are using an uninformative prior and the estimates are similar to that of ML (frequentist estimations)\n\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n\n\nconditional_effects(ols2_brm, categorical = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?).\n\nRun the below code. What do you think?\n\ncheck_predictions(ols2_brm) |&gt;  plot() + plot_aes\n\n\n\n\n\n\n\n\n\nYes, the model appears to fit the data"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "",
    "text": "This worksheet helps to give you a better idea about what to do with the trace plots."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#packages-and-data",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#packages-and-data",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "Packages and data",
    "text": "Packages and data\nLoad the primary packages.\n\nlibrary(pacman)\npacman::p_load(tidyverse, brms, tidybayes,\n               ggdist,bayesplot,moderndive,faux,GGally,ggmcmc,install = T)\nset.seed(42)\n\n\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n\nThis time we’ll simulate data with the faux package.\n\n# how many cases?\nn &lt;- 100\n\n# population values\nmu    &lt;- 0\nsigma &lt;- 1\nrho   &lt;- .5\n\n# simulate and save\nset.seed(1)\n\nd &lt;- rnorm_multi(\n  n = n,\n  mu = c(mu, mu),\n  sd = c(sigma, sigma), \n  r = rho, \n  varnames = list(\"x\", \"y\")\n)\n\nglimpse(d)\n\nRows: 100\nColumns: 2\n$ x &lt;dbl&gt; -0.232341576, 0.137981847, -0.268214782, 1.302539315, 0.612654423, -…\n$ y &lt;dbl&gt; -0.85270825, 0.18009772, -1.17913643, 1.46056809, -0.04193022, 0.173…\n\n\nWe might look at the data with a ggpairs() plot.\n\nd %&gt;% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", binwidth = 0.25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE))) + plot_aes\n\n\n\n\n\n\n\n\nCheck the sample statistics.\n\n# univariate\nd %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(m = mean(value),\n            s = sd(value))\n\n# A tibble: 2 × 3\n  name       m     s\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 x     0.113  0.914\n2 y     0.0754 0.913\n\n# bivariate\nd %&gt;% \n  summarise(r = cor(y, x))\n\n          r\n1 0.4502206"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#base-model",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#base-model",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "Base model",
    "text": "Base model\nLet’s fit a simple model\n\\[\n\\begin{align}\ny_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 x_i \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1),\n\\end{align}\n\\]\nAs we fit the model with brm(), take the opportunity to consider some of the default settings.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit13b.rds'\n\nif (!file.exists(model_path)) {\n\nfit13.b &lt;- brm(\n  data = d,\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 13,\n  \n  # default settings we've been ignoring up to this point\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n  # if you have a good computer, maybe try setting cores = 4\n)\nsaveRDS(fit13.b,model_path)\n} else {\n  fit13.b &lt;- readRDS(model_path)\n}\n\nIf you’d like to use multiple cores, but you’re not sure how many you have, execute parallel::detectCores().\n\nQuestion 1: How many cores do you have?\n\ncores = parallel::detectCores()\ncat(\"Steven's computer has:\",cores)\n\nSteven's computer has: 14\n\n\nCheck the model summary.\n\nsummary(fit13.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at the parameter posteriors in a pairs() plot.\n\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3)) \n\n\n\n\n\n\n\n\nThe pairs() plot is a wrapper around the mcmc_pairs() function from bayesplot. By default, half of the chains are depicted in the scatter plots below the diagonal, and the other half are displayed above the diagonal. The basic idea is you want the results form different chains to mirror one another. You can control this behavior with the condition argument.\n\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3),\n      # here we put the first chain in above the diagonal,\n      # and we put the second through fourth chains below the diagonal\n      condition = pairs_condition(chains = list(1, 2:4)))\n\n\n\n\n\n\n\n\nThis particular arrangement is a little silly, but it should give you a sense of how to control the output. Also, by default the histograms on the diagonal use the draws from all the chains.\nIf you wanted, you could also make a similar kind of plot with ggpairs().\n\nas_draws_df(fit13.b) %&gt;% \n  select(b_Intercept:sigma) %&gt;% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", bins = 25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE)),\n          lower = list(continuous = wrap(\"points\", size = 1/4, alpha = 1/3))) + plot_aes\n\n\n\n\n\n\n\n\nNow take a look at the plot() output.\n\nplot(fit13.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nThese trace plots look like a dream. They have the appearance of fuzzy caterpillars, which is why they’re even sometimes called caterpillar plots.\nLet’s work directly with the chains via as_draws_df().\n\nas_draws_df(fit13.b) %&gt;% \n  # notice the 3 meta-data columns at the end\n  glimpse()\n\nRows: 4,000\nColumns: 9\n$ b_Intercept &lt;dbl&gt; 0.023961191, 0.008764307, -0.066725637, -0.085990390, 0.01…\n$ b_x         &lt;dbl&gt; 0.5574909, 0.5822172, 0.4748096, 0.4267265, 0.4081808, 0.4…\n$ sigma       &lt;dbl&gt; 0.8455813, 0.8642759, 0.7653377, 0.9147409, 0.8412759, 0.7…\n$ Intercept   &lt;dbl&gt; 0.08707099, 0.07467320, -0.01297564, -0.03768355, 0.063654…\n$ lprior      &lt;dbl&gt; -2.842647, -2.874429, -2.716021, -2.844376, -2.764485, -2.…\n$ lp__        &lt;dbl&gt; -124.8296, -125.3474, -125.0186, -126.0295, -124.1495, -13…\n$ .chain      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ .iteration  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ .draw       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n\n\nWe can use those meta-data columns to make our own trace plots with ggplot functions.\n\nas_draws_df(fit13.b) %&gt;% \n  pivot_longer(b_Intercept:sigma) %&gt;% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for Greek formatted strip labels\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots!\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\nWe might restrict to the first few post-warmup iterations to help give us a better sense of what’s happening.\n\nas_draws_df(fit13.b) %&gt;% \n  filter(.iteration &lt; 21) %&gt;% \n  pivot_longer(b_Intercept:sigma) %&gt;% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for nice formatting\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots (zoomed in)\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\nNote that these are all post-warmup draws. The brms package doesn’t make it easy to visualize the warmup draws. But we can do so with a little help from the ggmcmc package’s ggs() function.\n\n# first execute without summarise()\nggs(fit13.b) %&gt;% \n  summarise(min = min(Iteration),\n            max = max(Iteration))\n\n# A tibble: 1 × 2\n    min   max\n  &lt;int&gt; &lt;int&gt;\n1     1  2000\n\n\nNote how how the values in the Iteration column range from 1 to 2,000. By brms default, the first 1,000 of those iterations are the warmup’s. Here is how we can use the ggs() output to make trace plots that include the warmup draws.\n\nggs(fit13.b) %&gt;% \n  filter(Parameter != \"lprior\") %&gt;% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  # this marks off the warmups\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  labs(title = \"More hand-made trace plots\",\n       subtitle = \"warmup/post-warmup by background\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\nLet’s take a closer look at the first few warmup iterations.\n\nggs(fit13.b) %&gt;% \n  filter(Parameter != \"lprior\") %&gt;% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 2/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  coord_cartesian(xlim = c(0, 50)) +\n  labs(title = \"More hand-made trace plots (zoomed in)\",\n       subtitle = \"warmup only\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\") + plot_aes\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Can you use the results here to describe the need for discarding warmup draws?\nThe warmup draws are not representative of the posterior distribution. The warmup draws are used to help the HMC algorithm find a good starting point for the post-warmup draws. The post-warmup draws are what we use to make inferences about the posterior distribution.\nAnother issue is autocorrelation, the degree to which a given HMC draw is correlated with the previous draw(s). We can make a plot of the autocorrelations with the mcmc_acf() function from the bayesplot package.\n\nfit13.b %&gt;% \n  mcmc_acf(pars = vars(b_Intercept, b_x, sigma),\n           lags = 10)  # lags = 20 is the default\n\n\n\n\n\n\n\n\nThis is what we like to see: Nice L-shaped autocorrelation plots. Low autocorrelations like this are one of the major achievements of Stan’s implementation of HMC. It’s not uncommon for MCMC via the older Gibbs sampler method to routinely show much higher autocorrelations. You can get a sense of this by comparing the various models in Kruschke’s (2015) textbook, which often uses the Gibbs sampler, versus their brms() analogues in my (2023) ebook translation.\n\n\n\n\n\n\nNote\n\n\n\nMixing describes how efficiently MCMC chains explore the posterior distribution. Good mixing means samples move freely across the parameter space. And high autocorrelation =&gt; poor mixing.\n\n\n\n\nQuestion 3: Why are L-shaped autocorrelation plots are desirable? What would an undesirable autocorrelation plot look like?\nL-shaped autocorrelation plots are desirable because they indicate that the MCMC chains are mixing well and exploring the parameter space efficiently. In contrast, an undesirable autocorrelation plot would show high autocorrelations at many lags, indicating that the samples are highly correlated and not effectively exploring the posterior distribution.\nThose low autocorrelations also have a lot to do with our effective sample size (ESS) estimates. Take another look at the summary() output.\n\nsummary(fit13.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere used to be a single ESS column. Starting with version 2.10.0, brms returns two columns: Bulk_ESS and Tail_ESS. These originate from Vehtari et al (2019). From the paper, we read:\n\nWhen reporting quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3, we show that convergence of Markov chains is not uniform across the parameter space, that is, convergence might be different in the bulk of the distribution (e.g., for the mean or median) than in the tails (e.g., for extreme quantiles). We propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as bulk-ESS), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (pp. 672-673)\n\nWe generally like the values in both the Bulk_ESS and Tail_ESS columns to be as close to the total number of post-warmup draws as possible, which would be 4,000 for a default brm() model. Sometimes, as in the case of the Bulk_ESS value for our \\(\\beta_1\\) parameter, the HMC chains are so efficient that we can get larger numbers than the actual number of post-warmup draws. This is related to when we have negative autocorrelations (see above).\nHow much is enough, and how low is too low? Yeah, indeed… Higher is generally better, with diminishing returns rolling in somewhere between 1,000 and 10,000. brms will give you a warning message when the ESS estimates get below a couple hundred.\nNow look back at the Rhat column in the summary() output. This is the potential scale reduction factor \\(\\hat R\\). It has its origins in Gelman & Rubin (1992), but the current version used in brms is from Vehtari et al (2019), as cited above. In short, it is something of a ratio of the between-chain variation versus the within-chain variation. This ratio is usually a little above 1, and we want it to be as close to 1 as possible. The Stan team (e.g., https://mc-stan.org/rstan/reference/Rhat.html) recommends against values greater than 1.05. In our case, we’re good to go."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#what-bad-chains-look-like..",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#what-bad-chains-look-like..",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "What bad chains look like..",
    "text": "What bad chains look like..\nNow let’s break the model. This time, we’ll subset the d data to just the first 2 rows, we’ll make the priors very wide on the scale of the data, and we’ll dramatically reduce the warmup period.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit14b.rds'\n\nif (!file.exists(model_path)) {\n\nfit14.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this for real data analyses\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 1100, warmup = 100, chains = 4, cores = 10\n)\n\nsaveRDS(fit14.b,model_path)\n} else {\n  fit14.b &lt;- readRDS(model_path)\n}\n\nCheck the parameter summary.\n\nprint(fit14.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %&gt;% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 1100; warmup = 100; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   478.83   2036.34 -1979.74  6652.81 1.46        8       13\nx         -1228.64   3124.10 -6776.94  5668.31 1.88        6       15\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma  6231.76  11923.16   165.35 42287.98 1.13       22      336\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNever ignore Warning messages like that.\nThose Rhat, Bulk_ESS, and Tail_ESS look really bad. Also notice how large the posterior means (Estimate) and standard deviations (Est.Error) are. Seems off, eh?\nLet’s investigate further with a pairs() plot.\n\nplot(fit14.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nThis is a full-scale disaster. DO NOT trust model results from chains that look like this.\nIn this case, just giving the model a longer warmup period helped a lot.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit15b.rds'\n\nif (!file.exists(model_path)) {\n\nfit15.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this in real life\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 10\n)\n\nsaveRDS(fit15.b,model_path)\n} else {\n  fit15.b &lt;- readRDS(model_path)\n}\n\n\nplot(fit15.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nWe still have a lot of Warning messages, but things have improved.\nWe can do an even better with default weakly-regularizing priors.\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit16b.rds'\n\nif (!file.exists(model_path)) {\n\nfit16.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n)\n\nsaveRDS(fit16.b,model_path)\n} else {\n  fit16.b &lt;- readRDS(model_path)\n}\n\n\nplot(fit16.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nNow look at the parameter summaries.\n\nprint(fit16.b)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %&gt;% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.23      0.49    -1.21     0.83 1.00     1810     1643\nx             0.47      0.99    -1.46     2.38 1.00     1955     1932\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.86      0.59     0.19     2.45 1.00     1065      982\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThose Warning messages still remain, but they’re less dire than before. Also, most of the other diagnostics look better. I still wouldn’t trust this model. It is only based on 2 data points, after all. But look how far we got by paying attention to the diagnostics and picking better priors."
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#references",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#references",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "References",
    "text": "References\nGelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences. Statistical Science, 7(4):457–472. https://dx.doi.org/10.1214/ss/1177011136\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\nKurz, A. S. (2023). Doing Bayesian data analysis in brms and the tidyverse (Version 1.1.0). https://bookdown.org/content/3686/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved \\(\\widehat R\\) for assessing convergence of MCMC (with discussion). Bayesian Analysis, 16(2), 667-718. https://doi.org/10.1214/20-BA1221"
  },
  {
    "objectID": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#session-information",
    "href": "posts/Lab-10/Bayes_Lab_3_2_HMC Diagnostics.html#session-information",
    "title": "Lab 10.2 (HMC Diagnostics)",
    "section": "Session information",
    "text": "Session information\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggmcmc_1.5.1.1   GGally_2.2.1     faux_1.2.2       moderndive_0.7.0\n [5] bayesplot_1.11.1 ggdist_3.3.2     tidybayes_3.0.7  brms_2.21.0     \n [9] Rcpp_1.0.13      lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.4      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  pacman_0.5.1    \n\nloaded via a namespace (and not attached):\n  [1] infer_1.0.7          RColorBrewer_1.1-3   tensorA_0.36.2.1    \n  [4] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [7] estimability_1.5.1   farver_2.1.2         nloptr_2.1.1        \n [10] rmarkdown_2.28       vctrs_0.6.5          minqa_1.2.8         \n [13] base64enc_0.1-3      janitor_2.2.1        htmltools_0.5.8.1   \n [16] distributional_0.5.0 curl_5.2.2           broom_1.0.7         \n [19] StanHeaders_2.32.10  htmlwidgets_1.6.4    plyr_1.8.9          \n [22] emmeans_1.10.7       zoo_1.8-12           igraph_2.0.3        \n [25] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n [28] colourpicker_1.3.0   Matrix_1.7-0         R6_2.5.1            \n [31] fastmap_1.2.0        rbibutils_2.3        shiny_1.9.1         \n [34] snakecase_0.11.1     digest_0.6.37        colorspace_2.1-1    \n [37] crosstalk_1.2.1      labeling_0.4.3       fansi_1.0.6         \n [40] timechange_0.3.0     abind_1.4-5          compiler_4.4.1      \n [43] withr_3.0.1          backports_1.5.0      inline_0.3.19       \n [46] shinystan_2.6.0      ggstats_0.9.0        QuickJSR_1.3.1      \n [49] pkgbuild_1.4.4       MASS_7.3-60.2        gtools_3.9.5        \n [52] loo_2.8.0            tools_4.4.1          httpuv_1.6.15       \n [55] threejs_0.3.3        glue_1.8.0           nlme_3.1-164        \n [58] promises_1.3.0       grid_4.4.1           checkmate_2.3.2     \n [61] reshape2_1.4.4       generics_0.1.3       operator.tools_1.6.3\n [64] gtable_0.3.5         tzdb_0.4.0           formula.tools_1.7.1 \n [67] hms_1.1.3            utf8_1.2.4           pillar_1.9.0        \n [70] markdown_1.13        posterior_1.6.0      later_1.3.2         \n [73] splines_4.4.1        lattice_0.22-6       survival_3.6-4      \n [76] tidyselect_1.2.1     miniUI_0.1.1.1       knitr_1.48          \n [79] reformulas_0.4.0     arrayhelpers_1.1-0   gridExtra_2.3       \n [82] V8_6.0.1             stats4_4.4.1         xfun_0.52           \n [85] rstanarm_2.32.1      bridgesampling_1.1-2 matrixStats_1.4.1   \n [88] DT_0.33              rstan_2.32.6         stringi_1.8.4       \n [91] yaml_2.3.10          boot_1.3-30          evaluate_1.0.0      \n [94] codetools_0.2-20     cli_3.6.4            RcppParallel_5.1.9  \n [97] shinythemes_1.2.0    xtable_1.8-4         Rdpack_2.6.2        \n[100] munsell_0.5.1        coda_0.19-4.1        svUnit_1.0.6        \n[103] parallel_4.4.1       rstantools_2.4.0     dygraphs_1.1.1.6    \n[106] Brobdingnag_1.2-9    lme4_1.1-36          viridisLite_0.4.2   \n[109] mvtnorm_1.3-1        scales_1.3.0         xts_0.14.1          \n[112] rlang_1.1.5          shinyjs_2.1.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats-blog (PSY-504)",
    "section": "",
    "text": "Lab-8: Bayes\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to MLM Exercise/Walkthrough\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nLab-7\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nLab 9\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project: Prediciting Head to Head Pokémon Wins with Multi-level Binary Logistic Regression\n\n\nPsy-504\n\n\n\nFinal-Project\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2: Logistic Regression\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Lab Answers\n\n\nPrinceton University\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4: Multinomial Regression\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Regression Lab Answers\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nMissings Data Lab\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\nMissing Data\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nLab 10.2 (HMC Diagnostics)\n\n\nPinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\nbayes\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nPriors and Predicitive Checks\n\n\nPrinceton University\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\nbayes\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data\n\n\nPsy-504\n\n\n\nLab\n\n\ncode\n\n\nanalysis\n\n\nmissing-data\n\n\n\n\n\n\n\n\n\nApr 29, 2025\n\n\nSteven Mesquiti\n\n\n\n\n\n\nNo matching items"
  }
]