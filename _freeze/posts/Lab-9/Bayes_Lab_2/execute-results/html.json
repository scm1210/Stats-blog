{
  "hash": "af1f4dfe4111fa7756f84f6df0803a18",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 9\"\nsubtitle: \"Princeton University\"\nauthor: \"Steven Mesquiti\"\noutput: \n  tufte::tufte_html:\n    css: \n    tufte_variant: \"envisioned\"\n    highlight: github-dark\n    fig_height: 10\n    fig_width: 16\n    toc: true\n    toc_depth: 1\nexecute: \n  message: false\n  warning: false\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\nformat: html\nengine: knitr\ncategories: [Lab, code, analysis]\n---\n\n\n\nFor Lab 1, you had explored the data and looked at models built via lm() and via brms(using default priors). You had also drawn posterior samples after fitting the model.\n\nFor Lab 2, we continue with the Palmer Penguins. And we will look more at distributions and priors.\n\nAgain, there will be conceptual questions to answer as you work through this example, and exercises.\n\n# Part 3: Distributions all the way down\n\nGiven it's a continuation of Lab 1, let's begin by loading relevant packages, cleaning/pre-processing the data, and fitting lm() and the default brm models\n\n## Setup: Packages and data\n\nWe load the primary packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\npacman::p_load(tidyverse,brms,tidybayes,ggdist,install = T)\n\npalette <- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n```\n:::\n\n\n\nWe want the same data set up as in the last lab.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the penguins data\ndata(penguins, package = \"palmerpenguins\")\n\n# subset the data\nchinstrap <- penguins %>% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap) |> \n  DT::datatable()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 68\nColumns: 8\n$ species           <fct> Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            <fct> Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    <dbl> 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     <dbl> 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm <int> 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       <int> 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               <fct> female, male, male, female, male, female, female, ma…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-89f006a138d94ad862ed\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-89f006a138d94ad862ed\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\"],[\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\"],[\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\"],[46.5,50,51.3,45.4,52.7,45.2,46.1,51.3,46,51.3,46.6,51.7,47,52,45.9,50.5,50.3,58,46.4,49.2,42.4,48.5,43.2,50.6,46.7,52,50.5,49.5,46.4,52.8,40.9,54.2,42.5,51,49.7,47.5,47.6,52,46.9,53.5,49,46.2,50.9,45.5,50.9,50.8,50.1,49,51.5,49.8,48.1,51.4,45.7,50.7,42.5,52.2,45.2,49.3,50.2,45.6,51.9,46.8,45.7,55.8,43.5,49.6,50.8,50.2],[17.9,19.5,19.2,18.7,19.8,17.8,18.2,18.2,18.9,19.9,17.8,20.3,17.3,18.1,17.1,19.6,20,17.8,18.6,18.2,17.3,17.5,16.6,19.4,17.9,19,18.4,19,17.8,20,16.6,20.8,16.7,18.8,18.6,16.8,18.3,20.7,16.6,19.9,19.5,17.5,19.1,17,17.9,18.5,17.9,19.6,18.7,17.3,16.4,19,17.3,19.7,17.3,18.8,16.6,19.9,18.8,19.4,19.5,16.5,17,19.8,18.1,18.2,19,18.7],[192,196,193,188,197,198,178,197,195,198,193,194,185,201,190,201,197,181,190,195,181,191,187,193,195,197,200,200,191,205,187,201,187,203,195,199,195,210,192,205,210,187,196,196,196,201,190,212,187,198,199,201,193,203,187,197,191,203,202,194,206,189,195,207,202,193,210,198],[3500,3900,3650,3525,3725,3950,3250,3750,4150,3700,3800,3775,3700,4050,3575,4050,3300,3700,3450,4400,3600,3400,2900,3800,3300,4150,3400,3800,3700,4550,3200,4300,3350,4100,3600,3900,3850,4800,2700,4500,3950,3650,3550,3500,3675,4450,3400,4300,3250,3675,3325,3950,3600,4050,3350,3450,3250,4050,3800,3525,3950,3650,3650,4000,3400,3775,4100,3775],[\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\"],[2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>species<\\/th>\\n      <th>island<\\/th>\\n      <th>bill_length_mm<\\/th>\\n      <th>bill_depth_mm<\\/th>\\n      <th>flipper_length_mm<\\/th>\\n      <th>body_mass_g<\\/th>\\n      <th>sex<\\/th>\\n      <th>year<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5,6,8]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"species\",\"targets\":1},{\"name\":\"island\",\"targets\":2},{\"name\":\"bill_length_mm\",\"targets\":3},{\"name\":\"bill_depth_mm\",\"targets\":4},{\"name\":\"flipper_length_mm\",\"targets\":5},{\"name\":\"body_mass_g\",\"targets\":6},{\"name\":\"sex\",\"targets\":7},{\"name\":\"year\",\"targets\":8}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Models\n\nOnce again, we'll fit the model\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) ,\n\\end{align}\n$$\n\nwith both `lm()` and `brm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS\nfit1.ols <- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n\n#define model path \n\nmodel_path <- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/fit1b.rds\")\n\nif (!file.exists(model_path)) {\nfit1.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit1.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit1.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\n## Bayesians have many kinds of distributions\n\nIn Bayesian statistics, we have at least 6 distributions to keep track of. Those are:\n\n-   the likelihood distributions\n-   the prior parameter distribution (aka priors)\n-   the prior predictive distributions\n-   the posterior parameter distributions (aka posteriors)\n-   the posterior-predictive distribution\n\nIn many respect, it's distributions 'all the way down,' with Bayesians. This can be indeed be difficult to keep track of at first. But since this is true for any class of Bayesian models (not just regression), you'll hopefully get used to it.\\\n\n### QUESTION 1: How would you represent these 6 distributions mathematically, using $P_0$'$P$, $D$, $|$, and $\\theta$ ?\n\n::: callout-tip\nHint 1: Many of these terms were in the Bayes Rule.\n:::\n\n### Answer: ....\n\n## Mathematical Representations\n\n1.  **Likelihood Distributions**: The likelihood represents the probability of the observed data, given the model parameters: $\\[ P(D \\| \\theta) \\]$\n\n2.  **Prior Parameter Distribution (Priors)**: The prior distribution reflects our belief about the parameters before observing the data: $\\[ P(\\theta) \\]$\n\n3.  **Prior Predictive Distribution**: This distribution represents the probability of the data before seeing any observations, based on the prior belief about the parameters: $\\[ P(D \\| P(\\theta)) \\]$\n\n4.  **Posterior Parameter Distribution (Posteriors)**: After observing the data, the posterior distribution represents our updated belief about the parameters. Using Bayes' theorem, it is given by: $\\[ P(\\theta \\| D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} \\]$\n\n5.  **Posterior Predictive Distribution**: This distribution gives the probability of new data points, based on the posterior distribution of the parameters: $\\[ P(D' \\| D) = \\int P(D' \\| \\theta) P(\\theta \\| D) d\\theta \\]$ We also have some other distributions that follow from these. For example, - the distributions of the model expectations (i.e., the predicted means)\n\n### Likelihood distributions.\n\nWe are approaching Bayesian statistics from a likelihood-based perspective. That is, we situate regression models within the greater context of a likelihood function. (There are ways to do non-parametric Bayesian statistics, which don't focus on likelihoods. We won't get into that right now.)\n\nSo far, we have been using the conventional Gaussian likelihood. If we have some variable $y$, we can express it as normally distributed by\n\n$$\n\\operatorname{Normal}(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{1}{2} \\left( \\frac{y - \\mu}{\\sigma}\\right)^2\\right),\n$$\n\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation. With this likelihood,\n\n-   $\\mu \\in \\mathbb R$\n    -   the mean can be any real number, ranging from $-\\infty$ to $\\infty$\n-   $\\sigma \\in \\mathbb R_{> 0}$\n    -   the standard deviation can take on any real number greater than zero.\n\nIt's also the assumption\n\n-   $y \\in \\mathbb R$\n    -   the focal variable $y$ can be any real number, ranging from $-\\infty$ to $\\infty$.\n\nOne of the ways we wrote our model formula back in the first file was\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n$$\n\nand further in the discussion, we updated that equation with the posterior means for our three parameters to\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, 2.92) \\\\\n\\mu_i & = 32.2 + 0.004 \\text{body_mass_g}_i.\n\\end{align}\n$$\n\nBefore we get into this, though, let's back up and consider an intercept-only model of the form\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 ,\n\\end{align}\n$$\n\nwhere there is no predictor variable. Here's how to fit the model with `brm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bayes\n\n\nmodel_path <- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/fit0b.rds\")\n\nif (!file.exists(model_path)) {\nfit0.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(fit0.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit0.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\nLet's look at the model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit0.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.21      3.49    25.42    38.97 1.00     4595     2924\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4626     2720\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.25     2.47     3.45 1.00     2010     2020\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThe intercept parameter $\\beta_0$ is a stand-in for $\\mu$. The $\\sigma$ parameter is just $\\sigma$. Here they are in a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(fit0.b) \n\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes + \n  scale_fill_manual(values = palette_condition) \n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nHere are the posterior means for those two parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(draws$b_Intercept)\nsigma <- mean(draws$sigma)\n\nmu; sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 32.21014\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.925302\n```\n\n\n:::\n:::\n\n\n\nWe can use `dnorm()` to compute the shape of $\\operatorname{Normal}(48.8, 3.4)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %>% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %>% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nWe can compare this to the sample distribution of the `bill_length_mm` data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nIt's not a great fit, but not horrible either.\n\nNow let's see what this means for our univariable model `fit1.b`. First, let's learn about the `posterior_summary()` function, which we'll use to save a few posterior means.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.211118e+01 3.4771611651  2.552389e+01  3.902608e+01\nb_body_mass_g  4.479243e-03 0.0009256637  2.642918e-03  6.245207e-03\nsigma          2.932229e+00 0.2496554440  2.499569e+00  3.471021e+00\nIntercept      4.883259e+01 0.3406152809  4.813486e+01  4.948863e+01\nlprior        -4.300245e+00 0.0678611888 -4.450849e+00 -4.187769e+00\nlp__          -1.722437e+02 1.1939186872 -1.753094e+02 -1.708890e+02\n```\n\n\n:::\n\n```{.r .cell-code}\nb0    <- posterior_summary(fit1.b)[1, 1]\nb1    <- posterior_summary(fit1.b)[2, 1]\nsigma <- posterior_summary(fit1.b)[3, 1]\n```\n:::\n\n\n\nNow we plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %>% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nOur univariable model `fit1.b` can be viewed as something like a 3-dimensional Gaussian hill.\n\n### Prior distributions & Prior predictive distributions.\n\nLet's hold off on this for a bit.\n\n### Parameter distributions.\n\nUp above, we plotted the posterior distributions for our intercept-only `fit0.b` model. Here they are again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nWe might practice making a similar plot for our univariable model `fit1.b`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit1.b) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nSome authors, like John Kruschke, have a strong preference for plotting their posteriors with histograms, rather than density plots.\n\n## Distributions of the model expectations.\n\nTake another look at the `conditional_effects()` plot from earlier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit1.b) %>% \n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nThe blue line is the posterior mean, for the $\\mu_i$, the model-based mean for `bill_length_mm`, given the value for the predictor `body_mass_g`. The semitransparent gray ribbon marks the percentile-based interval for the conditional mean.\n\nWe can make a similar plot with the `fitted()` function. First we'll need a predictor grid, we'll call `nd`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- tibble(body_mass_g = seq(\n  from = min(chinstrap$body_mass_g),\n  to = max(chinstrap$body_mass_g),\n  length.out = 100))\n\nglimpse(nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 1\n$ body_mass_g <dbl> 2700.000, 2721.212, 2742.424, 2763.636, 2784.848, 2806.061…\n```\n\n\n:::\n:::\n\n\n\nNow pump `nd` into the `fitted()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.20513 1.0197075 42.24705 46.17349\n[2,] 44.30015 1.0012213 42.37850 46.23949\n[3,] 44.39516 0.9827796 42.51240 46.29758\n[4,] 44.49018 0.9643851 42.64621 46.35822\n[5,] 44.58519 0.9460405 42.77985 46.42049\n[6,] 44.68020 0.9277487 42.91383 46.49053\n```\n\n\n:::\n:::\n\n\n\nNow plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nLook what happens if we augment the `probs` argument in `fitted()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nNow look what happens if we set `summary = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:100] 44.4 43.9 44.1 44.5 46.7 ...\n```\n\n\n:::\n:::\n\n\n\nWe get full 4,000 draw posterior distributions for each of the 100 levels of the predictor `body_mass_g`. Now look at what happens if we wrangle that output a little, and plot with aid from `stat_lineribbon()` from the **ggdist** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  stat_lineribbon() +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nLook what happens when we request more intervals in the `.width` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nThe conditional mean, $\\mu_i$, has its own distribution. We can take this visualization approach even further to make a color gradient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nFor technical details on this visualization approach, go here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-gradients>.\n\nThe **ggdist** package even has an experimental visualization approach that's based on density gradients, rather than interval-width gradients. Since this is experimental, I'm not going to go into the details. But if you're curious and adventurous, you can learn more here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-density-gradients>.\n\n### Posterior-predictive distributions.\n\nThe last section showed the posterior distributions for the model expectations (i.e., the conditional means). In the context of the Gaussian distribution, that's $\\mu$, or $\\mu_i$ in the case of the univariable model `fit1.b`. But the whole Gaussian distribution includes $\\mu$ and $\\sigma$.\n\nThis is where the `predict()` function comes in. First, we compare the `fitted()` output to `predict()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.20513 1.0197075 42.24705 46.17349\n[2,] 44.30015 1.0012213 42.37850 46.23949\n[3,] 44.39516 0.9827796 42.51240 46.29758\n[4,] 44.49018 0.9643851 42.64621 46.35822\n[5,] 44.58519 0.9460405 42.77985 46.42049\n[6,] 44.68020 0.9277487 42.91383 46.49053\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.17269  3.119825 38.04113 50.21742\n[2,] 44.37479  3.155426 37.96836 50.48929\n[3,] 44.40880  3.071269 38.66573 50.38529\n[4,] 44.46118  3.080088 38.24711 50.43833\n[5,] 44.61212  3.105005 38.47867 50.87375\n[6,] 44.71355  3.103898 38.64486 50.96648\n```\n\n\n:::\n:::\n\n\n\nThe posterior means (`Estimate`) are about the same, but the SD's (`Est.Error`) are much larger in the `predict()` output, and the widths of the 95% intervals are too. Let's make a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nThe gray band is the 95% interval for the entire posterior predictive distribution, not just the mean. In a good model, about 95% of the data points should be within those bands.\n\nDiscuss how the jagged lines have to do with the uncertainty in $\\sigma$.\n\nIf we wanted to, we could integrate the `fitted()`-based conditional posterior mean, with the `predict()`-based posterior-predictive distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save the fitted() results\nf <- fitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) \n\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nIt's the posterior predictive distribution that we use to predict new data points. For example, here's what happens if we use `predict()` without the `newdata` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 47.76025  2.949676 42.06117 53.56949\n[2,] 49.59141  2.932069 43.90961 55.37971\n[3,] 48.38616  2.915855 42.66543 54.07587\n[4,] 47.87208  2.971567 41.98423 53.79133\n[5,] 48.81092  2.958257 42.87191 54.60564\n[6,] 49.76666  2.921512 44.09992 55.48069\n```\n\n\n:::\n:::\n\n\n\nWe get posterior predictive summaries for each of the original data points. Here's what happens if we set `summary = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:68] 44.8 46.3 52 47.8 45.2 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n\nThis time, we got 4,000 posterior draws for each. We can reduce that output with the `ndraws` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:6, 1:68] 46.9 49.2 52 43.6 51.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n\nNow wrangle and plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  data.frame() %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(row = str_remove(name, \"X\") %>% as.double()) %>% \n  left_join(chinstrap %>% \n              mutate(row = 1:n()),\n            by = join_by(row)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\nWith `predict()`, we can use the entire posterior-predictive distribution to simulate new data based on the values of our predictor variable(s). To give you a better sense of what's happening under the hood, here's an `as_draws_df()` based alternative.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 6) %>% \n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\nNow take a look at what happens when we plot the densities of several simulated draws.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 50) %>%  # increase the number of random draws\n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(chinstrap$bill_length_mm) + c(-2, 2)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\nThe similarities and differences among the individual density lines give you a sense of the (un)certainty of the posterior-predictive distribution.\n\n**This may be a good time for you to work on Exercise 1 (see end of the document)**\n\n#Part 4: Beginning to look at priors\n\n## Bayes' rule\n\nBayes' theorem will allow us to determine the plausibility of various values of our parameter(s) of interest, $\\theta$, given the data $d$, which we can express formally as $\\Pr(\\theta \\mid d)$. Bayes' rule takes on the form\n\n$$\n\\Pr(\\theta \\mid d) = \\frac{\\Pr(d \\mid \\theta) \\Pr(\\theta)}{\\Pr(d)}.\n$$\n\nwhere\n\n-   $\\Pr(d \\mid \\theta)$ is the *likelihood*,\n-   $\\Pr(\\theta)$ is the *prior*,\n-   $\\Pr(d)$ is the *average probability of the data*, and\n-   $\\Pr(\\theta \\mid d)$ is the *posterior*.\n\nWe can express this in words as\n\n$$\n\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\n$$\n\nThe denominator $\\Pr(d)$ is a normalizing constant, and dividing by this constant is what converts the posterior $\\Pr(\\theta \\mid d)$ into a probability metric.\n\n## Default priors\n\nTo set your priors with **brms**, the `brm()` function has a `prior` argument. If you don't explicitly use the `prior` argument, `brm()` will use default priors. This is what happened with our `fit1.b` model from above. We used default priors. If you'd like to see what those priors are, execute `fit1.b$prior`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# maybe show str(fit1.b)\nfit1.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n\nThus, a fuller expression of our model is\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i \\\\\n\\beta_0 & \\sim \\operatorname{Student-t}(3, 49.5, 3.6) \\\\\n\\beta_1 & \\sim \\operatorname{Uniform}(-\\infty, \\infty) \\\\\n\\sigma & \\sim \\operatorname{Student-t}^+(3, 0, 3.6).\n\\end{align}\n$$\n\nIf we had wanted to see the `brm()` defaults before fitting the model, we could have used the `get_prior()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n\nIf you recall, the normal distribution is a member of the Student-t family, where the $\\nu$ (aka degrees of freedom or normality parameter) is set to $\\infty$. To give you a sense, here are the densities of three members of the Student-t family, with varying $\\nu$ values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(theta = seq(from = -4.5, to = 4.5, length.out = 200),\n         nu = c(3, 10, Inf)) %>% \n  mutate(density = dt(x = theta, df = nu)) %>% \n  \n  ggplot(aes(x = theta, y = density, color = factor(nu))) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(expression(nu), option = \"A\", end = .7) +\n  labs(title = \"3 members of the Student-t family\",\n       x = expression(theta)) +\n  coord_cartesian(xlim = c(-4, 4)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\nThus, Student-t distributions have thicker tails when they have smaller $\\nu$ parameters. In the case where $\\nu = 3$, the tails are pretty thick, which means they are more tolerant of more extreme values. And thus priors with small-$\\nu$ parameters will be weaker (i.e., more permissive) than their Gaussian counterparts.\n\nWe can visualize functions from **ggdist** to visualize the default `brm()` priors. We'll start with the `student_t(3, 49.5, 3.6)` $\\beta_0$ prior, and also take the opportunity to compare that with a slightly stronger `normal(49.5, 3.6)` alternative.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 49.5, 3.6)),\n  prior(normal(49.5, 3.6))) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\nSee how that $n = 3$ parameter in the default prior let do much thicker tails than it's Gaussian counterpart. We can make the same kind of plot for our default $\\sigma$ prior and its half-Gaussian counterpart.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 0, 3.6), lb = 0),  # note our use of the lb = 0 argument\n  prior(normal(0, 3.6), lb = 0)) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\nHere's how we could have explicitly set our priors by hand.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_path <- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/fit2b.rds\")\n\nif (!file.exists(model_path)) {\nfit2.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g,\n  prior = prior(student_t(3, 49.5, 3.6), class = Intercept) +\n    prior(student_t(3, 0, 3.6), class = sigma, lb = 0)\n)\n  saveRDS(fit2.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit2.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\nCompare the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.11      3.48    25.52    39.03 1.00     5495     2845\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5628     2873\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.25     2.50     3.47 1.00     1796     1802\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit2.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.22      3.48    25.26    38.86 1.00     4587     2988\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4608     2999\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.94      0.27     2.47     3.52 1.00     1894     1547\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n## QUESTION 2 Are the priors the same? What do you think is going on?\n\n### Answer: ....\n\n::: callout\nThe priors are not the same. The default priors for the intercept and sigma parameters in the `fit1.b` model are Student-t distributions with 3 degrees of freedom, while the `fit2.b` model has a normal distribution for the intercept and a half-normal distribution for sigma. The choice of priors can significantly affect the posterior distributions, especially when the sample size is small or when there are outliers in the data.\n:::\n\nIf you want to learn more about the default prior settings for **brms**, read through the `set_prior` section of the **brms** reference manual (https://CRAN.R-project.org/package=brms/brms.pdf).\n\n# EXERCISE 1\n\nIn the previous lab, we made a subset of the `penguins` data called `gentoo`, which was only the cases for which `species == \"Gentoo\"`. Do that again and refit the Bayesian model to those data. Remake some of the figures (From Part 3) in this file with the new version of the model?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the penguins data and filter only the gentoo\ngentoo <- penguins %>% \n  filter(species == \"Gentoo\")\n\ngentoo |> \n  head() |> \n  DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-3dd45c337b100eb7f683\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-3dd45c337b100eb7f683\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\"],[\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\"],[46.1,50,48.7,50,47.6,46.5],[13.2,16.3,14.1,15.2,14.5,13.5],[211,230,210,218,215,210],[4500,5700,4450,5700,5400,4550],[\"female\",\"male\",\"female\",\"male\",\"male\",\"female\"],[2007,2007,2007,2007,2007,2007]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>species<\\/th>\\n      <th>island<\\/th>\\n      <th>bill_length_mm<\\/th>\\n      <th>bill_depth_mm<\\/th>\\n      <th>flipper_length_mm<\\/th>\\n      <th>body_mass_g<\\/th>\\n      <th>sex<\\/th>\\n      <th>year<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5,6,8]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"species\",\"targets\":1},{\"name\":\"island\",\"targets\":2},{\"name\":\"bill_length_mm\",\"targets\":3},{\"name\":\"bill_depth_mm\",\"targets\":4},{\"name\":\"flipper_length_mm\",\"targets\":5},{\"name\":\"body_mass_g\",\"targets\":6},{\"name\":\"sex\",\"targets\":7},{\"name\":\"year\",\"targets\":8}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_path <- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-9/models/lab2_gentoo.rds\")\n\nif (!file.exists(model_path)) {\nlab2_gentoo <- brm(\n  data = gentoo,\n bill_length_mm ~ 1 + body_mass_g\n)\n  saveRDS(lab2_gentoo, model_path)\n} else {\n  lab2_gentoo <- readRDS(model_path)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(lab2_gentoo) \n\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed) + plot_aes + \n  scale_fill_manual(values = palette_condition) \n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngentoo_clean <- gentoo %>% filter(!is.na(body_mass_g))\n\n\nnd <- tibble(body_mass_g = seq(\n  from = min(gentoo_clean$body_mass_g),\n  to = max(gentoo_clean$body_mass_g),\n  length.out = 1000))\n\nf <- fitted(lab2_gentoo, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) \n\npredict(lab2_gentoo, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(gentoo_clean$bill_length_mm)) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n\n### Answer/ Your solution below: ....\n\n## References\n\nKruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press. <https://sites.google.com/site/doingbayesiandataanalysis/>\n\n## Session information\n",
    "supporting": [
      "Bayes_Lab_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}