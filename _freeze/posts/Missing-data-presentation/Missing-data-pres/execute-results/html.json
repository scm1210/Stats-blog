{
  "hash": "ae4f6abb9d32ffe934500fe632becc05",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Missing Data\"\nsubtitle: \"Psy-504\"\nauthor: \n  - Steven Mesquiti\ninstitute: \"Princeton University\"\ndate: today\nformat: \n  revealjs:\n    theme: [default, styles.scss]\n    highlight-style: github-dark\n    toc: true\n    toc-depth: 1\n    slide-number: true\n    fig-width: 8\n    fig-height: 5\n    transition: slide\ncategories: [Lab, code, analysis, missing-data]\nexecute:\n  message: false\n  warning: false\nparams:\n  SHOW_SOLS: true\n  TOGGLE: true\n---\n\n\n\n\n\n\n\n## Overview\n\n-   Importance of addressing missing data in research\\\n-   Traditional vs. modern methods\\\n-   Focus on:\n    -   Maximum Likelihood (ML)\\\n    -   Multiple Imputation (MI)\\\n-   Benefits of modern methods\\\n-   Demo using a toy dataset\n\n::: notes\nHi! Today im going to be talking to you today about missing data.\\\nMissing data is a common problem in research, and it can have a significant impact on the validity of your results.\\\nIn this presentation, I will discuss the importance of addressing missing data, traditional methods for handling it, and modern techniques like Maximum Likelihood and Multiple Imputation.\\\nI will also attempt to provide a demo using a toy dataset to illustrate these concepts.\\\n\nLet's get started\n:::\n\n------------------------------------------------------------------------\n\n## The Problem of Missing Data {.scrollable}\n\n-   Missing data is common in quantitative research\n-   Traditional methods (e.g., deletion, mean imputation) are often inadequate\n    -   Biased estimates\n    -   Reduced statistical power\n    \n<div style=\"text-align: center;\">\n  ![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExbTA0Ynp6Z21tcm5ubTBudzVzOGNsdnl2bHM0NnJjeXhoZnE5YnF2eiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ImHfnm01jqvostvlLP/giphy.gif)\n</div>\n    \n\n::: notes\nMissing data are ubiquitous in quantitative research studies.\\\nBecause of its pervasive nature, some methodologists have described missing data as “one of the most important statistical and design problems in research” (methodologist William Shadish, quoted in Azar, 2002, p. 70).\n:::\n\n------------------------------------------------------------------------\n\n## Different `Types` of Missing Data \n\n-   **Missing Completely at Random (MCAR)**: Missingness is unrelated to observed or unobserved data\n-   **Missing at Random (MAR)**: Missingness is related to observed data but not unobserved data\n-   **Missing Not at Random (MNAR)**: Missingness is related to unobserved data\n\n::: notes\nThere are theree different `types` of missing data. \\ \nThese mechanisms describe relationships between measured variables and the probability of missing data. While these terms have a precise probabilistic and mathematical meaning, they are essentially three different explanations for why the data are missing.\n\nCAR (Missing Completely at Random)\nDefinition: The missingness is entirely unrelated to any variables in the dataset, including the variable with missing values.\n\nIn Practice: Data are missing for reasons that are truly random.\n\nExample: A student moves to another district mid-study, and the move isn’t related to anything being measured (like grades or behavior).\n\nImplication: Analyses remain unbiased if MCAR is true, but it's a rare scenario.\n\nMAR (Missing at Random)\nDefinition: The missingness is related to other measured variables but not to the missing variable’s own values.\n\nIn Practice: You can predict the missingness based on other known data.\n\nExample: Students who use substances more often are more likely to miss school and therefore miss a self-esteem questionnaire. The missing self-esteem data is related to substance use (which is observed), not self-esteem itself.\n\nImplication: Common in real-world data. Methods like multiple imputation or maximum likelihood can handle MAR well.\n\nMNAR (Missing Not at Random)\nDefinition: The missingness depends on the missing values themselves.\n\nIn Practice: The reason data is missing is due to what the missing value would have been.\n\nExample: Poor readers skip hard test items because they don’t understand them—so the missing responses depend on their actual (low) reading ability.\n\nImplication: This is the most difficult type to deal with statistically, because the missingness contains information that is unobserved.\n:::\n\n---\n\n## Building and deploying example {.scrollable}\n\n::: panel-tabset\n### Create `Toy` Dataset\n\nFirst, let's create our toy dataset\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 1000  # Number of observations\ntoy <- data.frame(\n  x = rnorm(n, mean = 0, sd = 5),  \n  y = rnorm(n, mean = 0, sd = 7),  \n  z = sample(0:3, n, replace = TRUE)\n)\n\n# Introduce missingness in `y`\ntoy$y[sample(1:n, 300)] <- NA  # 300 missing values randomly assigned to `y`\n```\n:::\n\n\n\n### Visualize Missing Data\n\n-   visualize with `gg_miss_var` from the `naniar` package\n-   as you can see we have missing data in the `y` variable\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Missing_data/fig_unnamed-chunk-3-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n:::\n\n::: notes\nFirst, we'll create our synthetic dataset using hte code provided.\\\nThe dataset contains three variables: `x`, `y`, and `z`. and y has \\~300 missing values.\\\n\nWhat are some ways in which we can deal with them?\n:::\n\n------------------------------------------------------------------------\n\n## Traditional Missing Data Techniques\n\n-   **Listwise Deletion**: Drops any *row* with NA (e.g., `na.rm = T`)\\\n-   **Mean Imputation**: Replaces missing values with the mean value\\\n-   These are easy to use, but often leads to **biased** results, let's see why\n\n::: notes\nTraditionally, people use things like listwise deletion or mean imputation to deal with missing data.\\\nBut this can create problems because it'll reduce power (if you remove observations from your dataset) and can also introduce bias (if you replace missing values with the mean). Let's see how we'd implement this in R\n:::\n\n------------------------------------------------------------------------\n\n## How to implement Mean Imputation {.scrollable}\n\n-   Here, we are asking R to impute the mean of `y` and replace any missing values with that mean\\\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean imputation\ntoy$y_mean <- ifelse(is.na(toy$y), mean(toy$y, na.rm = TRUE), toy$y)\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Visualizing Mean Imputation {.scrollable}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Missing_data/fig_unnamed-chunk-5-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n::: notes\n*   Visualize the original data and the imputed data to compare the distributions\n*   Notice the thick line at the mean\n:::\n\n\n\n------------------------------------------------------------------------\n\n## How does this influence downstream processes? {.scrollable}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Missing_data/fig_unnamed-chunk-6-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n::: notes\n*   Now, let's see how this affects our regression model. \n*   The difference seems neligible (at least with working with big data) but the mean imputation is biased\n*   In smaller datasets mean imputation can have larger downstream consequences\n\n:::\n\n------------------------------------------------------------------------\n\n## Modern Method: Multiple Imputation (MI)\n\n-   Imputes multiple plausible values\\\n-   Models missingness using relationships among variables\\\n-   Pools results for accurate estimates and standard errors\\\n-   More robust than traditional methods\n\n------------------------------------------------------------------------\n\n## Multiple Imputation with `MICE` Package {.scrollable}\n\n::: notes\nMice stands for Multiple Imputation with Chained Equations\n:::\n\n::: panel-tabset\n## Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiple imputation\nimp <- mice(toy[, c(\"x\", \"y\", \"z\")], m = 5, method = \"pmm\", seed = 42,printFlag = F)\nfit_mi <- with(imp, lm(y ~ x + z))\npooled_summary = summary(pool(fit_mi))\n\npooled_summary |> \n  as.data.frame() |>\n  mutate_if(is.numeric, round, 3)  |> \n  DT::datatable(options = list(pageLength = 10, autoWidth = TRUE), \n                rownames = FALSE)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-289f59430a0724b372cb\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-289f59430a0724b372cb\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"(Intercept)\",\"x\",\"z\"],[-0.002,0.059,-0.019],[0.655,0.045,0.297],[-0.003,1.317,-0.063],[8.154,286.893,10.993],[0.998,0.189,0.951]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>term<\\/th>\\n      <th>estimate<\\/th>\\n      <th>std.error<\\/th>\\n      <th>statistic<\\/th>\\n      <th>df<\\/th>\\n      <th>p.value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":10,\"autoWidth\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5]},{\"name\":\"term\",\"targets\":0},{\"name\":\"estimate\",\"targets\":1},{\"name\":\"std.error\",\"targets\":2},{\"name\":\"statistic\",\"targets\":3},{\"name\":\"df\",\"targets\":4},{\"name\":\"p.value\",\"targets\":5}],\"order\":[],\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Explanation\n\n**What is this code doing?**\n\n-   Step 1: Use the `mice()` function to create 5 imputed datasets for the variables x, y, and z\n-   Step 2: Fit a linear model (y \\~ x + z) on each imputed dataset using x`with()`.\n-   Step 3: Combine the results across all models using `pool()` to account for variability between imputations\n:::\n\n------------------------------------------------------------------------\n\n## Plotting Imputed Datasets produced from MICE\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Missing_data/fig_unnamed-chunk-8-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Modern Method: Maximum Likelihood (ML) {.scrollable}\n\n*   MLE doesn't fill in missing values.\n\n*   Instead, it finds the parameter values (e.g., mean, regression coefficients) that make the observed data most probable.\n\n*   Based on the log likelihood function – it chooses parameters that minimize the distance between the model and the data.\n\n**Requires:**\n\n*   Assumes MAR\n\n*   Assumes multivariate normality\n\n* The under the hood math\n\n$$\n\\begin{align}\n\\log L &= \\sum_{i=1}^{N} \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{1}{2} \\left( \\frac{y_i - \\mu}{\\sigma} \\right)^2 \\right) \\right)\n\\end{align}\n$$\n\n::: notes\n\nLike multiple imputation, this approach assumes multivariate normality and MAR data. However, the mechanics of maximum likelihood are quite different. Rather than filling in the missing values, maximum likelihood uses all of the available data – complete and incomplete – to identify the parameter values that have the highest probability of producing the sample data. At a broad level, maximum likelihood estimation identifies the population parameter values that have the highest probability of producing the sample data. This estimation process uses a mathematical function called a log likelihood to quantify the standardized distance between the observed data points and the parameters of interest (e.g., the mean), and the goal is to identify parameter estimates that minimize these distances. This is conceptually similar to ordinary least squares estimation, where the goal is to identify the regression coefficients that minimize the collective distances between the data points and the predicted scores. The difference is that maximum likelihood uses all of the available data – complete and incomplete – to identify the parameter values that have the highest probability of producing the sample data. \n:::\n\n---\n\n## Using Auxiliary Variables\n\n-   Auxiliary variables can help improve imputation\\\n-   These are variables that are not of primary interest but can help explain the missingness\\\n-   Improves imputation quality and reduces bias\n\n------------------------------------------------------------------------\n\n## Implementation Example {.scrollable}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add auxiliary variable\ntoy$aux <- toy$x + rnorm(n)\ntoy$y[sample(1:n, 300)] <- NA  # More missingness\n\nimp_aux <- mice(toy[, c(\"x\", \"y\", \"z\", \"aux\")], m = 5, method = \"pmm\", seed = 42,printFlag = F)\nfit_aux <- with(imp_aux, lm(y ~ x + z + aux))\npooled_summary = summary(pool(fit_aux))\npooled_summary |> \n  as.data.frame() |>\n  mutate_if(is.numeric, round, 3)  |> \n  DT::datatable(options = list(pageLength = 10, autoWidth = TRUE), \n                rownames = FALSE)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-0db599c720acae7dee6d\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-0db599c720acae7dee6d\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"(Intercept)\",\"x\",\"z\",\"aux\"],[-0.036,0.222,0.049,-0.221],[0.385,0.234,0.199,0.225],[-0.095,0.95,0.249,-0.985],[114.422,68.312,128.582,87.264],[0.925,0.346,0.804,0.327]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>term<\\/th>\\n      <th>estimate<\\/th>\\n      <th>std.error<\\/th>\\n      <th>statistic<\\/th>\\n      <th>df<\\/th>\\n      <th>p.value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":10,\"autoWidth\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5]},{\"name\":\"term\",\"targets\":0},{\"name\":\"estimate\",\"targets\":1},{\"name\":\"std.error\",\"targets\":2},{\"name\":\"statistic\",\"targets\":3},{\"name\":\"df\",\"targets\":4},{\"name\":\"p.value\",\"targets\":5}],\"order\":[],\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Plotting Imputed Datasets produced from MICE with auxiliary variable\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Missing_data/fig_unnamed-chunk-10-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n::: notes \nHere we are testing whether adding the auxiliary variable aux improves your model's estimation of y, especially given the extra missingness. Including helpful predictors like aux can improve the quality of imputations and regression estimates. To do that we compare stuff like fit stats between the two models to see which produces the better fit. \n:::\n\n------------------------------------------------------------------------\n\n## Summary of Key Takeaways\n\n| Method          | Bias    | Variability | Ease of Use     |\n|-----------------|---------|-------------|-----------------|\n| Listwise        | ❌ High | ❌ Reduced  | ✅ Easy         |\n| Mean Imputation | ❌ High | ❌ Too Low  | ✅ Easy         |\n| ML              | ✅ Low  | ✅ Accurate | ⚠:Intermediate  |\n| MI              | ✅ Low  | ✅ Accurate | ⚠️ Intermediate |\n\n------------------------------------------------------------------------\n\n## Conclusion\n\n-   Traditional methods can lead to biased results\\\n-   Modern techniques (ML, MI) use all available data\\\n-   Better estimates, standard errors, and power\\\n-   Use tools like `mice` and `naniar` for effective handling of missing data\n\n------------------------------------------------------------------------\n\n## Package Citations\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. <https://www.jstatsoft.org/v40/i03/>.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, <https://CRAN.R-project.org/package=tibble>.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, <https://CRAN.R-project.org/package=patchwork>.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n  - Rinker TW, Kurkiewicz D (2018). _pacman: Package Management for R_. version 0.5.0, <http://github.com/trinker/pacman>.\n  - Tiedemann F (2022). _gghalves: Compose Half-Half Plots Using Your Favourite Geoms_. R package version 0.1.4, <https://CRAN.R-project.org/package=gghalves>.\n  - Tierney N, Cook D (2023). \"Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\" _Journal of Statistical Software_, *105*(7), 1-31. doi:10.18637/jss.v105.i07 <https://doi.org/10.18637/jss.v105.i07>.\n  - van Buuren S, Groothuis-Oudshoorn K (2011). \"mice: Multivariate Imputation by Chained Equations in R.\" _Journal of Statistical Software_, *45*(3), 1-67. doi:10.18637/jss.v045.i03 <https://doi.org/10.18637/jss.v045.i03>.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, <https://ggplot2.tidyverse.org>.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, <https://CRAN.R-project.org/package=forcats>.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, <https://CRAN.R-project.org/package=stringr>.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.\n  - Wickham H, Bryan J, Barrett M, Teucher A (2024). _usethis: Automate Package and Project Setup_. R package version 3.0.0, <https://CRAN.R-project.org/package=usethis>.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, <https://CRAN.R-project.org/package=dplyr>.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, <https://CRAN.R-project.org/package=purrr>.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, <https://CRAN.R-project.org/package=readr>.\n  - Wickham H, Hester J, Chang W, Bryan J (2022). _devtools: Tools to Make Developing R Packages Easier_. R package version 2.4.5, <https://CRAN.R-project.org/package=devtools>.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, <https://CRAN.R-project.org/package=tidyr>.\n```\n\n\n:::\n:::\n\n\n\n## Thanks for listening!\n\n<div style=\"text-align: center;\">\n  ![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ2t5cmgyeDVqbXc0djlsZ3BlcXE0bHd3N3h4Z282MmMxdmRxaDJ5NiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o7TKF44elNqqK84U0/giphy.gif)\n</div>\n\n\n\n\n---",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}