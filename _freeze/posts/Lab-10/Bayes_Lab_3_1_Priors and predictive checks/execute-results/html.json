{
  "hash": "49f3441a5e9da799261fd835461c62e6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Priors and Predicitive Checks\"\nsubtitle: \"Pinceton University\"\nauthor: \"Steven Mesquiti\"\noutput: \n  tufte::tufte_html:\n    css: \n    tufte_variant: \"envisioned\"\n    highlight: github-dark\n    fig_height: 10\n    fig_width: 16\n    toc: true\n    toc_depth: 1\nexecute: \n  message: false\n  warning: false\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\nformat: html\nengine: knitr\ncategories: [Lab, code, analysis, bayes]\n---\n\n\n\n\\\nDuring the first Bayes Lab you considered exploratory data analysis, compared default brms with lm(), and extracted posteriors after fitting models. You summarized posterior distributions and also generated a distribution of predictions using these posterior draws.\\\n\\\nDuring the second Bayes lab, you looked at the different types of distributions that are relevant for Bayesian analysis, including priors.\n\nDuring today's lab, you will go into prior predictive checks and some HMC diagnostics. While we look at the simple linear modeling case, this workflow is relevant for all Bayesian models.\n\n## Setup: Packages and data\n\nLoad the primary packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\npacman::p_load(tidyverse, brms, tidybayes,\n               ggdist,bayesplot,moderndive,faux,GGally,ggmcmc,install = T)\nset.seed(42)\n\n\npalette <- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\nplot_aes = theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.text = element_text(size = 12),\n    text = element_text(size = 16, family = \"Futura Medium\"),\n    axis.text = element_text(color = \"black\"),\n    axis.ticks.y = element_blank(),\n    plot.title = element_text(size = 20, hjust = 0.5) # Adjusted title size and centering\n  )\n```\n:::\n\n\n\nThis time we'll be taking data from the **moderndive** package. We want the `evals` data set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(evals, package = \"moderndive\")\n```\n:::\n\n\n\nThe `evals` data were originally in the paper by Hamermesh and Parker (2005; <https://doi.org/10.1016/j.econedurev.2004.07.013).> You can learn more about the data like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?moderndive::evals\n```\n:::\n\n\n\nYou can learn even more information about the data from <https://www.openintro.org/data/index.php?data=evals.>\n\nAnyway, we need to subset the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevals94 <- evals %>% \n  group_by(prof_ID) %>% \n  slice(1) %>% \n  ungroup()\n\nglimpse(evals94) |> \n  DT::datatable()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 94\nColumns: 14\n$ ID           <int> 1, 5, 8, 10, 18, 24, 31, 36, 43, 50, 60, 63, 68, 75, 79, …\n$ prof_ID      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ score        <dbl> 4.7, 4.6, 4.1, 4.5, 4.8, 4.4, 4.4, 3.4, 4.8, 4.0, 3.6, 4.…\n$ age          <int> 36, 59, 51, 40, 31, 62, 33, 51, 33, 47, 35, 37, 42, 49, 3…\n$ bty_avg      <dbl> 5.000, 3.000, 3.333, 3.167, 7.333, 5.500, 4.167, 4.000, 4…\n$ gender       <fct> female, male, male, female, female, male, female, female,…\n$ ethnicity    <fct> minority, not minority, not minority, not minority, not m…\n$ language     <fct> english, english, english, english, english, english, eng…\n$ rank         <fct> tenure track, tenured, tenured, tenured, tenure track, te…\n$ pic_outfit   <fct> not formal, not formal, not formal, not formal, not forma…\n$ pic_color    <fct> color, color, color, color, color, color, color, color, c…\n$ cls_did_eval <int> 24, 17, 55, 40, 42, 182, 33, 25, 48, 16, 18, 30, 28, 30, …\n$ cls_students <int> 43, 20, 55, 46, 48, 282, 41, 41, 60, 19, 25, 34, 40, 36, …\n$ cls_level    <fct> upper, upper, upper, upper, upper, upper, upper, upper, u…\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-30e4409849e39179307f\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-30e4409849e39179307f\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\"],[1,5,8,10,18,24,31,36,43,50,60,63,68,75,79,83,89,94,102,111,121,127,128,133,140,142,147,154,158,162,163,170,173,178,191,194,198,209,217,218,223,227,231,234,238,241,242,245,252,265,271,275,282,288,291,294,296,298,308,310,313,314,315,317,320,327,333,335,338,339,348,358,364,369,373,375,377,383,387,390,394,398,409,414,419,427,430,432,439,442,444,447,454,460],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94],[4.7,4.6,4.1,4.5,4.8,4.4,4.4,3.4,4.8,4,3.6,4.1,4.1,3.5,2.9,4.6,4.4,4,4.9,3.5,3.7,3.4,4.3,4.5,4.8,4.4,4.4,3.6,4.6,2.3,4.3,4.9,4.2,4.2,4.3,3.9,3.9,4.8,3.3,4.4,4.3,3.3,4,4.5,4.9,3.7,3.9,4.4,4.4,4.5,4.3,4.8,4.1,3.5,4,4.4,3.8,4.1,3.7,3.2,4.2,4.5,3.8,3.7,4.5,3.8,4.5,2.4,3,4.5,4.6,3.5,4.8,4.2,4,3.7,4.5,4.7,4.3,4,4.8,3.5,3.3,4.2,4.8,4.9,4.5,3.3,3.3,3.6,4.1,3.7,4.5,3.5],[36,59,51,40,31,62,33,51,33,47,35,37,42,49,37,45,56,48,46,57,52,29,62,64,34,58,52,73,70,41,63,47,39,47,54,44,47,60,37,42,35,39,49,61,33,58,56,50,52,33,57,38,34,34,32,32,42,43,35,62,42,39,52,52,52,64,50,60,51,43,50,52,51,38,47,43,38,43,57,51,45,57,47,54,58,42,33,62,35,61,52,60,32,42],[5,3,3.333,3.167,7.333,5.5,4.167,4,4.667,5.5,4.833,4.333,4.833,4,5.5,4.167,2.5,4.333,4.333,4.333,4.833,2.833,3,4.167,7.833,3.833,4.833,3,3,5.167,4.333,2.667,5.5,4.333,2.333,6.5,2.333,3.667,6.167,4,4.833,8.167,6.5,4.833,7,4.667,3.833,3.167,3.167,5.833,5.667,6.5,1.667,6.667,3.667,3.833,6.167,3.333,3.667,3.5,2.667,5.667,6,6.5,2.333,2.333,7.167,1.667,5.167,3.5,3.333,5.833,6.167,3.333,5.167,4.167,2.5,4.333,3,6.333,3.333,2.833,6.667,6.833,7.833,7.833,5.833,2,7.833,3.333,4.5,4.333,6.833,5.333],[\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"female\",\"female\",\"male\",\"male\",\"male\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"female\",\"female\",\"female\",\"male\",\"male\",\"female\",\"male\",\"male\",\"male\",\"male\",\"female\",\"male\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"male\",\"male\",\"male\",\"female\",\"male\",\"male\",\"male\",\"female\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"female\",\"male\",\"male\",\"male\",\"female\",\"male\",\"male\",\"male\",\"male\",\"male\",\"female\",\"female\",\"female\",\"male\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"male\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\"],[\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\",\"not minority\",\"not minority\",\"not minority\",\"not minority\",\"minority\"],[\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"non-english\",\"english\",\"english\",\"non-english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"non-english\",\"english\",\"english\",\"english\",\"english\",\"non-english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"non-english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"english\",\"non-english\",\"english\",\"non-english\"],[\"tenure track\",\"tenured\",\"tenured\",\"tenured\",\"tenure track\",\"tenured\",\"tenure track\",\"tenured\",\"tenure track\",\"teaching\",\"tenure track\",\"teaching\",\"tenured\",\"tenured\",\"tenure track\",\"tenured\",\"teaching\",\"teaching\",\"tenured\",\"teaching\",\"teaching\",\"tenure track\",\"tenured\",\"tenured\",\"tenure track\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"tenure track\",\"teaching\",\"tenured\",\"tenured\",\"tenure track\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"tenure track\",\"tenure track\",\"tenured\",\"teaching\",\"tenured\",\"tenured\",\"tenure track\",\"tenured\",\"tenured\",\"teaching\",\"tenured\",\"tenure track\",\"tenured\",\"tenured\",\"tenure track\",\"tenure track\",\"tenure track\",\"tenure track\",\"tenured\",\"tenured\",\"tenure track\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"teaching\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"tenure track\",\"teaching\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"tenured\",\"teaching\",\"tenured\",\"tenured\",\"tenured\",\"teaching\",\"tenured\",\"teaching\",\"tenured\",\"teaching\",\"tenured\",\"tenure track\",\"tenured\",\"tenure track\",\"tenured\",\"tenured\",\"tenure track\",\"tenure track\",\"tenure track\"],[\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"formal\",\"formal\",\"formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\",\"formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"formal\",\"formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"not formal\",\"formal\",\"not formal\",\"not formal\"],[\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"black&amp;white\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"black&amp;white\",\"color\",\"color\",\"color\",\"black&amp;white\",\"black&amp;white\",\"black&amp;white\",\"color\",\"color\",\"black&amp;white\",\"black&amp;white\",\"color\",\"color\",\"black&amp;white\",\"color\",\"color\",\"color\",\"black&amp;white\",\"black&amp;white\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"color\",\"black&amp;white\",\"black&amp;white\",\"black&amp;white\",\"black&amp;white\",\"black&amp;white\",\"color\",\"color\",\"color\",\"color\",\"color\",\"black&amp;white\",\"color\",\"color\"],[24,17,55,40,42,182,33,25,48,16,18,30,28,30,23,23,27,100,25,17,19,19,30,15,20,84,13,12,47,10,28,10,86,21,15,30,20,25,15,13,42,22,28,22,30,26,12,22,57,10,69,46,54,24,85,20,44,35,11,22,45,22,64,31,59,9,15,23,47,19,17,46,348,44,19,47,33,10,7,9,12,13,16,18,16,15,85,11,60,27,61,23,98,48],[43,20,55,46,48,282,41,41,60,19,25,34,40,36,28,33,41,135,31,28,22,26,39,24,26,159,16,17,134,12,43,15,246,21,15,55,27,34,17,14,51,24,45,27,31,34,19,27,67,15,77,65,91,36,248,22,62,51,19,27,86,29,88,44,75,11,16,32,67,22,24,69,574,87,24,103,68,13,13,11,17,14,21,18,17,18,120,38,96,39,111,27,132,84],[\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"lower\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"lower\",\"lower\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"lower\",\"upper\",\"lower\",\"upper\",\"upper\",\"upper\",\"lower\",\"upper\",\"upper\",\"upper\",\"upper\",\"lower\",\"upper\",\"lower\",\"upper\",\"lower\",\"lower\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"upper\",\"lower\",\"lower\",\"upper\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"upper\",\"upper\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"lower\",\"upper\",\"lower\",\"upper\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>ID<\\/th>\\n      <th>prof_ID<\\/th>\\n      <th>score<\\/th>\\n      <th>age<\\/th>\\n      <th>bty_avg<\\/th>\\n      <th>gender<\\/th>\\n      <th>ethnicity<\\/th>\\n      <th>language<\\/th>\\n      <th>rank<\\/th>\\n      <th>pic_outfit<\\/th>\\n      <th>pic_color<\\/th>\\n      <th>cls_did_eval<\\/th>\\n      <th>cls_students<\\/th>\\n      <th>cls_level<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,12,13]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"ID\",\"targets\":1},{\"name\":\"prof_ID\",\"targets\":2},{\"name\":\"score\",\"targets\":3},{\"name\":\"age\",\"targets\":4},{\"name\":\"bty_avg\",\"targets\":5},{\"name\":\"gender\",\"targets\":6},{\"name\":\"ethnicity\",\"targets\":7},{\"name\":\"language\",\"targets\":8},{\"name\":\"rank\",\"targets\":9},{\"name\":\"pic_outfit\",\"targets\":10},{\"name\":\"pic_color\",\"targets\":11},{\"name\":\"cls_did_eval\",\"targets\":12},{\"name\":\"cls_students\",\"targets\":13},{\"name\":\"cls_level\",\"targets\":14}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Intercept-only model\n\nLet's start by fitting an intercept-only model\n\n$$\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???},\n\\end{align} \n$$\n\nwhere $\\beta_0$ is the same as the unconditional population mean, and the population standard deviation is $\\sigma$. Our next task will be choosing our priors.\n\n#### Question 1: Why have we left some of the specification above unfilled / with questions marks at this point?\n\n\\[\\[Answer: We have left some of the specifcation abvoe unfilled because we have not ovserved the data yet and therefore do not have priors\\]\\]\n\n### Visualize possible prior distributions.\n\nIn this exercise, we'll choose the priors together. Let's start with prior on $\\beta_0$. Below are a few candidate distributions visualized with **ggdist** and friends.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(8, 2)),\n  prior(normal(5.5, 2))\n) %>% \n  parse_dist() %>% \n\n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper boundaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nThe red lines in the figures (shown at x=1 and x=10) represent the lower and upper boundaries for the beauty ratings scale used in the study. With the simple intercept model, setting a prior on the intercept parameter is the same as setting a prior on the expected mean in observation space.\n\nNow let's visualize a few potential priors for $\\sigma$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(\n  prior(exponential(1)), \n  prior(normal(0, 1), lb = 0), \n  prior(normal(2, 0.3), lb = 0)\n) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  xlab(expression(italic(p)(sigma))) +\n  ylab(NULL) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n#### Question 2: Given that $\\sigma$ refers to the standard deviation, are these three priors theoretically possible? If yes, give an example of a theoretically impossible prior for $\\sigma$.\n\n\\[\\[Answer: Yes, these three priors are theoretically possible. A theoretically impossible prior for sigma would be a normal distribution with a mean of 0 and a standard deviation of 1. This is because the standard deviation cannot be negative.\\]\\]\n\n### Prior-predictive checks (by hand).\n\nNote: It's possible we'll need the `truncnorm::rtruncnorm()` function in this section. Once we have candidate priors for both $\\beta_0$ and $\\sigma$, we can simulate values from those priors and plot the implied distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many distributions do you want?\nn <- 50\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %>% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %>% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %>% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThe simulated values constitute predictions that are made using our prior beliefs (a prior is set for beta0 and another for sigma) When you check if these predictions (prior predictive) make sense or not, it is called the prior predictive check. The point of the prior predictive check is to iterate on specifying the priors until the prior predictive is sensible/satisfactory.\n\n(Again, the red boundaries denote that the only possible bty_avg values are between 1 and 10.)\n\n#### Question 3: Can Explain what the section of the previous command, before ggplot is doing?\n\n\\[\\[This code generates a dataset of normal distribution densities across a range of bty_avg values for n randomly sampled (mean = beta0, sd = sigma) parameter pairs.\\]\\]\n\n#### Question 4: The prior predictive above is for one combination of our candidate priors. Why don't you also try the $\\beta_0$ prior centered at 8, along with the $\\sigma$ prior centered at 2? What do you observe? Among these two , which would you pick? And why? (Optional: try others too if you'd like)\n\n\\[\\[The second prior (with (\\beta\\_0) centered at 8 and (\\sigma) centered at 2) shifts the predictions higher and makes them more spread out; I would pick the one that best matches what I expect in the real data—if I expect higher and more variable values, I’d go with this new one.\\]\\]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many distributions do you want?\nn <- 50\n\n# do you want to make the simulation reproducible?\n# set.seed(1)\n\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 8, sd = 2),\n       sigma = rnorm(n = n, mean = 2, sd = 0.3)) %>% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %>% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %>% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n### Fit the model that you prefer\n\nWe should practice writing out our model equation with our priors of choice:\n\n$$\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(8,\\ 2) \\\\\n\\sigma & \\sim \\operatorname{Normal}(2,\\ 0.3)\n\\end{align}\n$$\n\nLet's fit a model with our priors of choice.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_path <- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit9b.rds\")\n\nif (!file.exists(model_path)) {\nfit9.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  # make sure we're settled on our priors \n  # we don't need to use these; they're placeholders\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma)\n)\n  saveRDS(fit9.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit9.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\nCheck the model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit9.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.63      0.17     4.29     4.95 1.00     2964     2310\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.60      0.12     1.39     1.85 1.00     3568     2545\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nNow we might do a posterior predictive check to see how well our model describes the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\npp_check(fit9.b, ndraws = 100) +\n  ggtitle(\"posterior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(2)\npp_check(fit9.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\")  +\n  ggtitle(\"posterior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n\nOur simple Gaussian model doesn't do a great job respecting the lower and upper boundaries, but this is about as good as it gets when you're in Gaussian land. On the whole, the model did a pretty okay reproducing the gross features of the distribution of the sample data.\n\n#### Question 5: To ensure you've understood things well, can you write below about the difference between the prior predictive check and the posterior predictive check? How do they differ in their objectives?\n\n\\[\\[The prior predictive check is used to evaluate the plausibility of the prior distributions before observing the data, while the posterior predictive check is used to evaluate how well the model fits the observed data after accounting for the priors and likelihood. The former focuses on the prior beliefs, while the latter focuses on the model's performance with actual data.\\]\\]\n\n## Prior-predictive checks (by `sample_prior = \"only\"`)\n\nWe can also sample from the prior predictive distribution from `brm()` itself. To do so, we use the `sample_prior` argument, which has the following options:\n\n-   `\"no\"`, which is the default, and does not sample from the prior;\n-   `\"yes\",`, which will sample from both the prior and the posterior; and\n-   `\"only\"`, which will only sample from the prior.\n\nLet's set `sample_prior = \"only\"`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check to see if we want to use other priors\nmodel_path <- file.path(\"~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit10b.rds\")\n\nif (!file.exists(model_path)) {\nfit10.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  # we can set our seed, too!\n  seed = 1\n)\nsaveRDS(fit10.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit10.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\nDid you notice how we used the `seed` argument? This makes the results reproducible.\n\nNow the `summary()` function only returns summaries for the priors, NOT the posterior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit10.b)  # this summarizes the prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     5.50      1.00     3.61     7.49 1.00     1876     1938\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.00      1.01     0.03     3.63 1.00     1957     1424\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThe `as_draws_df()` function also returns draws from the prior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit10.b) %>% \n  head() |> \n  DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-2e17466358a46d139971\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-2e17466358a46d139971\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[5.492759088082171,4.769111980281679,4.402793711046897,6.0584672280751,5.484559222447029,5.651943228769588],[0.8019175091438313,1.355200194182551,0.4609776691080583,0.7174536953284371,1.125867525998223,1.510549985807742],[5.492759088082171,4.769111980281679,4.402793711046897,6.0584672280751,5.484559222447029,5.651943228769588],[-1.720882257751205,-2.541237376071108,-1.981847022571851,-1.792335050950053,-2.044925268008616,-2.441031891396879],[-1.941631790586051,-2.237288187868459,-2.756252699843272,-2.124381920508573,-1.92637139530282,-2.02855807787358],[1,1,1,1,1,1],[1,2,3,4,5,6],[1,2,3,4,5,6]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>b_Intercept<\\/th>\\n      <th>sigma<\\/th>\\n      <th>Intercept<\\/th>\\n      <th>lprior<\\/th>\\n      <th>lp__<\\/th>\\n      <th>.chain<\\/th>\\n      <th>.iteration<\\/th>\\n      <th>.draw<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"b_Intercept\",\"targets\":1},{\"name\":\"sigma\",\"targets\":2},{\"name\":\"Intercept\",\"targets\":3},{\"name\":\"lprior\",\"targets\":4},{\"name\":\"lp__\",\"targets\":5},{\"name\":\".chain\",\"targets\":6},{\"name\":\".iteration\",\"targets\":7},{\"name\":\".draw\",\"targets\":8}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\nHere's how we might use that `as_draws_df()` output to make a similar plot to the one we made before.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many distributions do you want?\nn <- 50\n\n# do you want to make the results reproducible?\n# set.seed(1)\n\nas_draws_df(fit10.b) %>% \n  \n  # subset\n  slice_sample(n = n) %>% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %>% \n  # notice we're defining the mean by b_Intercept\n  mutate(density = dnorm(x = bty_avg, mean = b_Intercept, sd = sigma)) %>% \n  \n  ggplot(aes(x = bty_avg, y = density, \n             # notice we're grouping by .draw\n             group = .draw)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma))) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nWe can also use functions like `pp_check()` to compare the prior to the sample data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\npp_check(fit10.b, ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(2)\npp_check(fit10.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n\n## Univariable predictor model\n\nNow we'll add `gender` as the sole predictor in the model,\n\n$$\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{gender}_i \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\beta_1 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???}.\n\\end{align}\n$$\n\nLet's try these same set of $\\beta_0$ priors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# change as needed\n\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(7, 0.5)),\n  prior(normal(5.5, 2))\n) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper bondaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nNow we update our by-hand prior predictive simulation to accomodate $\\beta_0$ and $\\beta_1$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50\n\nset.seed(1)\n\ntibble(iter = 1:n,\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       # notice our new line\n       beta1 = rnorm(n = n, mean = 0, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %>% \n  # we have a new expand_grid() line\n  # make sure everyone understands this coding scheme\n  expand_grid(gendermale = 0:1) %>% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %>% \n  # notice the updated mean formula\n  mutate(density = dnorm(x = bty_avg, \n                         mean = beta0 + beta1 * gendermale, \n                         sd = sigma)) %>% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~ and~italic(p)(beta[1])~and~italic(p)(sigma))) +\n  facet_wrap(~ gendermale, labeller = label_both) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nBefore we fit the model, let's practice the `sample_prior = \"only\"` approach.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check to see if we want to use other priors\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit11b.rds'\n\nif (!file.exists(model_path)) {\n\nfit11.b = brm(\n  data = evals94,\n  family = gaussian,\n  # notice the 0 + Intercept syntax\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  seed = 2\n)\nsaveRDS(fit11.b, model_path)\n} else {\n  # If the RDS file already exists, load the data from it\n  fit11.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\nCheck the prior summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit11.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      5.46      1.00     3.53     7.43 1.00     3147     2820\ngendermale     0.02      0.99    -1.91     1.99 1.00     4109     2946\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.02      0.98     0.03     3.66 1.00     3149     1903\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nCompare the prior with the data with `pp_check()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\npp_check(fit11.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(2)\npp_check(fit11.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n:::\n\n\n\nThere isn't a great grouped histogram option for `pp_check()`, so we experimented with `type = \"freqpoly_grouped\"` instead.\n\nIf we wanted, we could also use the `predict()` function to simulate `bty_avg` values from the priors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# walk through this slowly\n\nset.seed(1)\n\npredict(fit11.b,\n        summary = FALSE,\n        ndraws = 5) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:5, 1:94] 5.37 7.82 3.87 4.89 6.06 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# customize the predictor grid, as desired\nnd <- tibble(gender = rep(c(\"female\", \"male\"), each = 50)) %>% \n  # this will make it easier to connect the nd data to the predict() output\n  mutate(row = 1:n())\n\nset.seed(1)\n\npredict(fit11.b,\n        newdata = nd,\n        summary = FALSE,\n        ndraws = 5) %>% \n  data.frame() %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(row = str_remove(name, \"X\") %>% as.double()) %>% \n  left_join(nd, by = \"row\") %>% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 0.5, boundary = 1) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  facet_grid(draw ~ gender, labeller = label_both) + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nOnce we've settled on our priors, we should once again practice writing out the full model equation:\n\n$$\n\\begin{align} \\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i & = \\beta_0 + \\beta_1 \\cdot \\text{gender(male)}_i \\\\ \\beta_0 & \\sim \\operatorname{Normal}(5.46,\\ 1.00) \\\\ \\beta_1 & \\sim \\operatorname{Normal}(0.02,\\ 0.99) \\\\ \\sigma & \\sim \\operatorname{Normal}(1.02,\\ 0.98) \\end{align}\n$$\n\nOkay, let's fit the real model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check to see if we want to use other priors\n\nmodel_path = '~/Library/CloudStorage/GoogleDrive-sm9518@princeton.edu/My Drive/Classes/Stats-blog/posts/Lab-10/models/fit12b.rds'\n\nif (!file.exists(model_path)) {\n\nfit12.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  \n  # yes, you can set your seed for your posteriors, too\n  # this makes the results reproducible\n  seed = 3\n)\nsaveRDS(fit12.b,model_path)\n} else {\n  fit12.b <- readRDS(model_path)\n}\n```\n:::\n\n\n\nCheck the model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit12.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      4.93      0.24     4.48     5.40 1.00     2134     2345\ngendermale    -0.54      0.31    -1.14     0.06 1.00     2050     2218\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.59      0.12     1.38     1.83 1.00     2431     2371\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nHow does the posterior-predictive check look?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\npp_check(fit12.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12)) +\n  ggtitle(\"posterior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(2)\npp_check(fit12.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\") + plot_aes\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_1_Priors-and-predictive-checks_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n:::\n\n\n\n#### Question 6: Does the posterior predictive check look satsifactory to you?\n\n\\[\\[The posterior predictive check looks satisfactory to me. The model seems to be able to capture the distribution of the data well, and the red lines marking the boundaries are respected.\\]\\]\n\n::: callout-note\nFor more on prior predictive checks, see McElreath (from Chapter 4), and Solomon Kurz's brms/tidverse implementations as well.\n\nFor a comprehensive guide to set priors for a given situation, look at reccomendations made by the Stan team https://github.com/stan-dev/stan/wiki/prior-choice-recommendations\n\nThey generally recommend against uniform priors on $\\beta$ and $\\sigma$ parameters. This is based on a general principle that you should not use a prior that places an artificial boundary on a parameter.\n\nE.g. $\\sigma$ parameters have natural lower boundaries at zero, but they don't have upper boundaries. Thus, a uniform prior adds an unnatural upper boundary. A better prior would be something that is weakly informative\n:::\n\n## References\n\nHamermesh, D. S., & Parker, A. (2005). Beauty in the classroom: Instructors' pulchritude and putative pedagogical productivity. *Economics of Education Review, 24*(4), 369-376. https://doi.org/10.1016/j.econedurev.2004.07.013\n\nKurz, A. S. (2023). *Statistical Rethinking with brms, ggplot2, and the tidyverse: Second Edition* (version 0.4.0). https://bookdown.org/content/4857/\n\nMcElreath, R. (2020). *Statistical rethinking: A Bayesian course with examples in R and Stan* (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n## Session information\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggmcmc_1.5.1.1   GGally_2.2.1     faux_1.2.2       moderndive_0.7.0\n [5] bayesplot_1.11.1 ggdist_3.3.2     tidybayes_3.0.7  brms_2.21.0     \n [9] Rcpp_1.0.13      lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.4      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  pacman_0.5.1    \n\nloaded via a namespace (and not attached):\n  [1] infer_1.0.7          RColorBrewer_1.1-3   tensorA_0.36.2.1    \n  [4] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [7] estimability_1.5.1   farver_2.1.2         nloptr_2.1.1        \n [10] rmarkdown_2.28       vctrs_0.6.5          minqa_1.2.8         \n [13] base64enc_0.1-3      janitor_2.2.1        htmltools_0.5.8.1   \n [16] distributional_0.5.0 curl_5.2.2           broom_1.0.7         \n [19] sass_0.4.9           StanHeaders_2.32.10  bslib_0.8.0         \n [22] htmlwidgets_1.6.4    plyr_1.8.9           emmeans_1.10.7      \n [25] zoo_1.8-12           cachem_1.1.0         igraph_2.0.3        \n [28] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n [31] colourpicker_1.3.0   Matrix_1.7-0         R6_2.5.1            \n [34] fastmap_1.2.0        rbibutils_2.3        shiny_1.9.1         \n [37] snakecase_0.11.1     digest_0.6.37        numDeriv_2016.8-1.1 \n [40] colorspace_2.1-1     crosstalk_1.2.1      labeling_0.4.3      \n [43] fansi_1.0.6          timechange_0.3.0     abind_1.4-5         \n [46] compiler_4.4.1       withr_3.0.1          backports_1.5.0     \n [49] inline_0.3.19        shinystan_2.6.0      ggstats_0.9.0       \n [52] QuickJSR_1.3.1       pkgbuild_1.4.4       MASS_7.3-60.2       \n [55] gtools_3.9.5         loo_2.8.0            tools_4.4.1         \n [58] httpuv_1.6.15        threejs_0.3.3        glue_1.8.0          \n [61] nlme_3.1-164         promises_1.3.0       grid_4.4.1          \n [64] checkmate_2.3.2      reshape2_1.4.4       generics_0.1.3      \n [67] operator.tools_1.6.3 gtable_0.3.5         tzdb_0.4.0          \n [70] formula.tools_1.7.1  hms_1.1.3            utf8_1.2.4          \n [73] pillar_1.9.0         markdown_1.13        posterior_1.6.0     \n [76] later_1.3.2          splines_4.4.1        lattice_0.22-6      \n [79] survival_3.6-4       tidyselect_1.2.1     miniUI_0.1.1.1      \n [82] knitr_1.48           reformulas_0.4.0     arrayhelpers_1.1-0  \n [85] gridExtra_2.3        V8_6.0.1             stats4_4.4.1        \n [88] xfun_0.52            rstanarm_2.32.1      bridgesampling_1.1-2\n [91] matrixStats_1.4.1    DT_0.33              rstan_2.32.6        \n [94] stringi_1.8.4        yaml_2.3.10          boot_1.3-30         \n [97] evaluate_1.0.0       codetools_0.2-20     cli_3.6.4           \n[100] RcppParallel_5.1.9   Rdpack_2.6.2         shinythemes_1.2.0   \n[103] xtable_1.8-4         munsell_0.5.1        jquerylib_0.1.4     \n[106] coda_0.19-4.1        svUnit_1.0.6         parallel_4.4.1      \n[109] rstantools_2.4.0     dygraphs_1.1.1.6     Brobdingnag_1.2-9   \n[112] lme4_1.1-36          mvtnorm_1.3-1        scales_1.3.0        \n[115] xts_0.14.1           rlang_1.1.5          shinyjs_2.1.0       \n```\n\n\n:::\n:::\n",
    "supporting": [
      "Bayes_Lab_3_1_Priors-and-predictive-checks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}